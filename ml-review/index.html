<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta http-equiv="Cache-Control" content="no-transform">
  <meta http-equiv="Cache-Control" content="no-siteapp">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"roubin.me","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="1.数据从哪里来？如何构建数据集？123456781.现场自行采集（成本比较高）2.甲方提供3.网络下载现成的4.企业真实的数据（商业价值最高）5.购买（合法购买和使用，千万不要侵犯别人的隐私）6.爬虫（合法使用，不要侵犯别人的隐私）数据分门别类,进行标注">
<meta property="og:type" content="article">
<meta property="og:title" content="ML&#x2F;DL复习">
<meta property="og:url" content="https://roubin.me/ml-review/index.html">
<meta property="og:site_name" content="肉饼博客">
<meta property="og:description" content="1.数据从哪里来？如何构建数据集？123456781.现场自行采集（成本比较高）2.甲方提供3.网络下载现成的4.企业真实的数据（商业价值最高）5.购买（合法购买和使用，千万不要侵犯别人的隐私）6.爬虫（合法使用，不要侵犯别人的隐私）数据分门别类,进行标注">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://roubin.me/images/ml_review_l1_expression.png">
<meta property="og:image" content="https://roubin.me/images/ml_review_mse_loss.png">
<meta property="og:image" content="https://roubin.me/images/ml_review_decision_tree.png">
<meta property="og:image" content="https://roubin.me/images/ml_review_svm.png">
<meta property="og:image" content="https://roubin.me/images/ml_review_kmeans.png">
<meta property="og:image" content="https://roubin.me/images/ml_review_dbscan.png">
<meta property="og:image" content="https://roubin.me/images/ml_review_sigmoid.png">
<meta property="og:image" content="https://roubin.me/images/ml_review_tanh.png">
<meta property="og:image" content="https://roubin.me/images/ml_review_relu_variant.png">
<meta property="og:image" content="https://roubin.me/images/ml_review_relu.png">
<meta property="og:image" content="https://roubin.me/images/ml_review_leaky_relu.png">
<meta property="og:image" content="https://roubin.me/images/ml_review_mish.png">
<meta property="og:image" content="https://roubin.me/images/ml_review_swish.png">
<meta property="article:published_time" content="2024-05-21T01:14:43.000Z">
<meta property="article:modified_time" content="2024-05-31T06:50:30.445Z">
<meta property="article:author" content="roubin">
<meta property="article:tag" content="ml">
<meta property="article:tag" content="dl">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://roubin.me/images/ml_review_l1_expression.png">

<link rel="canonical" href="https://roubin.me/ml-review/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>ML/DL复习 | 肉饼博客</title>
  


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?b55ece62ebeb2e72a8efe3f8b5b43960";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">肉饼博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Talk is cheap. Show me the code.</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://roubin.me/ml-review/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="roubin">
      <meta itemprop="description" content="芝兰其室，金石其心<br/>仁义为友，道德为师">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="肉饼博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          ML/DL复习
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-05-21 09:14:43" itemprop="dateCreated datePublished" datetime="2024-05-21T09:14:43+08:00">2024-05-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-05-31 14:50:30" itemprop="dateModified" datetime="2024-05-31T14:50:30+08:00">2024-05-31</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/ml-review/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="ml-review/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>19k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>17 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h4 id="1-数据从哪里来？如何构建数据集？"><a href="#1-数据从哪里来？如何构建数据集？" class="headerlink" title="1.数据从哪里来？如何构建数据集？"></a>1.数据从哪里来？如何构建数据集？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1.现场自行采集（成本比较高）</span><br><span class="line">2.甲方提供</span><br><span class="line">3.网络下载现成的</span><br><span class="line">4.企业真实的数据（商业价值最高）</span><br><span class="line">5.购买（合法购买和使用，千万不要侵犯别人的隐私）</span><br><span class="line">6.爬虫（合法使用，不要侵犯别人的隐私）</span><br><span class="line"></span><br><span class="line">数据分门别类,进行标注</span><br></pre></td></tr></table></figure>
<a id="more"></a>

<h4 id="2-数据量多大？"><a href="#2-数据量多大？" class="headerlink" title="2.数据量多大？"></a>2.数据量多大？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">深度学习越多越好</span><br><span class="line">最起码每个类别数百级单位</span><br></pre></td></tr></table></figure>

<h4 id="3-数据量不够如何处理？"><a href="#3-数据量不够如何处理？" class="headerlink" title="3.数据量不够如何处理？"></a>3.数据量不够如何处理？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">数据增强</span><br><span class="line">CV: 旋转、缩放、裁剪、调整色调、亮度、对比度、添加噪声。。。</span><br><span class="line">NLP: 近义词替换、文本摘要。。。。</span><br></pre></td></tr></table></figure>

<h4 id="4-采用的模型是什么？为什么使用YOLOv8？"><a href="#4-采用的模型是什么？为什么使用YOLOv8？" class="headerlink" title="4.采用的模型是什么？为什么使用YOLOv8？"></a>4.采用的模型是什么？为什么使用YOLOv8？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">效果</span><br><span class="line">  依据：根据实际需求，以及项目的难易程度，选择现有的、经典的、成熟的模型，更换成自己的数据集，进行训练，并调参</span><br><span class="line">  </span><br><span class="line">  如果不确定模型，选择多个模型，进行对比，选择效果最好的</span><br><span class="line">  </span><br><span class="line">  在有些情况下，需要多个模型配合使用，发挥各自的特长</span><br><span class="line"></span><br><span class="line">YOLOv8优势</span><br><span class="line">    1.保持高精度的同时，具有更快的推理速度，适合实时检测</span><br><span class="line">    2.多模型尺寸、多任务能力</span><br><span class="line">    3.友好的文档、API、社区支持</span><br></pre></td></tr></table></figure>

<h4 id="5-什么情况下使用OpenCV，什么情况下使用深度学习？"><a href="#5-什么情况下使用OpenCV，什么情况下使用深度学习？" class="headerlink" title="5.什么情况下使用OpenCV，什么情况下使用深度学习？"></a>5.什么情况下使用OpenCV，什么情况下使用深度学习？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Opencv：不需要程序理解图像的场景和内容，图像相对单一，干扰因素较少，需求比较简单，数据量比较少</span><br><span class="line">深度学习：需要程序理解图像的内容和场景，场景复杂，干扰因素多，样本变化大，需求复杂，数据量足够大</span><br></pre></td></tr></table></figure>

<h4 id="6-准确率是多少？"><a href="#6-准确率是多少？" class="headerlink" title="6.准确率是多少？"></a>6.准确率是多少？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">工业中至少要95%以上，越高越好，不要过拟合</span><br></pre></td></tr></table></figure>

<h4 id="7-写项目经验注意的问题"><a href="#7-写项目经验注意的问题" class="headerlink" title="7.写项目经验注意的问题"></a>7.写项目经验注意的问题</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">项目背景(需求)：用户是谁？用在什么地方？解决什么问题？</span><br><span class="line">数据集：来源？数量？数据增强？标注？预处理手段？</span><br><span class="line">模型：模型选择？训练过程？调参优化过程？</span><br><span class="line">遇到了什么问题？怎么解决的？</span><br><span class="line">过拟合，欠拟合问题怎么解决的？</span><br><span class="line">效果？</span><br><span class="line">部署？</span><br></pre></td></tr></table></figure>

<h4 id="8-什么是有监督学习（Supervised-Learning）和无监督学习（Unsupervised-Learning）？请举例说明每种类型的应用场景"><a href="#8-什么是有监督学习（Supervised-Learning）和无监督学习（Unsupervised-Learning）？请举例说明每种类型的应用场景" class="headerlink" title="8.什么是有监督学习（Supervised Learning）和无监督学习（Unsupervised Learning）？请举例说明每种类型的应用场景"></a>8.什么是有监督学习（Supervised Learning）和无监督学习（Unsupervised Learning）？请举例说明每种类型的应用场景</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">有监督：训练数据集包含了标签，在训练过程中，模型学习输入与标签之间的映射关系</span><br><span class="line">分类：图像分类、垃圾邮件分类</span><br><span class="line">回归：房价预测</span><br><span class="line"></span><br><span class="line">无监督：不依赖于标注数据，模型通过输入数据自行发现数据的结构或模式</span><br><span class="line">聚类：客户分组</span><br><span class="line">降维：从高维度数据中提取主要特征</span><br></pre></td></tr></table></figure>

<h4 id="9-贝叶斯公式及推导过程，有哪些应用场景？"><a href="#9-贝叶斯公式及推导过程，有哪些应用场景？" class="headerlink" title="9.贝叶斯公式及推导过程，有哪些应用场景？"></a>9.贝叶斯公式及推导过程，有哪些应用场景？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">公式：P(A|B) &#x3D; P(B|A)⋅P(A) &#x2F; P(B)</span><br><span class="line">解释：由先验概率和条件概率，推算出后验概率</span><br><span class="line"></span><br><span class="line">推导：</span><br><span class="line">由联合概率可知：P(A,B)&#x3D;P(A∣B)⋅P(B)</span><br><span class="line">由联合概率对称性可知：P(A,B)&#x3D;P(B∣A)⋅P(A)</span><br><span class="line">等式相等：P(A∣B)⋅P(B) &#x3D; P(B∣A)⋅P(A)</span><br><span class="line">两边除以P(B)得：P(A∣B) &#x3D; P(B∣A)⋅P(A) &#x2F; P(B)</span><br><span class="line"></span><br><span class="line">应用：</span><br><span class="line">朴素贝叶斯分类器</span><br><span class="line">医疗诊断中，计算患者患有特定疾病的概率</span><br><span class="line">风险评估</span><br></pre></td></tr></table></figure>

<h4 id="10-什么是似然？"><a href="#10-什么是似然？" class="headerlink" title="10.什么是似然？"></a>10.什么是似然？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">概念：在已知某些数据的情况下，模型参数取特定值的概率</span><br><span class="line">与概率的区别：概率描述的是在已知参数的情况下，观测到某数据的概率，而似然则是相反的情况，即在已知数据的情况下，参数的可能值</span><br></pre></td></tr></table></figure>

<h4 id="11-什么是欠拟合、过拟合？如何避免过拟合？如何避免欠拟合？"><a href="#11-什么是欠拟合、过拟合？如何避免过拟合？如何避免欠拟合？" class="headerlink" title="11.什么是欠拟合、过拟合？如何避免过拟合？如何避免欠拟合？"></a>11.什么是欠拟合、过拟合？如何避免过拟合？如何避免欠拟合？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">欠拟合：指模型无法从训练数据中学习到足够的模式，导致其在训练集和测试集上都表现不佳</span><br><span class="line">原因：模型太小太简单、训练数据太少、训练时间不够</span><br><span class="line">解决：</span><br><span class="line">    1.增加模型复杂度</span><br><span class="line">    2.增加特征数量</span><br><span class="line">    3.增加训练数据</span><br><span class="line">    4.训练更多轮数</span><br><span class="line">    5.减小正则化强度</span><br><span class="line"></span><br><span class="line">过拟合：指模型在训练集上表现很好，但在测试集上表现较差，即模型过度适应了训练数据，忽略了数据的总体趋势，导致泛化能力差</span><br><span class="line">原因：模型太复杂、数据集太小、训练时间过长</span><br><span class="line">解决：</span><br><span class="line">    1.数据增强</span><br><span class="line">    2.正则化</span><br><span class="line">    3.提前终止</span><br><span class="line">    4.集成学习</span><br><span class="line">    5.dropout</span><br></pre></td></tr></table></figure>

<h4 id="12-神经网络加速训练方法有哪些？"><a href="#12-神经网络加速训练方法有哪些？" class="headerlink" title="12.神经网络加速训练方法有哪些？"></a>12.神经网络加速训练方法有哪些？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">硬件：多GPU、分布式训练</span><br><span class="line">数据：归一化、数据增强</span><br><span class="line">模型：更好的优化器、BN、量化、剪枝、使用预训练模型</span><br><span class="line">训练：提前终止、混合精度训练、自动化超参数调整</span><br></pre></td></tr></table></figure>

<h4 id="13-目标检测常用算法有哪些，简述对算法的理解"><a href="#13-目标检测常用算法有哪些，简述对算法的理解" class="headerlink" title="13.目标检测常用算法有哪些，简述对算法的理解"></a>13.目标检测常用算法有哪些，简述对算法的理解</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">两阶段检测</span><br><span class="line">   	先产生候选区，在候选区上分类+定位</span><br><span class="line">   	速度相对比较慢，精度高</span><br><span class="line">   	RCNN系列  </span><br><span class="line">   	</span><br><span class="line">一阶段检测</span><br><span class="line">   	预定义候选区，直接在特征图上分类+定位</span><br><span class="line">   	速度比较快，精度较低</span><br><span class="line">   	YOLO系列、SSD、RetinaNet</span><br></pre></td></tr></table></figure>

<h4 id="14-什么是感受野？"><a href="#14-什么是感受野？" class="headerlink" title="14.什么是感受野？"></a>14.什么是感受野？</h4><h4 id="15-什么是正则化？L1、L2、smooth-L1正则化的区别"><a href="#15-什么是正则化？L1、L2、smooth-L1正则化的区别" class="headerlink" title="15.什么是正则化？L1、L2、smooth L1正则化的区别"></a>15.什么是正则化？L1、L2、smooth L1正则化的区别</h4><p>正则化：在损失函数后面添加一个范数（惩罚项），整体上压缩了参数的大小，来防止过拟合的手段</p>
<p>范数表达式如下：</p>
<p><img src="../images/ml_review_l1_expression.png" alt="img.png"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">L1正则化：也称为Lasso正则化，当p&#x3D;1时，是L1范数，表示某个向量中所有元素绝对值之和</span><br><span class="line">L2正则化：也称为Ridge正则化，当p&#x3D;2时，是L2范数，表示某个向量中所有元素平方和再开方，即欧氏距离</span><br><span class="line"></span><br><span class="line">区别：</span><br><span class="line">L1正则化的效果是使模型的参数变得稀疏，即部分参数的值为0，可用于特征选择和模型压缩</span><br><span class="line">L2正则化的效果是使模型的参数变得平滑，即相邻参数的值相差较小，可提升泛化能力</span><br><span class="line">smooth L1: 结合了L1和L2的特点</span><br></pre></td></tr></table></figure>

<h4 id="16-YOLOv8中的objectness-loss是什么？"><a href="#16-YOLOv8中的objectness-loss是什么？" class="headerlink" title="16.YOLOv8中的objectness loss是什么？"></a>16.YOLOv8中的objectness loss是什么？</h4><h4 id="17-什么是特征归一化？为什么要归一化？"><a href="#17-什么是特征归一化？为什么要归一化？" class="headerlink" title="17.什么是特征归一化？为什么要归一化？"></a>17.什么是特征归一化？为什么要归一化？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">归一化一般是将数据映射到指定的范围（[0, 1] 或 [-1, 1]），从而消除不同特征量纲的影响。</span><br><span class="line">进行归一化处理，使得不同指标之间处于同一数量级，具有可比性。另外还能加速模型收敛，提升性能</span><br></pre></td></tr></table></figure>

<h4 id="18-归一化常用方法？"><a href="#18-归一化常用方法？" class="headerlink" title="18.归一化常用方法？"></a>18.归一化常用方法？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">Min-Max Normalization</span><br><span class="line">公式：X &#x3D; (X-Xmin) &#x2F; (Xmax - Xmin)</span><br><span class="line">--------------------------------------</span><br><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line">import sklearn.preprocessing as sp</span><br><span class="line"></span><br><span class="line">raw_sample &#x3D; np.array([[3.0, -100.0, 2000.0],</span><br><span class="line">                       [0.0, 400.0, 3000.0],</span><br><span class="line">                       [1.0, -400.0, 2000.0]])</span><br><span class="line"></span><br><span class="line">mms_sample &#x3D; raw_sample.copy()</span><br><span class="line"></span><br><span class="line"># 1.减去最小值</span><br><span class="line"># 2.减完之后的结果&#x2F;极差</span><br><span class="line">for col in mms_sample.T:</span><br><span class="line">    col_min &#x3D; col.min()</span><br><span class="line">    col_max &#x3D; col.max()</span><br><span class="line">    col -&#x3D; col_min</span><br><span class="line">    col &#x2F;&#x3D; (col_max - col_min)</span><br><span class="line">  </span><br><span class="line">    </span><br><span class="line"># 基于skLearn提供的API实现</span><br><span class="line">scaler &#x3D; sp.MinMaxScaler()</span><br><span class="line">res &#x3D; scaler.fit_transform(raw_sample)</span><br></pre></td></tr></table></figure>

<h4 id="19-归一化处理适用模型"><a href="#19-归一化处理适用模型" class="headerlink" title="19.归一化处理适用模型"></a>19.归一化处理适用模型</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">应用归一化的模型：在实际应用中，通过梯度下降法求解的模型通常是需要归一化的，包括线性回归、逻辑回归、支持向量机、神经网络等模型。</span><br><span class="line">不使用归一化的模型：如决策树分类</span><br></pre></td></tr></table></figure>

<h4 id="20-什么是标准化？常用方法？"><a href="#20-什么是标准化？常用方法？" class="headerlink" title="20.什么是标准化？常用方法？"></a>20.什么是标准化？常用方法？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">标准化是将特征值调整为均值为0，标准差为1的标准正态分布</span><br><span class="line"></span><br><span class="line">Z-Score Normalization</span><br><span class="line">公式：X &#x3D; (X - Xmean) &#x2F; Xstd</span><br><span class="line">-----------------------------------</span><br><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line">import sklearn.preprocessing as sp</span><br><span class="line"></span><br><span class="line">raw_sample &#x3D; np.array([[3.0, -100.0, 2000.0],</span><br><span class="line">                       [0.0, 400.0, 3000.0],</span><br><span class="line">                       [1.0, -400.0, 2000.0]])</span><br><span class="line"></span><br><span class="line">std_sample &#x3D; raw_sample.copy()</span><br><span class="line"></span><br><span class="line"># 1.减去当前列的平均值</span><br><span class="line"># 2.离差&#x2F;原始数据的标准差</span><br><span class="line">for col in std_sample.T:</span><br><span class="line">    col_mean &#x3D; col.mean()  # 平均值</span><br><span class="line">    col_std &#x3D; col.std()  # 标准差</span><br><span class="line">    col -&#x3D; col_mean</span><br><span class="line">    col &#x2F;&#x3D; col_std</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"># 基于skLearn提供的API实现</span><br><span class="line">scaler &#x3D; sp.StandardScaler()</span><br><span class="line">res &#x3D; scaler.fit_transform(raw_sample)</span><br></pre></td></tr></table></figure>

<h4 id="21-标准化和归一化的联系和区别"><a href="#21-标准化和归一化的联系和区别" class="headerlink" title="21.标准化和归一化的联系和区别"></a>21.标准化和归一化的联系和区别</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">联系:</span><br><span class="line">    我们都知道归一化是指normalization，标准化是指standardization，但根据wiki上对feature scaling方法的定义，standardization其实就是z-score normalization，也就是说标准化其实是归一化的一种，而一般情况下，我们会把z-score归一化称为标准化，把min-max归一化简称为归一化</span><br><span class="line">    目的：都是通过缩放和平移来实现数据映射，消除不同特征量纲的影响</span><br><span class="line"></span><br><span class="line">区别：</span><br><span class="line">    归一化不会改变数据的状态分布，但标准化会</span><br><span class="line">    归一化会将数据限定在一个具体的范围内，如 [0, 1]，但标准化不会</span><br><span class="line">    归一化只受原样本数据中的极值影响，而标准化则受所有样本值的影响</span><br><span class="line">    归一化对异常值敏感，而标准化则对异常值鲁棒</span><br></pre></td></tr></table></figure>

<h4 id="22-均值、离差、离差方、方差、标准差之间的关系"><a href="#22-均值、离差、离差方、方差、标准差之间的关系" class="headerlink" title="22.均值、离差、离差方、方差、标准差之间的关系"></a>22.均值、离差、离差方、方差、标准差之间的关系</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">S &#x3D; np.array([1, 2, 3, 4, 5, 6])</span><br><span class="line"></span><br><span class="line"># 均值</span><br><span class="line">mean &#x3D; np.mean(S)</span><br><span class="line">print(mean)  # 3.5</span><br><span class="line"></span><br><span class="line"># 离差 &#x3D; 观测值 - 均值</span><br><span class="line">deviation &#x3D; S - mean</span><br><span class="line">print(deviation)  # [-2.5 -1.5 -0.5  0.5  1.5  2.5]</span><br><span class="line"></span><br><span class="line"># 离差方 &#x3D; 离差 ** 2</span><br><span class="line">deviation_square &#x3D; deviation ** 2</span><br><span class="line">print(deviation_square)  # [6.25 2.25 0.25 0.25 2.25 6.25]</span><br><span class="line"></span><br><span class="line"># 方差 &#x3D; 离差方的均值</span><br><span class="line">variance &#x3D; np.mean(deviation_square)</span><br><span class="line">print(variance)  # 2.9166666666666665</span><br><span class="line"></span><br><span class="line"># 标准差 &#x3D; 方差的平方根</span><br><span class="line">std &#x3D; np.sqrt(variance)</span><br><span class="line">print(std)  # 1.707825127659933</span><br></pre></td></tr></table></figure>

<h4 id="23-方差和标准差有什么区别？"><a href="#23-方差和标准差有什么区别？" class="headerlink" title="23.方差和标准差有什么区别？"></a>23.方差和标准差有什么区别？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">方差和标准差都可以用来衡量数据的离散程度</span><br><span class="line">区别：</span><br><span class="line">1.计算方法不同</span><br><span class="line">2.单位不同：标准差单位与数据的单位一致，因此更直观易理解</span><br><span class="line"></span><br><span class="line">方差和标准差的缺点：</span><br><span class="line">1.对异常值比较敏感</span><br><span class="line">2.只能衡量单个变量的离散程度，不能反映变量之间的关系</span><br><span class="line">3.数据需要满足正态分布</span><br></pre></td></tr></table></figure>

<h4 id="24-回归问题的模型评估指标"><a href="#24-回归问题的模型评估指标" class="headerlink" title="24.回归问题的模型评估指标"></a>24.回归问题的模型评估指标</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">1.均方误差(Mean Squared Error, MSE)</span><br><span class="line">  公式：MSE &#x3D; Σ(y_i - y&#39;_i)^2 &#x2F; n</span><br><span class="line">  取值：越小越好</span><br><span class="line">  特点：L2范数，对离群值敏感，因为平方放大了误差</span><br><span class="line"></span><br><span class="line">2.均方根误差（Root Mean Squared Error, RMSE）</span><br><span class="line">  公式：RMSE &#x3D; √MSE</span><br><span class="line">  取值：越小越好</span><br><span class="line">  特点：RMSE是MSE的平方根，其单位与原始数据相同，便于描述真实值。也对离群值敏感</span><br><span class="line">  </span><br><span class="line">3.平均绝对误差（Mean Absolute Error, MAE）</span><br><span class="line">  公式：MAE &#x3D; Σ|y_i - y&#39;_i| &#x2F; n</span><br><span class="line">  取值：越小越好</span><br><span class="line">  特点：L1范数，对离群值不敏感</span><br><span class="line">  </span><br><span class="line">4.决定系数&#x2F;拟合优度（R² 或 Coefficient of Determination）</span><br><span class="line">  解释：表示模型中自变量能解释因变量变异的百分比</span><br><span class="line">  公式：R2 &#x3D; 1 - MSE &#x2F; Variance</span><br><span class="line">  取值：0～1之间，越大越好</span><br><span class="line">  特点：仅表示拟合程度，不代表模型预测准确度</span><br></pre></td></tr></table></figure>

<h4 id="25-分类问题中的TP、FP、TN、FN是什么"><a href="#25-分类问题中的TP、FP、TN、FN是什么" class="headerlink" title="25.分类问题中的TP、FP、TN、FN是什么"></a>25.分类问题中的TP、FP、TN、FN是什么</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">TP：True Positive，正确得预测为正样本，实际就是正样本，即正样本被正确识别的数量</span><br><span class="line">FP：False Positive，错误得预测为正样本，实际为负样本，即误报的数量</span><br><span class="line">TN：True Negative，正确得预测为负样本，实际就是负样本，即负样本被正确识别的数量</span><br><span class="line">FN：False Negative，错误得预测为负样本，实际为正样本，即漏报的数量</span><br><span class="line"></span><br><span class="line">TP+FN：真实正样本的数量</span><br><span class="line">FP+TN：真实负样本的数量</span><br><span class="line">TP+FP：预测为正样本的数量</span><br><span class="line">TN+FN: 预测为负样本的数量</span><br><span class="line">TP+TN: 预测正确的数量</span><br><span class="line">TP+TN+FP+FN: 样本总数量</span><br></pre></td></tr></table></figure>

<h4 id="26-如何查看混淆矩阵"><a href="#26-如何查看混淆矩阵" class="headerlink" title="26.如何查看混淆矩阵"></a>26.如何查看混淆矩阵</h4><table>
<thead>
<tr>
<th></th>
<th>Real A</th>
<th>Real B</th>
</tr>
</thead>
<tbody><tr>
<td>Predict A</td>
<td>10(TP)</td>
<td>20(FP)</td>
</tr>
<tr>
<td>Predict B</td>
<td>30(FN)</td>
<td>5(TN)</td>
</tr>
</tbody></table>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">该类的预测总数：某一行的和</span><br><span class="line">该类的真实总数：某一列的和</span><br><span class="line">预测对的数量：主对角线上的值</span><br><span class="line"></span><br><span class="line">以A为例：</span><br><span class="line">    TP: 预测为A，实际也为A &#x3D; 10</span><br><span class="line">    FP: 预测为A，实际不是A &#x3D; 20</span><br><span class="line">    TN: 预测为B，实际也为B &#x3D; 5</span><br><span class="line">    FN: 预测为B，实际为A &#x3D; 30 </span><br><span class="line">    Accuracy(和某个类别无关)：预测正确的数量 &#x2F; 样本总数量 &#x3D; 10 + 5 &#x2F; 10 + 20 + 30 + 5</span><br><span class="line">    Precision(A)：正确预测为A的数量 &#x2F; 预测为A的数量（行） &#x3D; 10 &#x2F; 10 + 20</span><br><span class="line">    Recall(A): 正确预测为A的数量 &#x2F; 真实为A的数量（列） &#x3D; 10 &#x2F; 10 + 30</span><br></pre></td></tr></table></figure>

<h4 id="27-分类问题的模型评估指标"><a href="#27-分类问题的模型评估指标" class="headerlink" title="27.分类问题的模型评估指标"></a>27.分类问题的模型评估指标</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">注意：每个类别都有自己的查准率、召回率、f1得分</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">假设有100张图片，50张狗（正样本），50张猫（负样本），模型预测结果为60张狗（其中有40张是正确的，还有20张是猫）、40张猫（10张狗 + 30张猫）</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">1.准确率（Accuracy）</span><br><span class="line">  公式：(TP+TN) &#x2F; (TP+TN+FP+FN)</span><br><span class="line">  解释：预测正确的数量 &#x2F; 样本总数量 &#x3D; (40 + 30) &#x2F; 100 &#x3D; 0.7</span><br><span class="line">  特点：如果样本不平衡，则准确率就没有参考价值</span><br><span class="line"></span><br><span class="line">2.查准率（Precision）</span><br><span class="line">  公式：TP &#x2F; (TP+FP) &#x3D; 40 &#x2F; 60 &#x3D; 0.67</span><br><span class="line">  解释：正确预测为正样本的数量 &#x2F; 预测为正样本的数量。Precision越高，表示FP越小，即误报越少</span><br><span class="line">  </span><br><span class="line">3.召回率&#x2F;查全率（Recall）</span><br><span class="line">  公式：TP &#x2F; (TP+FN) &#x3D; 40 &#x2F; 50 &#x3D; 0.8</span><br><span class="line">  解释：正确预测为正样本的数量 &#x2F; 真实正样本的数量。Recall越高，表示FN越小，即漏报越少</span><br><span class="line">  </span><br><span class="line">4.F1分数（F1 Score）</span><br><span class="line">  公式：F1 &#x3D; 2 * Precision * Recall &#x2F; (Precision + Recall)</span><br><span class="line">  解释：F1分数是查准率和召回率的调和平均数，在两者之间取得平衡。适用于需要在查准率和召回率之间权衡的场景。</span><br><span class="line"> </span><br><span class="line">5.混淆矩阵（Confusion Matrix）</span><br><span class="line"> </span><br><span class="line">6.ROC和AUC</span><br><span class="line">  ROC 曲线以FPR为横坐标，以TPR为纵坐标，连接不同阈值下的点绘制而成。</span><br><span class="line">    真正率（TPR）&#x3D; 灵敏度 &#x3D; Recall &#x3D; TP&#x2F;(TP+FN)</span><br><span class="line">    假正率（FPR） &#x3D; 1- 特异度 &#x3D; FP&#x2F;(FP+TN)，代表有多少负样本被错误得预测成了正样本</span><br><span class="line">  ROC 曲线越靠近左上角，说明模型性能越好</span><br><span class="line">  </span><br><span class="line">  AUC是 ROC 曲线下的面积，其值介于 0 和 1 之间，值越大，说明模型性能越好</span><br><span class="line">  </span><br><span class="line">  特点：可以避免样本不平衡的问题，因为TPR只关注正样本，FPR只关注负样本</span><br></pre></td></tr></table></figure>

<h4 id="28-回归问题的损失函数，为何使用平方（MSE）而不是绝对值（MAE）？"><a href="#28-回归问题的损失函数，为何使用平方（MSE）而不是绝对值（MAE）？" class="headerlink" title="28.回归问题的损失函数，为何使用平方（MSE）而不是绝对值（MAE）？"></a>28.回归问题的损失函数，为何使用平方（MSE）而不是绝对值（MAE）？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">因为MSE函数是可微的，而MAE函数不可微，具体的：</span><br><span class="line">1.曲线最低点可导</span><br><span class="line">2.越接近最低点，曲线的坡度逐渐放缓，有助于通过当前的梯度来判断接近最低点的程度</span><br></pre></td></tr></table></figure>
<p><img src="../images/ml_review_mse_loss.png" alt="mse_loss"></p>
<h4 id="29-损失函数和评估函数（指标）的区别？"><a href="#29-损失函数和评估函数（指标）的区别？" class="headerlink" title="29.损失函数和评估函数（指标）的区别？"></a>29.损失函数和评估函数（指标）的区别？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">损失函数作用于训练集，为梯度下降提供方向，用来训练模型参数</span><br><span class="line">评估函数（指标）作用于验证集和测试集，用于评估模型</span><br></pre></td></tr></table></figure>

<h4 id="30-什么是超参数？有哪些常用的超参数调优手段？"><a href="#30-什么是超参数？有哪些常用的超参数调优手段？" class="headerlink" title="30.什么是超参数？有哪些常用的超参数调优手段？"></a>30.什么是超参数？有哪些常用的超参数调优手段？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">超参数是在训练之前，人为预习设定的参数，而不是在训练中获得的参数</span><br><span class="line"></span><br><span class="line">调优手段：</span><br><span class="line">1.网格搜索（Grid Search）</span><br><span class="line">  定义：在预定义的超参数空间内进行穷举搜索，尝试所有可能的组合</span><br><span class="line">  特点：实现简单，但时间长，计算开销大</span><br><span class="line">  </span><br><span class="line">2.随机搜索（Random Search）</span><br><span class="line">  定义：从预定义的超参数空间内随机选择超参数组合进行评估</span><br><span class="line">  特点：相较网格搜索更高效，但可能无法找到最优组合</span><br><span class="line">  </span><br><span class="line">3.贝叶斯优化（Bayesian Optimization）</span><br><span class="line">  定义：一种基于贝叶斯定理的调优方法。它会根据已评估的超参数组合来预测新的超参数组合。</span><br><span class="line">  特点：比网格搜索和随机搜索更有效，但需要更多的计算时间 </span><br><span class="line">  </span><br><span class="line">4.遗传算法（Genetic Algorithms）</span><br><span class="line">  定义：使用自然选择等生物进化的思想来优化超参数</span><br></pre></td></tr></table></figure>

<h4 id="31-有哪些常见的超参数？各自对模型有怎样的影响？"><a href="#31-有哪些常见的超参数？各自对模型有怎样的影响？" class="headerlink" title="31.有哪些常见的超参数？各自对模型有怎样的影响？"></a>31.有哪些常见的超参数？各自对模型有怎样的影响？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">1.学习率（Learning Rate）</span><br><span class="line">  定义：学习率控制着模型参数更新的步长</span><br><span class="line">  影响：学习率太大导致无法收敛，太小导致收敛缓慢</span><br><span class="line">  </span><br><span class="line">2.批量大小（Batch Size）</span><br><span class="line">  定义：在一次迭代中使用的训练样本数量</span><br><span class="line">  影响：较大的batch size可以提高训练速度，但会占用更大的内存和计算资源，较小的则可能导致收敛不稳定</span><br><span class="line">  </span><br><span class="line">3.正则化参数（Regularization Parameter）</span><br><span class="line">  定义：用于控制模型的复杂度，以避免过拟合</span><br><span class="line">  影响：太大可能欠拟合，太小可能过拟合</span><br><span class="line">  </span><br><span class="line">4.隐藏层数量和神经元数量（Number of Hidden Layers and Neurons）</span><br><span class="line">  定义：决定了神经网络的容量和表达能力</span><br><span class="line">  影响：更多的隐藏层和神经元可以捕捉更复杂的模式，但也增加了模型复杂度和过拟合的风险</span><br><span class="line">  </span><br><span class="line">5.迭代次数（Epochs）</span><br><span class="line">  定义：模型训练的轮数</span><br><span class="line">  影响：过少可能导致模型没有学习到数据的特征，过多可能导致过拟合</span><br><span class="line">  </span><br><span class="line">6.优化器（Optimizer）</span><br><span class="line">  定义：优化器决定了模型的权重更新策略</span><br><span class="line">  影响：不同优化器有不同的收敛速度和稳定性，对模型的最终性能影响显著</span><br><span class="line">  </span><br><span class="line">7.权重初始化（Weight Initialization）</span><br><span class="line">  定义：权重初始化影响模型的初始状态，从而影响训练收敛速度和结果</span><br><span class="line">  影响：常见的方法有随机初始化、Xavier初始化和He初始化等。选择合适的初始化方法可以加快收敛速度，避免梯度消失或梯度爆炸</span><br></pre></td></tr></table></figure>

<h4 id="32-什么是置信概率？"><a href="#32-什么是置信概率？" class="headerlink" title="32. 什么是置信概率？"></a>32. 什么是置信概率？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">置信概率指模型对某个预测结果的确信程度，通常，分类模型（如逻辑回归、神经网络、随机森林等）在进行预测时，不仅给出一个类别标签，还会输出每个类别的置信概率。</span><br><span class="line">值越大说明越确定</span><br></pre></td></tr></table></figure>

<h4 id="33-什么是交叉验证？它有哪些常见类型？"><a href="#33-什么是交叉验证？它有哪些常见类型？" class="headerlink" title="33.什么是交叉验证？它有哪些常见类型？"></a>33.什么是交叉验证？它有哪些常见类型？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">交叉验证（Cross-Validation）是一种模型性能评估技术，在样本数量较少的情况下，它将数据集分成多份，每份轮流作为测试集，剩下部分作为训练集，通过这种方式，可以多次评估模型，每次的评估结果综合起来给出模型的总体性能</span><br><span class="line"></span><br><span class="line">常见类型：</span><br><span class="line">1.K折交叉验证（K-Fold CV）</span><br><span class="line">  定义: 将数据集随机分成k个子集（或称为“折”），然后进行k次训练和测试。每次用k-1个子集进行训练，用剩下的一个子集进行测试</span><br><span class="line">  </span><br><span class="line">2.留一法交叉验证（Leave-One-Out Cross-Validation, LOOCV）</span><br><span class="line">  定义: 留一法交叉验证是一种特殊的 K 折交叉验证，其中 k 等于样本数。在留一法交叉验证中，每次只使用一个样本作为测试集，其余样本作为训练集</span><br><span class="line">  特点：最大限度利用数据，但计算量大</span><br></pre></td></tr></table></figure>

<h4 id="34-对于类别不均衡问题，有哪些处理方法？"><a href="#34-对于类别不均衡问题，有哪些处理方法？" class="headerlink" title="34.对于类别不均衡问题，有哪些处理方法？"></a>34.对于类别不均衡问题，有哪些处理方法？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">数据层面：</span><br><span class="line">1.过采样（Oversampling）：增加少数类样本的数量</span><br><span class="line">2.欠采样（Undersampling）：减少多数类样本的数量</span><br><span class="line">3.合成数据（Synthetic Data Generation）：为少数类样本合成新的数据</span><br><span class="line"></span><br><span class="line">算法层面：</span><br><span class="line">1.调整类权重（Class Weight Adjustment）：给少数类样本赋予更高的权重，以增加其对损失函数的影响</span><br><span class="line">2.集成方法（Ensemble Methods）：使用集成学习方法，如Bagging、Boosting来提高模型对少数类的识别能力</span><br><span class="line">3.调整评估指标：使用适合不平衡数据集的评估指标，如PR曲线、F1、ROC、AUC</span><br></pre></td></tr></table></figure>

<h4 id="35-神经网络权重初始值如何设置？"><a href="#35-神经网络权重初始值如何设置？" class="headerlink" title="35.神经网络权重初始值如何设置？"></a>35.神经网络权重初始值如何设置？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1.不能使用零初始化</span><br><span class="line">2.随机初始化：对于小型网络，可以使用高斯分布或均匀分布来初始化权重，这有助于打破网络的对称性</span><br><span class="line">3.Xavier初始化：又称为Glorot初始化，该方法将权重的方差初始化为1&#x2F;输入特征数，当激活函数为sigmoid&#x2F;tanh时，适合用这个</span><br><span class="line">4.He初始化：又称为Delving初始化，该方法将权重的方差初始化为2&#x2F;输入特征数，当激活函数为RELU时，适合用这个</span><br></pre></td></tr></table></figure>

<h4 id="36-什么是线性回归？线性回归的特点是什么？"><a href="#36-什么是线性回归？线性回归的特点是什么？" class="headerlink" title="36.什么是线性回归？线性回归的特点是什么？"></a>36.什么是线性回归？线性回归的特点是什么？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">定义：用于分析两个或多个变量之间的关系的机器学习方法。它通过拟合一条直线来表示自变量和因变量之间的线性关系</span><br><span class="line">公式：y &#x3D; w0 + w1 * x1 + w2 * x2 + ... + wn * xn + b</span><br><span class="line">特点：容易计算和实现、难以很好地表达非线性的数据</span><br></pre></td></tr></table></figure>

<h4 id="37-什么是多项式回归？多项式回归的特点是什么？"><a href="#37-什么是多项式回归？多项式回归的特点是什么？" class="headerlink" title="37.什么是多项式回归？多项式回归的特点是什么？"></a>37.什么是多项式回归？多项式回归的特点是什么？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">定义：多项式回归是一种扩展的线性回归模型，用于处理自变量与因变量之间的非线性关系。虽然模型仍然是线性模型（因为参数是线性的），但它引入了自变量的多项式来捕捉更复杂的模式</span><br><span class="line">公式：y &#x3D; w0 + w1 * x + w2 * x^2 + ... + wn * x^n + b</span><br><span class="line">特点：可以拟合非线性关系</span><br></pre></td></tr></table></figure>

<h4 id="38-什么是决策树？工作原理是什么？特点？"><a href="#38-什么是决策树？工作原理是什么？特点？" class="headerlink" title="38.什么是决策树？工作原理是什么？特点？"></a>38.什么是决策树？工作原理是什么？特点？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">定义：通过树形结构来表示决策过程，每个节点表示一个属性，每个分支表示一个属性值，叶节点表示一个类或决策结果</span><br><span class="line"></span><br><span class="line">工作原理：</span><br><span class="line">1.选择特征：从数据集中选择一个特征进行分割。常用的标准包括信息增益、信息增益率、基尼指数等。</span><br><span class="line">2.分割数据：根据选择的特征，将数据分割成不同的子集。</span><br><span class="line">3.递归构建：对每个子集重复上述过程，直到满足停止条件，如达到最大深度或子集中没有足够的数据点。</span><br><span class="line">4.形成叶节点：当达到停止条件时，将当前节点设为叶节点，并分配一个预测值。</span><br><span class="line"></span><br><span class="line">特点：</span><br><span class="line">易于理解和解释、不需要预处理数据、可用于分类和回归任务</span><br><span class="line">受数据影响大（高方差），容易出现过拟合（特征多，参数复杂，缺乏正则化手段）</span><br></pre></td></tr></table></figure>

<h4 id="39-有哪些常见的决策树算法？"><a href="#39-有哪些常见的决策树算法？" class="headerlink" title="39.有哪些常见的决策树算法？"></a>39.有哪些常见的决策树算法？</h4><p><img src="../images/ml_review_decision_tree.png" alt="decision_tree"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ID3：基于信息增益来选择分裂属性（每步选择信息增益最大的属性作为分裂节点，树可能是多叉的）。</span><br><span class="line">C4.5：基于信息增益率来选择分裂属性（每步选择信息增益率最大的属性作为分裂节点，树可能是多叉的）。</span><br><span class="line">CART(Classification And Regression Tree)：基于基尼系数&#x2F;均方差来构建决策树（每步要求基尼系数最小，树是二叉的</span><br></pre></td></tr></table></figure>

<h4 id="40-CART-在分类问题和回归问题中的异同"><a href="#40-CART-在分类问题和回归问题中的异同" class="headerlink" title="40.CART 在分类问题和回归问题中的异同"></a>40.CART 在分类问题和回归问题中的异同</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">相同：</span><br><span class="line">• 在分类问题和回归问题中，CART 都是一棵二叉树，除叶子节点外的所有节点都有且仅有两个子节点；</span><br><span class="line">• 所有落在同一片叶子中的输入都有同样的输出。</span><br><span class="line"></span><br><span class="line">不同：</span><br><span class="line">• 在分类问题中，CART 使用基尼指数（Gini index）作为选择特征（feature）和划分（split）的依据；在回归问题中，CART 使用 mse（mean square error）或者 mae（mean absolute error）作为选择 feature 和 split 的 criteria。</span><br><span class="line">• 在分类问题中，CART 的每一片叶子都代表的是一个 class；在回归问题中，CART 的每一片叶子表示的是一个预测值，取值是连续的。</span><br></pre></td></tr></table></figure>

<h4 id="41-什么是集成学习？"><a href="#41-什么是集成学习？" class="headerlink" title="41.什么是集成学习？"></a>41.什么是集成学习？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">集成学习就是组合多个弱监督模型，以期望得到一个更好的强监督模型。集成学习潜在的思想是即便某一个弱分类器得到了错误的预测，其他分类器也可以将错误纠正回来</span><br><span class="line">类型：Bagging、Boosting、Stacking</span><br><span class="line">关键点：</span><br><span class="line">  1.弱学习器不能太“弱”，需要有一定的准确性</span><br><span class="line">  2.弱学习器之间要具有“多样性”，即弱学习器之间存在差异性</span><br></pre></td></tr></table></figure>

<h4 id="42-Bagging与Boosting的原理是什么？二者有何区别？"><a href="#42-Bagging与Boosting的原理是什么？二者有何区别？" class="headerlink" title="42.Bagging与Boosting的原理是什么？二者有何区别？"></a>42.Bagging与Boosting的原理是什么？二者有何区别？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Bagging: 利用bootstrap方法从整体数据集中采取有放回抽样得到N个数据集，在每个数据集上学习出一个模型，最后的预测结果利用N个模型的输出得到；具体地：分类问题采用N个模型预测投票的方式，回归问题采用N个模型预测平均的方式。通过随机抽取数据的方式减少了可能的数据干扰，因此Bagging模型具有低方差</span><br><span class="line">Boosting: 思路是逐步优化模型，持续地通过新模型来优化同一个基模型，从而不断减小模型的预测误差（偏差）</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">区别：</span><br><span class="line">1.Bagging中每个训练集互不相关，也就是每个基分类器互不相关，而Boosting中训练集要在上一轮的结果上进行调整，也使得其不能并行计算；</span><br><span class="line">2.Bagging中预测函数是均匀平等的，但在Boosting中预测函数是加权的</span><br><span class="line">3.从偏差-方差分解角度看，Bagging主要关注降低方差，而Boosting主要关注降低偏差</span><br></pre></td></tr></table></figure>

<h4 id="43-机器学习模型的偏差和方差是什么？"><a href="#43-机器学习模型的偏差和方差是什么？" class="headerlink" title="43.机器学习模型的偏差和方差是什么？"></a>43.机器学习模型的偏差和方差是什么？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">偏差：评判的是机器学习模型的准确度，偏差越小，模型越准确。它度量了算法的预测与真实结果的离散程度，刻画了学习算法本身的拟合能力。也就是每次打靶都比较准，比较靠近靶心。</span><br><span class="line">方差：评判的是机器学习模型的稳定性(或称精度)，方差越小，模型越稳定。它度量了训练集变动所导致的学习性能变化，刻画了数据扰动所造成的影响。也就是每次打靶，不管打得准不准，击中点都比较集中</span><br></pre></td></tr></table></figure>

<h4 id="44-什么是基于决策树的集合算法-集成学习-？"><a href="#44-什么是基于决策树的集合算法-集成学习-？" class="headerlink" title="44.什么是基于决策树的集合算法(集成学习)？"></a>44.什么是基于决策树的集合算法(集成学习)？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">基于决策树的集合算法，就是按照某种规则，构建多棵彼此不同的决策树模型，分别给出针对未知样本的预测结果，最后通过平均或投票得到相对综合的结论。常用的集合模型包括Boosting类模型（AdaBoost、GBDT）与Bagging（自助聚合、随机森林）类模型。</span><br></pre></td></tr></table></figure>

<h4 id="45-简要介绍AdaBoost、GBDT、XGBoost"><a href="#45-简要介绍AdaBoost、GBDT、XGBoost" class="headerlink" title="45.简要介绍AdaBoost、GBDT、XGBoost"></a>45.简要介绍AdaBoost、GBDT、XGBoost</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">AdaBoost: 它通过对多个弱分类器进行加权组合来提高最终分类器的性能，具体的：在每一轮迭代中，样本的权重会根据前一轮的分类结果进行调整。分类错误的样本权重会增加，分类正确的样本权重会减小，以使后续的分类器更关注前一轮分类错误的样本</span><br><span class="line">GBDT: Gradient Boosting Decision Tree, 它通过构造一系列的决策树，每一棵树都会针对前一棵树的残差进行优化，以此来减少整个模型的误差。</span><br><span class="line">XGBoost: 是GBDT的改进版本，使用了二阶导数来优化决策树的划分点、添加正则化项、支持并行计算，来提升模型的性能</span><br></pre></td></tr></table></figure>

<h4 id="46-什么是随机森林？"><a href="#46-什么是随机森林？" class="headerlink" title="46.什么是随机森林？"></a>46.什么是随机森林？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RF是一种集成学习算法，在以决策树作为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中加入了随机属性的选择,由此，随机森林的基学习器的“多样性”不仅来自样本的扰动，还来自属性的扰动，使得最终集成的泛化能力进一步增强</span><br></pre></td></tr></table></figure>

<h4 id="47-什么是逻辑回归？它是如何实现二分类的？"><a href="#47-什么是逻辑回归？它是如何实现二分类的？" class="headerlink" title="47.什么是逻辑回归？它是如何实现二分类的？"></a>47.什么是逻辑回归？它是如何实现二分类的？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">逻辑回归是一种广义的线性回归，其原理是利用线性模型根据输入计算输出，然后在逻辑函数（sigmoid）和阈值作用下，将连续值转换为两个离散值（0或1），从而实现二分类</span><br></pre></td></tr></table></figure>

<h4 id="48-什么是逻辑函数（sigmoid）？它有什么特点？"><a href="#48-什么是逻辑函数（sigmoid）？它有什么特点？" class="headerlink" title="48.什么是逻辑函数（sigmoid）？它有什么特点？"></a>48.什么是逻辑函数（sigmoid）？它有什么特点？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">sigmoid函数能将(−∞,+∞)的值映射到(0,1)之间，通过选取合适的阈值转换为两个</span><br><span class="line">离散值，从而实现二分类</span><br><span class="line">公式：f(x) &#x3D; 1 &#x2F; (1 + exp(-x))</span><br><span class="line">导数：f&#39;(x) &#x3D; f(x) * (1 - f(x))</span><br><span class="line"></span><br><span class="line">代码实现：</span><br><span class="line">def sigmoid(x):</span><br><span class="line">    return 1 &#x2F; (1 + np.exp(-x))</span><br><span class="line">    </span><br><span class="line">特点：</span><br><span class="line">1.函数可微</span><br><span class="line">2.输出范围为(0,1)，可以解释为概率，为分类做准备</span><br><span class="line">3.单调递增：输入值越大，输出值越接近1；输入值越小，输出值越接近0</span><br></pre></td></tr></table></figure>

<h4 id="49-什么是信息熵？"><a href="#49-什么是信息熵？" class="headerlink" title="49.什么是信息熵？"></a>49.什么是信息熵？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">信息熵（information entropy）是度量样本集合纯度的常用指标，该值越大，表示该集合纯度越低（或越混乱），该值越小，表示该集合纯度越高（或越有序）</span><br></pre></td></tr></table></figure>

<h4 id="50-什么是交叉熵损失函数？"><a href="#50-什么是交叉熵损失函数？" class="headerlink" title="50.什么是交叉熵损失函数？"></a>50.什么是交叉熵损失函数？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">交叉熵（Cross Entropy Loss）是一种在机器学习和深度学习中广泛使用的损失函数，主要用来衡量真实概率与预测概率之间的差异</span><br><span class="line">二分类公式：−(ylog(p)+(1−y)log(1−p))</span><br><span class="line">多分类公式：- ∑(y(i) * log(p(i)))</span><br><span class="line">公式解释：y为真实标签（0或1），p为模型输出的预测值，i为第i个类别</span><br><span class="line"></span><br><span class="line">代码实现：</span><br><span class="line">def cross_entropy_error(p, y):</span><br><span class="line">    delta &#x3D; 1e-7 # 防止当出现np.log(0)时，np.log(0)会变为负无限大</span><br><span class="line">    return -np.sum(y * np.log(p + delta))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 神经网络版</span><br><span class="line">def cross_entropy_error(y, t):</span><br><span class="line">    if y.ndim &#x3D;&#x3D; 1:</span><br><span class="line">        t &#x3D; t.reshape(1, t.size)</span><br><span class="line">        y &#x3D; y.reshape(1, y.size)</span><br><span class="line"></span><br><span class="line">    # 在监督标签为one-hot-vector的情况下，转换为正确解标签的索引</span><br><span class="line">    if t.size &#x3D;&#x3D; y.size:</span><br><span class="line">        t &#x3D; t.argmax(axis&#x3D;1)</span><br><span class="line"></span><br><span class="line">    batch_size &#x3D; y.shape[0]</span><br><span class="line"></span><br><span class="line">    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) &#x2F; batch_size</span><br></pre></td></tr></table></figure>

<h4 id="51-什么是朴素贝叶斯分类？特点是什么？何时使用？"><a href="#51-什么是朴素贝叶斯分类？特点是什么？何时使用？" class="headerlink" title="51.什么是朴素贝叶斯分类？特点是什么？何时使用？"></a>51.什么是朴素贝叶斯分类？特点是什么？何时使用？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">朴素贝叶斯分类（Naive Bayes Classifier）是一种基于贝叶斯定理的概率分类方法。它假设特征之间是相互独立的，这一假设称为“朴素”假设。</span><br><span class="line"></span><br><span class="line">原理：</span><br><span class="line">1.对于给定的待分类样本，计算它属于每个类的后验概率。</span><br><span class="line">2.选择具有最大后验概率的类作为该样本的预测类别</span><br><span class="line"></span><br><span class="line">特点：</span><br><span class="line">1.简单易实现</span><br><span class="line">2.计算效率高</span><br><span class="line">3.独立性假设不现实：特征之间往往存在相关性，这一假设在许多情况下并不成立</span><br><span class="line"></span><br><span class="line">何时使用：</span><br><span class="line">根据先验概率计算后验概率的情况，且样本特征之间独立性较强</span><br></pre></td></tr></table></figure>

<h4 id="52-常见的朴素贝叶斯分类器有哪些？"><a href="#52-常见的朴素贝叶斯分类器有哪些？" class="headerlink" title="52.常见的朴素贝叶斯分类器有哪些？"></a>52.常见的朴素贝叶斯分类器有哪些？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">高斯朴素贝叶斯（Gaussian Naive Bayes）：假设特征服从高斯分布，常用于连续数据</span><br><span class="line">多项式朴素贝叶斯（Multinomial Naive Bayes）：适用于离散数据，常用于文本分类</span><br><span class="line">伯努利朴素贝叶斯（Bernoulli Naive Bayes）：适用于二元离散值或是稀疏的多元离散值</span><br></pre></td></tr></table></figure>

<h4 id="53-什么是支持向量机？"><a href="#53-什么是支持向量机？" class="headerlink" title="53.什么是支持向量机？"></a>53.什么是支持向量机？</h4><p><img src="../images/ml_review_svm.png" alt="svm"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">支持向量机（Support Vector Machine, SVM）是一种用于分类和回归的监督学习模型。它的基本思想是找到一个能够最大程度分离不同类别数据的超平面</span><br><span class="line"></span><br><span class="line">超平面（Hyperplane）：在n维空间中的一个具有n-1维的几何结构</span><br><span class="line">间隔（Margin）：是指超平面与离它最近的训练样本之间的距离。SVM通过最大化这个间隔来实现最优分类</span><br><span class="line">支持向量（Support Vectors）：是指位于边界上或边界附近的训练样本点。这些点对定义超平面的位置起关键作用</span><br></pre></td></tr></table></figure>

<h4 id="54-SVM寻找最优边界要求有哪些？"><a href="#54-SVM寻找最优边界要求有哪些？" class="headerlink" title="54.SVM寻找最优边界要求有哪些？"></a>54.SVM寻找最优边界要求有哪些？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">（1）正确性：对大部分样本都可以正确划分类别；</span><br><span class="line">（2）安全性：离支持向量的距离最远；</span><br><span class="line">（3）公平性：支持向量与分类边界的距离相等；</span><br><span class="line">（4）简单性：采用线性方程表示分类边界。如果在原始维度中无法做线性划分，那么就通过升维变换，在更高维度空间寻求线性分割超平面</span><br></pre></td></tr></table></figure>

<h4 id="55-SVM的工作原理？"><a href="#55-SVM的工作原理？" class="headerlink" title="55.SVM的工作原理？"></a>55.SVM的工作原理？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">线性可分情况：如果数据可以用一个直线（或超平面）完全分开，SVM会找到那个使得两个类别之间的间隔最大的超平面</span><br><span class="line">线性不可分情况：SVM通过引入软间隔（Soft Margin）和核函数（Kernel Function）来处理。软间隔允许一些数据点位于错误的一侧，但通过引入惩罚项来最小化错误分类的影响。核函数则通过将数据映射到高维空间，使得在高维空间中数据变得线性可分</span><br></pre></td></tr></table></figure>

<h4 id="56-SVM中的核函数是什么？常用的核函数有哪些？"><a href="#56-SVM中的核函数是什么？常用的核函数有哪些？" class="headerlink" title="56.SVM中的核函数是什么？常用的核函数有哪些？"></a>56.SVM中的核函数是什么？常用的核函数有哪些？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">主要作用是将原始特征空间中的数据映射到一个更高维的空间，使得在这个高维空间中，数据线性可分</span><br><span class="line"></span><br><span class="line">常用：</span><br><span class="line">1.线性核函数（Linear Kernel）：实际上没有进行升维，直接在原始空间中进行计算，适用于线性可分的数据</span><br><span class="line">2.多项式核函数（Polynomial Kernel）：用增加高次项特征的方法做升维变换，当多项式阶数高时复杂度会很高</span><br><span class="line">3.径向基核函数（Radial Basis Function, RBF）：又称高斯核函数，通过计算样本之间的欧几里得距离来实现映射，灵活性好，比多项式核函数参数少</span><br></pre></td></tr></table></figure>

<h4 id="57-SVM的特点？"><a href="#57-SVM的特点？" class="headerlink" title="57.SVM的特点？"></a>57.SVM的特点？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">优点：</span><br><span class="line">有严格的数学理论支持，可解释性强</span><br><span class="line">采用核函数之后，可以处理非线性分类&#x2F;回归任务</span><br><span class="line"></span><br><span class="line">缺点：</span><br><span class="line">训练时间长</span><br><span class="line">样本较多时，效率不高，故只适合小批量样本的任务</span><br></pre></td></tr></table></figure>

<h4 id="58-什么是聚类？"><a href="#58-什么是聚类？" class="headerlink" title="58.什么是聚类？"></a>58.什么是聚类？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">聚类是一种无监督学习方法，根据数据集中样本相似性，将它们分到不同的簇中，同一个簇中的样本之间相似度较高，不同簇之间的样本相似度较低</span><br><span class="line"></span><br><span class="line">主要方法：</span><br><span class="line">原型聚类：K-means</span><br><span class="line">密度聚类：DBSCAN</span><br><span class="line">层次聚类：凝聚层次</span><br></pre></td></tr></table></figure>

<h4 id="59-有哪些常用的相似度度量方式？"><a href="#59-有哪些常用的相似度度量方式？" class="headerlink" title="59.有哪些常用的相似度度量方式？"></a>59.有哪些常用的相似度度量方式？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">1.欧氏距离（Euclidean Distance）：L2距离</span><br><span class="line">公式：d(a, b) &#x3D; √(Σ(xi - xj)²)</span><br><span class="line"></span><br><span class="line">2.曼哈顿距离（Manhattan Distance）：L1距离</span><br><span class="line">公式：d(a, b) &#x3D; Σ|xi - xj|</span><br><span class="line"></span><br><span class="line">3.切比雪夫距离（Chebyshev Distance）： L∞ 距离</span><br><span class="line">公式：d(a, b) &#x3D; max(|xi - xj|)</span><br><span class="line"></span><br><span class="line">4.余弦相似度（Cosine Similarity）：表示两个向量的夹角余弦值</span><br><span class="line">公式：cos(θ) &#x3D; a·b &#x2F; ||a|| ||b||</span><br><span class="line">解释：a·b是点积（dot product），||a|| 和 ||b|| 分别为 a 和 b 的欧氏模长，||a|| &#x3D; √(a1^2 + a2^2 + ... + an^2)</span><br><span class="line"></span><br><span class="line">5.杰卡德相似系数（Jaccard Similarity Coefficient）：表示两个集合的交集与并集之比（与目标检测中的IOU概念相同）</span><br><span class="line">公式：J(A, B) &#x3D; |A ∩ B| &#x2F; |A ∪ B|</span><br></pre></td></tr></table></figure>

<h4 id="60-聚类问题的评价指标是什么？"><a href="#60-聚类问题的评价指标是什么？" class="headerlink" title="60.聚类问题的评价指标是什么？"></a>60.聚类问题的评价指标是什么？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">轮廓系数（Silhouette Coefficient）：综合考虑簇内紧密程度和簇间分离程度来衡量聚类效果，取值[-1,1]，越接近1越好</span><br><span class="line">公式：s(i) &#x3D; (b(i) - a(i)) &#x2F; max(b(i),a(i))</span><br><span class="line">解释：</span><br><span class="line">  a(i) &#x3D; average(i向量到所有它属于的簇中其它点的距离)</span><br><span class="line">  b(i) &#x3D; min (i向量到各个非本身所在簇的所有点的平均距离)</span><br></pre></td></tr></table></figure>

<h4 id="61-什么是K-Means聚类？"><a href="#61-什么是K-Means聚类？" class="headerlink" title="61.什么是K-Means聚类？"></a>61.什么是K-Means聚类？</h4><p><img src="../images/ml_review_kmeans.png" alt="kmeans"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">K-Means聚类是一种基于原型的聚类算法，通过迭代的方式，将每个数据点分配到K个预定义的簇中，目标是最小化每个簇内点与簇中心的距离之和</span><br><span class="line"></span><br><span class="line">步骤：</span><br><span class="line">1.确定簇的数量K：首先决定要分成的簇的数量K。</span><br><span class="line">2.初始化质心：随机选择K个点作为初始质心。</span><br><span class="line">3.分配数据点：将每个数据点分配到最近的质心，形成K个簇。</span><br><span class="line">4.更新质心：计算每个簇的质心，即簇内所有数据点的平均值。</span><br><span class="line">5.重复步骤3和4：不断重新分配数据点并更新质心，直到质心不再变化或达到预定的迭代次数。</span><br><span class="line"></span><br><span class="line">优点：</span><br><span class="line">简单易理解</span><br><span class="line">计算效率高</span><br><span class="line"></span><br><span class="line">缺点：</span><br><span class="line">需要预先设定K值</span><br><span class="line">对初始质心敏感</span><br><span class="line"></span><br><span class="line">何时使用：</span><br><span class="line">已知K值、数据分布有明显的中心</span><br></pre></td></tr></table></figure>

<h4 id="62-什么是DBSCAN（噪声密度）？"><a href="#62-什么是DBSCAN（噪声密度）？" class="headerlink" title="62.什么是DBSCAN（噪声密度）？"></a>62.什么是DBSCAN（噪声密度）？</h4><p><img src="../images/ml_review_dbscan.png" alt="DBSCAN"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">DBSCAN是一种基于密度的聚类算法，用于发现数据中的簇和噪声。DBSCAN与K-Means不同，不需要预先指定簇的数量，并且能够有效处理噪声点</span><br><span class="line"></span><br><span class="line">主要概念：</span><br><span class="line">• 核心点（Core point）：一个点的邻域内包含的点数大于等于MinPts个点，该点被称为核心点；</span><br><span class="line">• 边界点（Border point）：一个点的邻域内包含的点数少于MinPts，但该点位于某个核心点的邻域内；</span><br><span class="line">• 噪声点（Noise）：既不是核心点，也不是边界点的点；</span><br><span class="line"></span><br><span class="line">步骤：</span><br><span class="line">1.初始化：选择一个未访问的样本点p。</span><br><span class="line">2.检查核心点：如果p的邻域内的样本点数目大于等于MinPts，则p是一个核心点，创建一个新的簇C，并将p及其邻域内的所有点加入簇C。</span><br><span class="line">3.扩展簇：对于簇C中的每个点q，如果q也是核心点，则将q的邻域内的所有点也加入簇C。重复此过程，直到簇C不再扩展。</span><br><span class="line">4.处理剩余点：选择下一个未访问的点，重复步骤2和3，直到所有点都被访问过</span><br><span class="line"></span><br><span class="line">优点：</span><br><span class="line">可自动确定簇的数量</span><br><span class="line">可以发现形状不规则的聚类</span><br><span class="line">能够有效处理噪声点</span><br><span class="line"></span><br><span class="line">缺点：</span><br><span class="line">参数敏感：邻域半径和最少样本数量两个参数对聚类结果影响较大</span><br><span class="line"></span><br><span class="line">何时使用：</span><br><span class="line">（1）数据没有明显中心</span><br><span class="line">（2）噪声数据较多</span><br><span class="line">（3）未知聚簇的数量</span><br></pre></td></tr></table></figure>

<h4 id="63-什么是凝聚层次算法？"><a href="#63-什么是凝聚层次算法？" class="headerlink" title="63.什么是凝聚层次算法？"></a>63.什么是凝聚层次算法？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">凝聚层次算法（Agglomerative Clustering）是一种基于层次的聚类方法，主要用于数据点或簇的逐步合并，以形成层次结构。其核心思想是从每个数据点作为一个独立的簇开始，然后逐步合并最相似的簇，直到满足某种终止条件，如达到预设的簇数量或所有数据点都在同一个簇中</span><br><span class="line"></span><br><span class="line">步骤：</span><br><span class="line">1.初始化：将每个数据点视为一个单独的簇。</span><br><span class="line">2.计算相似度：计算所有簇之间的相似度，通常基于距离度量。</span><br><span class="line">3.合并簇：选择最相似的两个簇进行合并，形成一个新的更大的簇。</span><br><span class="line">4.重复过程：重复上述步骤，直到满足终止条件</span><br><span class="line"></span><br><span class="line">优点：</span><br><span class="line">没有聚类中心，不依赖中心的选择</span><br><span class="line">不需要事先指定簇的数量K</span><br><span class="line"></span><br><span class="line">缺点：</span><br><span class="line">计算复杂度高</span><br><span class="line">对噪声敏感</span><br></pre></td></tr></table></figure>

<h4 id="64-什么是神经网络？它有哪些常见类型？"><a href="#64-什么是神经网络？它有哪些常见类型？" class="headerlink" title="64.什么是神经网络？它有哪些常见类型？"></a>64.什么是神经网络？它有哪些常见类型？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">神经网络是一种模拟生物大脑结构和功能的机器学习技术，它由多个人工神经元组成，可以学习复杂的模式并作出预测</span><br><span class="line"></span><br><span class="line">常见类型：</span><br><span class="line">前馈神经网络（Feedforward Neural Networks, FNN）： 信息单向流动的网络，没有循环或反馈，最基础的神经网络类型</span><br><span class="line">卷积神经网络（Convolutional Neural Networks, CNN）： 常用于图像和视频处理，具有卷积层和池化层，擅长提取局部特征</span><br><span class="line">循环神经网络（Recurrent Neural Networks, RNN）： 具有循环连接，能够处理序列数据，如时间序列和自然语言处理。LSTM和GRU是RNN的改进版本</span><br></pre></td></tr></table></figure>

<h4 id="65-神经网络中的权重和偏置是什么？"><a href="#65-神经网络中的权重和偏置是什么？" class="headerlink" title="65.神经网络中的权重和偏置是什么？"></a>65.神经网络中的权重和偏置是什么？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">权重：控制输入信号重要性</span><br><span class="line">偏置：控制神经元被激活的难易程度</span><br></pre></td></tr></table></figure>

<h4 id="66-深度学习的优缺点是什么？"><a href="#66-深度学习的优缺点是什么？" class="headerlink" title="66.深度学习的优缺点是什么？"></a>66.深度学习的优缺点是什么？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">优点：</span><br><span class="line">1.擅长处理复杂数据</span><br><span class="line">2.自动特征提取</span><br><span class="line">3.高精度高性能</span><br><span class="line"></span><br><span class="line">缺点：</span><br><span class="line">1.对数据质量要求高</span><br><span class="line">2.模型复杂</span><br><span class="line">3.难以解释</span><br></pre></td></tr></table></figure>

<h4 id="67-什么是激活函数？为什么要使用激活函数？"><a href="#67-什么是激活函数？为什么要使用激活函数？" class="headerlink" title="67.什么是激活函数？为什么要使用激活函数？"></a>67.什么是激活函数？为什么要使用激活函数？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">在神经网络中，将输入信号的总和转换为输出信号的函数被称为激活函数（activation function）</span><br><span class="line">激活函数可以引入非线性因素，增强模型的表达能力，选择合适的激活函数还可以避免梯度消失或梯度爆炸问题</span><br></pre></td></tr></table></figure>

<h4 id="68-神经网络中常用的激活函数有哪些，各自有什么特点？"><a href="#68-神经网络中常用的激活函数有哪些，各自有什么特点？" class="headerlink" title="68.神经网络中常用的激活函数有哪些，各自有什么特点？"></a>68.神经网络中常用的激活函数有哪些，各自有什么特点？</h4><p><img src="../images/ml_review_sigmoid.png" alt="sigmoid"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1.sigmoid: 又叫逻辑(Logistic)函数，能将(-∞, +∞)的数值映射到(0, 1)的区间，可以用来做二分类</span><br><span class="line">输出范围：(0, 1)</span><br><span class="line">公式：f(x) &#x3D; 1 &#x2F; (1 + exp(-x))</span><br><span class="line">导数：f&#39;(x) &#x3D; f(x) * (1 - f(x))</span><br><span class="line">优点：平滑、易于求导</span><br><span class="line">缺点：容易出现梯度消失</span><br></pre></td></tr></table></figure>
<p><img src="../images/ml_review_tanh.png" alt="tanh"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">2.tanh: 双曲正切函数</span><br><span class="line">输出范围：(-1, 1)</span><br><span class="line">公式：f(x) &#x3D; (1 - exp(-2x)) &#x2F; (1 + exp(-2x))</span><br><span class="line">导数：f&#39;(x) &#x3D; 1 - f(x)^2</span><br><span class="line">优点：比sigmoid收敛快</span><br><span class="line">缺点：仍有轻微梯度消失</span><br></pre></td></tr></table></figure>
<p><img src="../images/ml_review_relu_variant.png" alt="leaky relu variant"><br><img src="../images/ml_review_relu.png" alt="relu"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">3.relu: Rectified Linear Units，修正线性单元，</span><br><span class="line">输出范围：[0,+∞)</span><br><span class="line">公式：relu(x) &#x3D; max(0, x)</span><br><span class="line">导数：f&#39;(x) &#x3D; 1 if x &gt; 0 else 0</span><br><span class="line">优点：计算速度快，避免了梯度消失问题（它的梯度在 x&gt;0 时始终为1）</span><br><span class="line">缺点：Dead ReLU问题：当输入为负时，梯度为0，导致神经元不再更新</span><br><span class="line"></span><br><span class="line">代码实现：</span><br><span class="line">def relu(x):</span><br><span class="line">    return np.maximum(0, x)</span><br></pre></td></tr></table></figure>
<p><img src="../images/ml_review_leaky_relu.png" alt="leaky_relu"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">4.Leaky ReLU</span><br><span class="line">输出范围：(-∞, +∞)</span><br><span class="line">公式：f(x) &#x3D; max(α * x, x)，α通常是一个小于1的常数（如0.01）</span><br><span class="line">导数：f&#39;(x) &#x3D; 1 if x &gt; 0 else α</span><br><span class="line">优点：解决了Dead ReLU问题</span><br><span class="line">缺点：需要额外参数α</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">5.ELU</span><br><span class="line">公式：x if x &gt; 0 else a(exp(x) - 1)</span><br><span class="line">优点：ReLU的梯度消失问题，同时还具有平滑性</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">6.softmax</span><br><span class="line">输出范围：(0,1)，总和为1</span><br><span class="line">公式：f(x) &#x3D; exp(x) &#x2F; sum(exp(x))；分子是输入信号的指数函数，分母是所有输入信号的指数函数之和</span><br><span class="line">特点：可以将多分类的输出数值转化为相对概率，而这些值的累和为1</span><br><span class="line"></span><br><span class="line">代码实现：</span><br><span class="line">def softmax(x):</span><br><span class="line">    if x.ndim &#x3D;&#x3D; 2:</span><br><span class="line">        x &#x3D; x - x.max(axis&#x3D;1, keepdims&#x3D;True)  # 防止溢出</span><br><span class="line">        x &#x3D; np.exp(x)</span><br><span class="line">        x &#x2F;&#x3D; x.sum(axis&#x3D;1, keepdims&#x3D;True)</span><br><span class="line">    elif x.ndim &#x3D;&#x3D; 1:</span><br><span class="line">        x &#x3D; x - np.max(x)  # 防止溢出</span><br><span class="line">        x &#x3D; np.exp(x) &#x2F; np.sum(np.exp(x))</span><br><span class="line">    return x</span><br></pre></td></tr></table></figure>
<p><img src="../images/ml_review_mish.png" alt="Mish"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">7.Mish</span><br><span class="line">公式：Mish(x)&#x3D;x⋅tanh(ln(1+exp(x)))</span><br></pre></td></tr></table></figure>
<p><img src="../images/ml_review_swish.png" alt="Swish"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">8.Swish</span><br><span class="line">公式：f(x) &#x3D; x * sigmoid(x)</span><br></pre></td></tr></table></figure>

<h4 id="69-激活函数的比较"><a href="#69-激活函数的比较" class="headerlink" title="69.激活函数的比较"></a>69.激活函数的比较</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">1)Sigmoid和RELU: Sigmoid容易出现梯度消失， RELU修正了梯度消失的问题</span><br><span class="line"></span><br><span class="line">2)Sigmoid和tanh</span><br><span class="line">  - Sigmoid范围0~1，均值点为0.5; tanh范围-1~1，均值点为0</span><br><span class="line">  - tanh收敛速度比sigmoid快</span><br><span class="line"></span><br><span class="line">3)Sigmoid和softmax: Sigmoid用于二分类，softmax用于多分类</span><br><span class="line"></span><br><span class="line">4)softmax和Relu: softmax主要用于输出层，Relu主要用于隐藏层</span><br></pre></td></tr></table></figure>

<h4 id="70-什么是损失函数？损失函数的作用是什么？"><a href="#70-什么是损失函数？损失函数的作用是什么？" class="headerlink" title="70.什么是损失函数？损失函数的作用是什么？"></a>70.什么是损失函数？损失函数的作用是什么？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">损失函数（Loss Function），又称代价函数（Cost Function），用来度量预测值和实际值之间的差异，在模型训练中，为梯度下降更新参数指明方向</span><br></pre></td></tr></table></figure>

<h4 id="71-什么是梯度？什么是梯度下降？"><a href="#71-什么是梯度？什么是梯度下降？" class="headerlink" title="71.什么是梯度？什么是梯度下降？"></a>71.什么是梯度？什么是梯度下降？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">梯度是一个向量，它表示一个函数在某一点的方向导数。简单来说，梯度告诉我们函数在某一点的增长最快的方向</span><br><span class="line">梯度下降(Gradient Descent)是一种优化算法，用于寻找损失函数的极小值。其基本思想是，从一个初始点开始，沿着梯度的负方向逐步移动，从而使函数值逐步减少，直到达到某个极小值</span><br><span class="line"></span><br><span class="line">步骤：</span><br><span class="line">1）求损失函数值</span><br><span class="line">2）损失是否足够小？如果不是，计算损失函数的梯度</span><br><span class="line">3）按梯度的反方向走一小步（调整权重，Wi+1&#x3D;Wi−η∇f(Wi)）</span><br><span class="line">4）循环到第2步，迭代执行</span><br></pre></td></tr></table></figure>

<h4 id="72-什么是梯度消失？如何解决梯度消失问题？"><a href="#72-什么是梯度消失？如何解决梯度消失问题？" class="headerlink" title="72.什么是梯度消失？如何解决梯度消失问题？"></a>72.什么是梯度消失？如何解决梯度消失问题？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">梯度消失问题指的是在反向传播过程中，梯度在层与层之间逐渐变小，最终导致靠近输入层的隐层的权重更新非常缓慢甚至几乎不更新，使得模型难以学习到数据特征</span><br><span class="line"></span><br><span class="line">解决方法：</span><br><span class="line">1.选择合适的激活函数，如ReLU、Leaky ReLU等</span><br><span class="line">2.选择合适的权重初始化方式</span><br><span class="line">3.使用残差结构</span><br><span class="line">4.使用批量归一化（Batch Normalization）</span><br></pre></td></tr></table></figure>

<h4 id="73-什么是梯度爆炸？如何解决梯度爆炸问题？"><a href="#73-什么是梯度爆炸？如何解决梯度爆炸问题？" class="headerlink" title="73.什么是梯度爆炸？如何解决梯度爆炸问题？"></a>73.什么是梯度爆炸？如何解决梯度爆炸问题？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">梯度爆炸是指在神经网络训练过程中，梯度在反向传播的过程中不断累积，导致梯度值变得非常大，从而使权重更新幅度过大，导致网络训练不稳定，甚至无法收敛</span><br><span class="line"></span><br><span class="line">解决方法：</span><br><span class="line">1.动态调整学习率</span><br><span class="line">2.选择合适的激活函数</span><br><span class="line">3.梯度剪裁（Gradient Clipping）：在每次参数更新前对梯度进行剪裁，将梯度值控制在一定范围内，从而防止梯度过大</span><br></pre></td></tr></table></figure>

<h4 id="74-什么是反向传播算法？为何要使用？"><a href="#74-什么是反向传播算法？为何要使用？" class="headerlink" title="74.什么是反向传播算法？为何要使用？"></a>74.什么是反向传播算法？为何要使用？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">反向传播（Backpropagation algorithm）全称“误差反向传播”，是在深度神经网络中，根据输出层输出值，来反向调整隐层权重的一种方法</span><br><span class="line">为了对隐层的参数使用梯度下降，需要先将误差反向传播至隐层，然后才能应用</span><br></pre></td></tr></table></figure>

<h4 id="75-深度学习中，常见的优化器（梯度下降算法）有哪些？各有什么特点？"><a href="#75-深度学习中，常见的优化器（梯度下降算法）有哪些？各有什么特点？" class="headerlink" title="75.深度学习中，常见的优化器（梯度下降算法）有哪些？各有什么特点？"></a>75.深度学习中，常见的优化器（梯度下降算法）有哪些？各有什么特点？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1.随机梯度下降（SGD）：每次随机使用一个样本来计算梯度和更新权重。优点是计算快，缺点是收敛速度慢</span><br><span class="line">2.小批量梯度下降（Mini-batch Gradient Descent，MBGD）：结合了BGD和SGD的优点，使用一个小批量样本来计算梯度。平衡了计算效率和收敛速度</span><br><span class="line">3.动量法（Momentum）：在SGD的基础上引入了一个动量项，会考虑之前参数更新的方向和速度，解决了SGD的收敛速度慢和陷入局部最低点这两个问题</span><br><span class="line">4.Nesterov加速（NAG）：通过在梯度方向上进行预先的“看向”操作来加速收敛</span><br><span class="line">5.自适应梯度（Adagrad）：根据每个参数的历史梯度来调整学习率，适合处理稀疏数据</span><br><span class="line">6.RMSprop：采用滑动窗口加权平均值计算二阶动量，解决了Adagrad中学习率有时持续下降的问题</span><br><span class="line">7.Adam（Adaptive Moment Estimation）：结合了Momentum和RMSprop的优点，适应性强，收敛速度快，适合大多数深度学习任务</span><br><span class="line">8.AdamW：是Adam的改进版本，加入了权重衰减（Weight Decay）来正则化模型，防止过拟合。被证明在很多任务中比Adam效果更好</span><br></pre></td></tr></table></figure>

    </div>

    
    
    

      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/ml/" rel="tag"><i class="fa fa-tag"></i> ml</a>
              <a href="/tags/dl/" rel="tag"><i class="fa fa-tag"></i> dl</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/python-review/" rel="prev" title="Python复习">
      <i class="fa fa-chevron-left"></i> Python复习
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    
  <div class="comments">
    <div id="disqus_thread">
      <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
    </div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-数据从哪里来？如何构建数据集？"><span class="nav-text">1.数据从哪里来？如何构建数据集？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-数据量多大？"><span class="nav-text">2.数据量多大？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-数据量不够如何处理？"><span class="nav-text">3.数据量不够如何处理？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-采用的模型是什么？为什么使用YOLOv8？"><span class="nav-text">4.采用的模型是什么？为什么使用YOLOv8？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-什么情况下使用OpenCV，什么情况下使用深度学习？"><span class="nav-text">5.什么情况下使用OpenCV，什么情况下使用深度学习？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-准确率是多少？"><span class="nav-text">6.准确率是多少？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-写项目经验注意的问题"><span class="nav-text">7.写项目经验注意的问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-什么是有监督学习（Supervised-Learning）和无监督学习（Unsupervised-Learning）？请举例说明每种类型的应用场景"><span class="nav-text">8.什么是有监督学习（Supervised Learning）和无监督学习（Unsupervised Learning）？请举例说明每种类型的应用场景</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-贝叶斯公式及推导过程，有哪些应用场景？"><span class="nav-text">9.贝叶斯公式及推导过程，有哪些应用场景？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#10-什么是似然？"><span class="nav-text">10.什么是似然？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#11-什么是欠拟合、过拟合？如何避免过拟合？如何避免欠拟合？"><span class="nav-text">11.什么是欠拟合、过拟合？如何避免过拟合？如何避免欠拟合？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#12-神经网络加速训练方法有哪些？"><span class="nav-text">12.神经网络加速训练方法有哪些？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#13-目标检测常用算法有哪些，简述对算法的理解"><span class="nav-text">13.目标检测常用算法有哪些，简述对算法的理解</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#14-什么是感受野？"><span class="nav-text">14.什么是感受野？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#15-什么是正则化？L1、L2、smooth-L1正则化的区别"><span class="nav-text">15.什么是正则化？L1、L2、smooth L1正则化的区别</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#16-YOLOv8中的objectness-loss是什么？"><span class="nav-text">16.YOLOv8中的objectness loss是什么？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#17-什么是特征归一化？为什么要归一化？"><span class="nav-text">17.什么是特征归一化？为什么要归一化？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#18-归一化常用方法？"><span class="nav-text">18.归一化常用方法？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#19-归一化处理适用模型"><span class="nav-text">19.归一化处理适用模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#20-什么是标准化？常用方法？"><span class="nav-text">20.什么是标准化？常用方法？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#21-标准化和归一化的联系和区别"><span class="nav-text">21.标准化和归一化的联系和区别</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#22-均值、离差、离差方、方差、标准差之间的关系"><span class="nav-text">22.均值、离差、离差方、方差、标准差之间的关系</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#23-方差和标准差有什么区别？"><span class="nav-text">23.方差和标准差有什么区别？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#24-回归问题的模型评估指标"><span class="nav-text">24.回归问题的模型评估指标</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#25-分类问题中的TP、FP、TN、FN是什么"><span class="nav-text">25.分类问题中的TP、FP、TN、FN是什么</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#26-如何查看混淆矩阵"><span class="nav-text">26.如何查看混淆矩阵</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#27-分类问题的模型评估指标"><span class="nav-text">27.分类问题的模型评估指标</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#28-回归问题的损失函数，为何使用平方（MSE）而不是绝对值（MAE）？"><span class="nav-text">28.回归问题的损失函数，为何使用平方（MSE）而不是绝对值（MAE）？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#29-损失函数和评估函数（指标）的区别？"><span class="nav-text">29.损失函数和评估函数（指标）的区别？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#30-什么是超参数？有哪些常用的超参数调优手段？"><span class="nav-text">30.什么是超参数？有哪些常用的超参数调优手段？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#31-有哪些常见的超参数？各自对模型有怎样的影响？"><span class="nav-text">31.有哪些常见的超参数？各自对模型有怎样的影响？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#32-什么是置信概率？"><span class="nav-text">32. 什么是置信概率？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#33-什么是交叉验证？它有哪些常见类型？"><span class="nav-text">33.什么是交叉验证？它有哪些常见类型？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#34-对于类别不均衡问题，有哪些处理方法？"><span class="nav-text">34.对于类别不均衡问题，有哪些处理方法？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#35-神经网络权重初始值如何设置？"><span class="nav-text">35.神经网络权重初始值如何设置？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#36-什么是线性回归？线性回归的特点是什么？"><span class="nav-text">36.什么是线性回归？线性回归的特点是什么？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#37-什么是多项式回归？多项式回归的特点是什么？"><span class="nav-text">37.什么是多项式回归？多项式回归的特点是什么？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#38-什么是决策树？工作原理是什么？特点？"><span class="nav-text">38.什么是决策树？工作原理是什么？特点？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#39-有哪些常见的决策树算法？"><span class="nav-text">39.有哪些常见的决策树算法？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#40-CART-在分类问题和回归问题中的异同"><span class="nav-text">40.CART 在分类问题和回归问题中的异同</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#41-什么是集成学习？"><span class="nav-text">41.什么是集成学习？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#42-Bagging与Boosting的原理是什么？二者有何区别？"><span class="nav-text">42.Bagging与Boosting的原理是什么？二者有何区别？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#43-机器学习模型的偏差和方差是什么？"><span class="nav-text">43.机器学习模型的偏差和方差是什么？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#44-什么是基于决策树的集合算法-集成学习-？"><span class="nav-text">44.什么是基于决策树的集合算法(集成学习)？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#45-简要介绍AdaBoost、GBDT、XGBoost"><span class="nav-text">45.简要介绍AdaBoost、GBDT、XGBoost</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#46-什么是随机森林？"><span class="nav-text">46.什么是随机森林？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#47-什么是逻辑回归？它是如何实现二分类的？"><span class="nav-text">47.什么是逻辑回归？它是如何实现二分类的？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#48-什么是逻辑函数（sigmoid）？它有什么特点？"><span class="nav-text">48.什么是逻辑函数（sigmoid）？它有什么特点？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#49-什么是信息熵？"><span class="nav-text">49.什么是信息熵？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#50-什么是交叉熵损失函数？"><span class="nav-text">50.什么是交叉熵损失函数？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#51-什么是朴素贝叶斯分类？特点是什么？何时使用？"><span class="nav-text">51.什么是朴素贝叶斯分类？特点是什么？何时使用？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#52-常见的朴素贝叶斯分类器有哪些？"><span class="nav-text">52.常见的朴素贝叶斯分类器有哪些？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#53-什么是支持向量机？"><span class="nav-text">53.什么是支持向量机？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#54-SVM寻找最优边界要求有哪些？"><span class="nav-text">54.SVM寻找最优边界要求有哪些？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#55-SVM的工作原理？"><span class="nav-text">55.SVM的工作原理？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#56-SVM中的核函数是什么？常用的核函数有哪些？"><span class="nav-text">56.SVM中的核函数是什么？常用的核函数有哪些？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#57-SVM的特点？"><span class="nav-text">57.SVM的特点？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#58-什么是聚类？"><span class="nav-text">58.什么是聚类？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#59-有哪些常用的相似度度量方式？"><span class="nav-text">59.有哪些常用的相似度度量方式？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#60-聚类问题的评价指标是什么？"><span class="nav-text">60.聚类问题的评价指标是什么？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#61-什么是K-Means聚类？"><span class="nav-text">61.什么是K-Means聚类？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#62-什么是DBSCAN（噪声密度）？"><span class="nav-text">62.什么是DBSCAN（噪声密度）？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#63-什么是凝聚层次算法？"><span class="nav-text">63.什么是凝聚层次算法？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#64-什么是神经网络？它有哪些常见类型？"><span class="nav-text">64.什么是神经网络？它有哪些常见类型？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#65-神经网络中的权重和偏置是什么？"><span class="nav-text">65.神经网络中的权重和偏置是什么？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#66-深度学习的优缺点是什么？"><span class="nav-text">66.深度学习的优缺点是什么？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#67-什么是激活函数？为什么要使用激活函数？"><span class="nav-text">67.什么是激活函数？为什么要使用激活函数？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#68-神经网络中常用的激活函数有哪些，各自有什么特点？"><span class="nav-text">68.神经网络中常用的激活函数有哪些，各自有什么特点？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#69-激活函数的比较"><span class="nav-text">69.激活函数的比较</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#70-什么是损失函数？损失函数的作用是什么？"><span class="nav-text">70.什么是损失函数？损失函数的作用是什么？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#71-什么是梯度？什么是梯度下降？"><span class="nav-text">71.什么是梯度？什么是梯度下降？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#72-什么是梯度消失？如何解决梯度消失问题？"><span class="nav-text">72.什么是梯度消失？如何解决梯度消失问题？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#73-什么是梯度爆炸？如何解决梯度爆炸问题？"><span class="nav-text">73.什么是梯度爆炸？如何解决梯度爆炸问题？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#74-什么是反向传播算法？为何要使用？"><span class="nav-text">74.什么是反向传播算法？为何要使用？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#75-深度学习中，常见的优化器（梯度下降算法）有哪些？各有什么特点？"><span class="nav-text">75.深度学习中，常见的优化器（梯度下降算法）有哪些？各有什么特点？</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="roubin"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">roubin</p>
  <div class="site-description" itemprop="description">芝兰其室，金石其心<br/>仁义为友，道德为师</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">137</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">38</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">76</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/satorioh" title="GitHub → https://github.com/satorioh" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:wangbinxp@gmail.com" title="E-Mail → mailto:wangbinxp@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2015 – 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">roubin</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  
  <script>
    (function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();
  </script>




  
<script src="/js/local-search.js"></script>













  

  

<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://roubinme.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>
<script>
  var disqus_config = function() {
    this.page.url = "https://roubin.me/ml-review/";
    this.page.identifier = "ml-review/";
    this.page.title = "ML/DL复习";
    };
  NexT.utils.loadComments(document.querySelector('#disqus_thread'), () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://roubinme.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });
</script>

</body>
</html>
