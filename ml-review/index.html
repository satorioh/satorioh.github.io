<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta http-equiv="Cache-Control" content="no-transform">
  <meta http-equiv="Cache-Control" content="no-siteapp">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"roubin.me","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="1.数据从哪里来？如何构建数据集？123456781.现场自行采集（成本比较高）2.甲方提供3.网络下载现成的4.企业真实的数据（商业价值最高）5.购买（合法购买和使用，千万不要侵犯别人的隐私）6.爬虫（合法使用，不要侵犯别人的隐私）数据分门别类,进行标注">
<meta property="og:type" content="article">
<meta property="og:title" content="ML&#x2F;DL复习">
<meta property="og:url" content="https://roubin.me/ml-review/index.html">
<meta property="og:site_name" content="肉饼博客">
<meta property="og:description" content="1.数据从哪里来？如何构建数据集？123456781.现场自行采集（成本比较高）2.甲方提供3.网络下载现成的4.企业真实的数据（商业价值最高）5.购买（合法购买和使用，千万不要侵犯别人的隐私）6.爬虫（合法使用，不要侵犯别人的隐私）数据分门别类,进行标注">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://roubin.me/images/ml_review_l1_expression.png">
<meta property="og:image" content="https://roubin.me/images/ml_review_mse_loss.png">
<meta property="og:image" content="https://roubin.me/images/ml_review_decision_tree.png">
<meta property="og:image" content="https://roubin.me/images/ml_review_svm.png">
<meta property="og:image" content="https://roubin.me/images/ml_review_kmeans.png">
<meta property="og:image" content="https://roubin.me/images/ml_review_dbscan.png">
<meta property="og:image" content="https://roubin.me/images/ml_review_sigmoid.png">
<meta property="og:image" content="https://roubin.me/images/ml_review_tanh.png">
<meta property="og:image" content="https://roubin.me/images/ml_review_relu_variant.png">
<meta property="og:image" content="https://roubin.me/images/ml_review_relu.png">
<meta property="og:image" content="https://roubin.me/images/ml_review_leaky_relu.png">
<meta property="og:image" content="https://roubin.me/images/ml_review_mish.png">
<meta property="og:image" content="https://roubin.me/images/ml_review_swish.png">
<meta property="og:image" content="https://roubin.me/images/ml_review_tfidf.png">
<meta property="og:image" content="https://roubin.me/images/ml_review_erode.png">
<meta property="og:image" content="https://roubin.me/images/ml_review_dilate.png">
<meta property="og:image" content="https://roubin.me/images/ml_review_open.png">
<meta property="og:image" content="https://roubin.me/images/ml_review_close.png">
<meta property="og:image" content="https://roubin.me/images/ml_review_morph_gradient.png">
<meta property="og:image" content="https://roubin.me/images/ml_review_hat.png">
<meta property="og:image" content="https://roubin.me/images/ml_review_rf.png">
<meta property="og:image" content="https://roubin.me/images/ml_review_dot_func.png">
<meta property="og:image" content="https://roubin.me/images/fasterrcnn_roi_pooling.png">
<meta property="og:image" content="https://roubin.me/images/ml_review_sigmoid_dot_process.png">
<meta property="og:image" content="https://roubin.me/images/ml_review_original_conv.png">
<meta property="og:image" content="https://roubin.me/images/ml_review_depthwise_conv.png">
<meta property="og:image" content="https://roubin.me/images/ml_review_pointwise_conv.png">
<meta property="article:published_time" content="2024-05-21T01:14:43.000Z">
<meta property="article:modified_time" content="2024-07-29T09:42:01.347Z">
<meta property="article:author" content="roubin">
<meta property="article:tag" content="ml">
<meta property="article:tag" content="dl">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://roubin.me/images/ml_review_l1_expression.png">

<link rel="canonical" href="https://roubin.me/ml-review/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>ML/DL复习 | 肉饼博客</title>
  


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?b55ece62ebeb2e72a8efe3f8b5b43960";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">肉饼博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Talk is cheap. Show me the code.</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://roubin.me/ml-review/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="roubin">
      <meta itemprop="description" content="芝兰其室，金石其心<br/>仁义为友，道德为师">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="肉饼博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          ML/DL复习
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-05-21 09:14:43" itemprop="dateCreated datePublished" datetime="2024-05-21T09:14:43+08:00">2024-05-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-07-29 17:42:01" itemprop="dateModified" datetime="2024-07-29T17:42:01+08:00">2024-07-29</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/ml-review/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="ml-review/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>35k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>32 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h4 id="1-数据从哪里来？如何构建数据集？"><a href="#1-数据从哪里来？如何构建数据集？" class="headerlink" title="1.数据从哪里来？如何构建数据集？"></a>1.数据从哪里来？如何构建数据集？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1.现场自行采集（成本比较高）</span><br><span class="line">2.甲方提供</span><br><span class="line">3.网络下载现成的</span><br><span class="line">4.企业真实的数据（商业价值最高）</span><br><span class="line">5.购买（合法购买和使用，千万不要侵犯别人的隐私）</span><br><span class="line">6.爬虫（合法使用，不要侵犯别人的隐私）</span><br><span class="line"></span><br><span class="line">数据分门别类,进行标注</span><br></pre></td></tr></table></figure>
<a id="more"></a>

<h4 id="2-数据量多大？"><a href="#2-数据量多大？" class="headerlink" title="2.数据量多大？"></a>2.数据量多大？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">深度学习越多越好</span><br><span class="line">最起码每个类别数百级单位</span><br></pre></td></tr></table></figure>

<h4 id="3-数据量不够如何处理？"><a href="#3-数据量不够如何处理？" class="headerlink" title="3.数据量不够如何处理？"></a>3.数据量不够如何处理？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">数据增强</span><br><span class="line">CV: 旋转、缩放、裁剪、调整色调、亮度、对比度、添加噪声。。。</span><br><span class="line">NLP: 近义词替换、文本摘要。。。。</span><br></pre></td></tr></table></figure>

<h4 id="4-采用的模型是什么？为什么使用YOLOv8？"><a href="#4-采用的模型是什么？为什么使用YOLOv8？" class="headerlink" title="4.采用的模型是什么？为什么使用YOLOv8？"></a>4.采用的模型是什么？为什么使用YOLOv8？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">效果</span><br><span class="line">  依据：根据实际需求，以及项目的难易程度，选择现有的、经典的、成熟的模型，更换成自己的数据集，进行训练，并调参</span><br><span class="line">  </span><br><span class="line">  如果不确定模型，选择多个模型，进行对比，选择效果最好的</span><br><span class="line">  </span><br><span class="line">  在有些情况下，需要多个模型配合使用，发挥各自的特长</span><br><span class="line"></span><br><span class="line">YOLOv8优势</span><br><span class="line">    1.保持高精度的同时，具有更快的推理速度，适合实时检测</span><br><span class="line">    2.多模型尺寸、多任务能力</span><br><span class="line">    3.友好的文档、API、社区支持</span><br></pre></td></tr></table></figure>

<h4 id="5-什么情况下使用OpenCV，什么情况下使用深度学习？"><a href="#5-什么情况下使用OpenCV，什么情况下使用深度学习？" class="headerlink" title="5.什么情况下使用OpenCV，什么情况下使用深度学习？"></a>5.什么情况下使用OpenCV，什么情况下使用深度学习？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Opencv：不需要程序理解图像的场景和内容，图像相对单一，干扰因素较少，需求比较简单，数据量比较少</span><br><span class="line">深度学习：需要程序理解图像的内容和场景，场景复杂，干扰因素多，样本变化大，需求复杂，数据量足够大</span><br></pre></td></tr></table></figure>

<h4 id="6-准确率是多少？"><a href="#6-准确率是多少？" class="headerlink" title="6.准确率是多少？"></a>6.准确率是多少？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">工业中至少要95%以上，越高越好，不要过拟合</span><br></pre></td></tr></table></figure>

<h4 id="7-写项目经验注意的问题"><a href="#7-写项目经验注意的问题" class="headerlink" title="7.写项目经验注意的问题"></a>7.写项目经验注意的问题</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">项目背景(需求)：用户是谁？用在什么地方？解决什么问题？</span><br><span class="line">数据集：来源？数量？数据增强？标注？预处理手段？</span><br><span class="line">模型：模型选择？训练过程？调参优化过程？</span><br><span class="line">遇到了什么问题？怎么解决的？</span><br><span class="line">过拟合，欠拟合问题怎么解决的？</span><br><span class="line">效果？</span><br><span class="line">部署？</span><br></pre></td></tr></table></figure>

<h4 id="8-什么是有监督学习（Supervised-Learning）和无监督学习（Unsupervised-Learning）？请举例说明每种类型的应用场景"><a href="#8-什么是有监督学习（Supervised-Learning）和无监督学习（Unsupervised-Learning）？请举例说明每种类型的应用场景" class="headerlink" title="8.什么是有监督学习（Supervised Learning）和无监督学习（Unsupervised Learning）？请举例说明每种类型的应用场景"></a>8.什么是有监督学习（Supervised Learning）和无监督学习（Unsupervised Learning）？请举例说明每种类型的应用场景</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">有监督：训练数据集包含了标签，在训练过程中，模型学习输入与标签之间的映射关系</span><br><span class="line">分类：图像分类、垃圾邮件分类</span><br><span class="line">回归：房价预测</span><br><span class="line"></span><br><span class="line">无监督：不依赖于标注数据，模型通过输入数据自行发现数据的结构或模式</span><br><span class="line">聚类：客户分组</span><br><span class="line">降维：从高维度数据中提取主要特征</span><br></pre></td></tr></table></figure>

<h4 id="9-贝叶斯公式及推导过程，有哪些应用场景？"><a href="#9-贝叶斯公式及推导过程，有哪些应用场景？" class="headerlink" title="9.贝叶斯公式及推导过程，有哪些应用场景？"></a>9.贝叶斯公式及推导过程，有哪些应用场景？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">公式：P(A|B) &#x3D; P(B|A)⋅P(A) &#x2F; P(B)</span><br><span class="line">解释：由先验概率和条件概率，推算出后验概率</span><br><span class="line"></span><br><span class="line">推导：</span><br><span class="line">由联合概率可知：P(A,B)&#x3D;P(A∣B)⋅P(B)</span><br><span class="line">由联合概率对称性可知：P(A,B)&#x3D;P(B∣A)⋅P(A)</span><br><span class="line">等式相等：P(A∣B)⋅P(B) &#x3D; P(B∣A)⋅P(A)</span><br><span class="line">两边除以P(B)得：P(A∣B) &#x3D; P(B∣A)⋅P(A) &#x2F; P(B)</span><br><span class="line"></span><br><span class="line">应用：</span><br><span class="line">朴素贝叶斯分类器</span><br><span class="line">医疗诊断中，计算患者患有特定疾病的概率</span><br><span class="line">风险评估</span><br></pre></td></tr></table></figure>

<h4 id="10-什么是似然？"><a href="#10-什么是似然？" class="headerlink" title="10.什么是似然？"></a>10.什么是似然？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">概念：在已知某些数据的情况下，模型参数取特定值的概率</span><br><span class="line">与概率的区别：概率描述的是在已知参数的情况下，观测到某数据的概率，而似然则是相反的情况，即在已知数据的情况下，参数的可能值</span><br></pre></td></tr></table></figure>

<h4 id="11-什么是欠拟合、过拟合？如何避免过拟合？如何避免欠拟合？"><a href="#11-什么是欠拟合、过拟合？如何避免过拟合？如何避免欠拟合？" class="headerlink" title="11.什么是欠拟合、过拟合？如何避免过拟合？如何避免欠拟合？"></a>11.什么是欠拟合、过拟合？如何避免过拟合？如何避免欠拟合？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">欠拟合：指模型无法从训练数据中学习到足够的模式，导致其在训练集和测试集上都表现不佳</span><br><span class="line">原因：模型太小太简单、训练数据太少、训练时间不够</span><br><span class="line">解决：</span><br><span class="line">    1.增加模型复杂度</span><br><span class="line">    2.增加特征数量</span><br><span class="line">    3.增加训练数据</span><br><span class="line">    4.训练更多轮数</span><br><span class="line">    5.减小正则化强度</span><br><span class="line"></span><br><span class="line">过拟合：指模型在训练集上表现很好，但在测试集上表现较差，即模型过度适应了训练数据，忽略了数据的总体趋势，导致泛化能力差</span><br><span class="line">原因：模型太复杂、数据集太小、训练时间过长</span><br><span class="line">解决：</span><br><span class="line">    1.数据增强</span><br><span class="line">    2.正则化</span><br><span class="line">    3.提前终止</span><br><span class="line">    4.集成学习</span><br><span class="line">    5.dropout</span><br></pre></td></tr></table></figure>

<h4 id="12-神经网络加速训练方法有哪些？"><a href="#12-神经网络加速训练方法有哪些？" class="headerlink" title="12.神经网络加速训练方法有哪些？"></a>12.神经网络加速训练方法有哪些？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">硬件：多GPU、分布式训练</span><br><span class="line">数据：归一化、数据增强</span><br><span class="line">模型：更好的优化器、BN、量化、剪枝、使用预训练模型</span><br><span class="line">训练：提前终止、混合精度训练、自动化超参数调整</span><br></pre></td></tr></table></figure>

<h4 id="13-目标检测常用算法有哪些，简述对算法的理解"><a href="#13-目标检测常用算法有哪些，简述对算法的理解" class="headerlink" title="13.目标检测常用算法有哪些，简述对算法的理解"></a>13.目标检测常用算法有哪些，简述对算法的理解</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">两阶段检测</span><br><span class="line">   	先产生候选区，在候选区上分类+定位</span><br><span class="line">   	速度相对比较慢，精度高</span><br><span class="line">   	RCNN系列  </span><br><span class="line">   	</span><br><span class="line">一阶段检测</span><br><span class="line">   	预定义候选区，直接在特征图上分类+定位</span><br><span class="line">   	速度比较快，精度较低</span><br><span class="line">   	YOLO系列、SSD、RetinaNet</span><br></pre></td></tr></table></figure>

<h4 id="14-什么是感受野？"><a href="#14-什么是感受野？" class="headerlink" title="14.什么是感受野？"></a>14.什么是感受野？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">CNN的feature map上的像素点，在原始图像上对应的区域大小</span><br><span class="line">卷积层次越深：特征图越小，感受野越大，整体感越强（语意信息）</span><br><span class="line">卷积层次越浅：特征图越大，感受野越小，几何细节越丰富</span><br></pre></td></tr></table></figure>

<h4 id="15-什么是正则化？L1、L2、smooth-L1正则化的区别"><a href="#15-什么是正则化？L1、L2、smooth-L1正则化的区别" class="headerlink" title="15.什么是正则化？L1、L2、smooth L1正则化的区别"></a>15.什么是正则化？L1、L2、smooth L1正则化的区别</h4><p>正则化：在损失函数后面添加一个范数（惩罚项），整体上压缩了参数的大小，来防止过拟合的手段</p>
<p>范数表达式如下：</p>
<p><img src="../images/ml_review_l1_expression.png" alt="img.png"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">L1正则化：也称为Lasso正则化，当p&#x3D;1时，是L1范数，表示某个向量中所有元素绝对值之和</span><br><span class="line">L2正则化：也称为Ridge正则化，当p&#x3D;2时，是L2范数，表示某个向量中所有元素平方和再开方，即欧氏距离</span><br><span class="line"></span><br><span class="line">区别：</span><br><span class="line">L1正则化的效果是使模型的参数变得稀疏，即部分参数的值为0，可用于特征选择和模型压缩</span><br><span class="line">L2正则化的效果是使模型的参数变得平滑，即相邻参数的值相差较小，可提升泛化能力</span><br><span class="line">smooth L1: 结合了L1和L2的特点</span><br></pre></td></tr></table></figure>

<h4 id="16-Loss-Function、Cost-Function-和-Objective-Function-的区别"><a href="#16-Loss-Function、Cost-Function-和-Objective-Function-的区别" class="headerlink" title="16.Loss Function、Cost Function 和 Objective Function 的区别"></a>16.Loss Function、Cost Function 和 Objective Function 的区别</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">损失函数(Loss Function)通常是针对单个训练样本而言，给定一个模型输出y&#39;和一个真实值y，损失函数输出一个实值损失 L &#x3D; f(y,y&#39;)</span><br><span class="line">代价函数(Cost Function)通常是针对整个训练集（或者在使用 mini-batch gradient descent 时一个 mini-batch）的总损失 J &#x3D; ∑f(y,y&#39;)</span><br><span class="line">目标函数(Objective Function)是一个更通用的术语，表示任意希望被优化的函数，用于机器学习领域和非机器学习领域（比如运筹优化）</span><br><span class="line"></span><br><span class="line">一句话总结三者的关系就是：A loss function is a part of a cost function which is a type of an objective function.</span><br></pre></td></tr></table></figure>

<h4 id="17-什么是特征归一化？为什么要归一化？"><a href="#17-什么是特征归一化？为什么要归一化？" class="headerlink" title="17.什么是特征归一化？为什么要归一化？"></a>17.什么是特征归一化？为什么要归一化？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">归一化一般是将数据映射到指定的范围（[0, 1] 或 [-1, 1]），从而消除不同特征量纲的影响。</span><br><span class="line">进行归一化处理，使得不同指标之间处于同一数量级，具有可比性。另外还能加速模型收敛，提升性能</span><br></pre></td></tr></table></figure>

<h4 id="18-归一化常用方法？"><a href="#18-归一化常用方法？" class="headerlink" title="18.归一化常用方法？"></a>18.归一化常用方法？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">Min-Max Normalization</span><br><span class="line">公式：X &#x3D; (X-Xmin) &#x2F; (Xmax - Xmin)</span><br><span class="line">--------------------------------------</span><br><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line">import sklearn.preprocessing as sp</span><br><span class="line"></span><br><span class="line">raw_sample &#x3D; np.array([[3.0, -100.0, 2000.0],</span><br><span class="line">                       [0.0, 400.0, 3000.0],</span><br><span class="line">                       [1.0, -400.0, 2000.0]])</span><br><span class="line"></span><br><span class="line">mms_sample &#x3D; raw_sample.copy()</span><br><span class="line"></span><br><span class="line"># 1.减去最小值</span><br><span class="line"># 2.减完之后的结果&#x2F;极差</span><br><span class="line">for col in mms_sample.T:</span><br><span class="line">    col_min &#x3D; col.min()</span><br><span class="line">    col_max &#x3D; col.max()</span><br><span class="line">    col -&#x3D; col_min</span><br><span class="line">    col &#x2F;&#x3D; (col_max - col_min)</span><br><span class="line">  </span><br><span class="line">    </span><br><span class="line"># 基于skLearn提供的API实现</span><br><span class="line">scaler &#x3D; sp.MinMaxScaler()</span><br><span class="line">res &#x3D; scaler.fit_transform(raw_sample)</span><br></pre></td></tr></table></figure>

<h4 id="19-归一化处理适用模型"><a href="#19-归一化处理适用模型" class="headerlink" title="19.归一化处理适用模型"></a>19.归一化处理适用模型</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">应用归一化的模型：在实际应用中，通过梯度下降法求解的模型通常是需要归一化的，包括线性回归、逻辑回归、支持向量机、神经网络等模型。</span><br><span class="line">不使用归一化的模型：如决策树分类</span><br></pre></td></tr></table></figure>

<h4 id="20-什么是标准化？常用方法？"><a href="#20-什么是标准化？常用方法？" class="headerlink" title="20.什么是标准化？常用方法？"></a>20.什么是标准化？常用方法？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">标准化是将特征值调整为均值为0，标准差为1的标准正态分布</span><br><span class="line"></span><br><span class="line">Z-Score Normalization</span><br><span class="line">公式：X &#x3D; (X - Xmean) &#x2F; Xstd</span><br><span class="line">-----------------------------------</span><br><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line">import sklearn.preprocessing as sp</span><br><span class="line"></span><br><span class="line">raw_sample &#x3D; np.array([[3.0, -100.0, 2000.0],</span><br><span class="line">                       [0.0, 400.0, 3000.0],</span><br><span class="line">                       [1.0, -400.0, 2000.0]])</span><br><span class="line"></span><br><span class="line">std_sample &#x3D; raw_sample.copy()</span><br><span class="line"></span><br><span class="line"># 1.减去当前列的平均值</span><br><span class="line"># 2.离差&#x2F;原始数据的标准差</span><br><span class="line">for col in std_sample.T:</span><br><span class="line">    col_mean &#x3D; col.mean()  # 平均值</span><br><span class="line">    col_std &#x3D; col.std()  # 标准差</span><br><span class="line">    col -&#x3D; col_mean</span><br><span class="line">    col &#x2F;&#x3D; col_std</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"># 基于skLearn提供的API实现</span><br><span class="line">scaler &#x3D; sp.StandardScaler()</span><br><span class="line">res &#x3D; scaler.fit_transform(raw_sample)</span><br></pre></td></tr></table></figure>

<h4 id="21-标准化和归一化的联系和区别"><a href="#21-标准化和归一化的联系和区别" class="headerlink" title="21.标准化和归一化的联系和区别"></a>21.标准化和归一化的联系和区别</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">联系:</span><br><span class="line">    我们都知道归一化是指normalization，标准化是指standardization，但根据wiki上对feature scaling方法的定义，standardization其实就是z-score normalization，也就是说标准化其实是归一化的一种，而一般情况下，我们会把z-score归一化称为标准化，把min-max归一化简称为归一化</span><br><span class="line">    目的：都是通过缩放和平移来实现数据映射，消除不同特征量纲的影响</span><br><span class="line"></span><br><span class="line">区别：</span><br><span class="line">    归一化不会改变数据的状态分布，但标准化会</span><br><span class="line">    归一化会将数据限定在一个具体的范围内，如 [0, 1]，但标准化不会</span><br><span class="line">    归一化只受原样本数据中的极值影响，而标准化则受所有样本值的影响</span><br><span class="line">    归一化对异常值敏感，而标准化则对异常值鲁棒</span><br></pre></td></tr></table></figure>

<h4 id="22-均值、离差、离差方、方差、标准差之间的关系"><a href="#22-均值、离差、离差方、方差、标准差之间的关系" class="headerlink" title="22.均值、离差、离差方、方差、标准差之间的关系"></a>22.均值、离差、离差方、方差、标准差之间的关系</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">S &#x3D; np.array([1, 2, 3, 4, 5, 6])</span><br><span class="line"></span><br><span class="line"># 均值</span><br><span class="line">mean &#x3D; np.mean(S)</span><br><span class="line">print(mean)  # 3.5</span><br><span class="line"></span><br><span class="line"># 离差 &#x3D; 观测值 - 均值</span><br><span class="line">deviation &#x3D; S - mean</span><br><span class="line">print(deviation)  # [-2.5 -1.5 -0.5  0.5  1.5  2.5]</span><br><span class="line"></span><br><span class="line"># 离差方 &#x3D; 离差 ** 2</span><br><span class="line">deviation_square &#x3D; deviation ** 2</span><br><span class="line">print(deviation_square)  # [6.25 2.25 0.25 0.25 2.25 6.25]</span><br><span class="line"></span><br><span class="line"># 方差 &#x3D; 离差方的均值</span><br><span class="line">variance &#x3D; np.mean(deviation_square)</span><br><span class="line">print(variance)  # 2.9166666666666665</span><br><span class="line"></span><br><span class="line"># 标准差 &#x3D; 方差的平方根</span><br><span class="line">std &#x3D; np.sqrt(variance)</span><br><span class="line">print(std)  # 1.707825127659933</span><br></pre></td></tr></table></figure>

<h4 id="23-方差和标准差有什么区别？"><a href="#23-方差和标准差有什么区别？" class="headerlink" title="23.方差和标准差有什么区别？"></a>23.方差和标准差有什么区别？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">方差和标准差都可以用来衡量数据的离散程度</span><br><span class="line">区别：</span><br><span class="line">1.计算方法不同</span><br><span class="line">2.单位不同：标准差单位与数据的单位一致，因此更直观易理解</span><br><span class="line"></span><br><span class="line">方差和标准差的缺点：</span><br><span class="line">1.对异常值比较敏感</span><br><span class="line">2.只能衡量单个变量的离散程度，不能反映变量之间的关系</span><br><span class="line">3.数据需要满足正态分布</span><br></pre></td></tr></table></figure>

<h4 id="24-回归问题的模型评估指标"><a href="#24-回归问题的模型评估指标" class="headerlink" title="24.回归问题的模型评估指标"></a>24.回归问题的模型评估指标</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">1.均方误差(Mean Squared Error, MSE)</span><br><span class="line">  公式：MSE &#x3D; Σ(y_i - y&#39;_i)^2 &#x2F; n</span><br><span class="line">  取值：越小越好</span><br><span class="line">  特点：L2范数，对离群值敏感，因为平方放大了误差</span><br><span class="line"></span><br><span class="line">2.均方根误差（Root Mean Squared Error, RMSE）</span><br><span class="line">  公式：RMSE &#x3D; √MSE</span><br><span class="line">  取值：越小越好</span><br><span class="line">  特点：RMSE是MSE的平方根，其单位与原始数据相同，便于描述真实值。也对离群值敏感</span><br><span class="line">  </span><br><span class="line">3.平均绝对误差（Mean Absolute Error, MAE）</span><br><span class="line">  公式：MAE &#x3D; Σ|y_i - y&#39;_i| &#x2F; n</span><br><span class="line">  取值：越小越好</span><br><span class="line">  特点：L1范数，对离群值不敏感</span><br><span class="line">  </span><br><span class="line">4.决定系数&#x2F;拟合优度（R² 或 Coefficient of Determination）</span><br><span class="line">  解释：表示模型的拟合度</span><br><span class="line">  公式：R2 &#x3D; 1 - MSE &#x2F; Variance</span><br><span class="line">  取值：0～1之间，越大越好</span><br><span class="line">  特点：仅表示拟合程度，不代表模型预测准确度</span><br></pre></td></tr></table></figure>

<h4 id="25-分类问题中的TP、FP、TN、FN是什么"><a href="#25-分类问题中的TP、FP、TN、FN是什么" class="headerlink" title="25.分类问题中的TP、FP、TN、FN是什么"></a>25.分类问题中的TP、FP、TN、FN是什么</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">TP：True Positive，正确得预测为正样本，实际就是正样本，即正样本被正确识别的数量</span><br><span class="line">FP：False Positive，错误得预测为正样本，实际为负样本，即误报的数量</span><br><span class="line">TN：True Negative，正确得预测为负样本，实际就是负样本，即负样本被正确识别的数量</span><br><span class="line">FN：False Negative，错误得预测为负样本，实际为正样本，即漏报的数量</span><br><span class="line"></span><br><span class="line">TP+FN：真实正样本的数量</span><br><span class="line">FP+TN：真实负样本的数量</span><br><span class="line">TP+FP：预测为正样本的数量</span><br><span class="line">TN+FN: 预测为负样本的数量</span><br><span class="line">TP+TN: 预测正确的数量</span><br><span class="line">TP+TN+FP+FN: 样本总数量</span><br></pre></td></tr></table></figure>

<h4 id="26-如何查看混淆矩阵"><a href="#26-如何查看混淆矩阵" class="headerlink" title="26.如何查看混淆矩阵"></a>26.如何查看混淆矩阵</h4><p>假设A为正样本：</p>
<table>
<thead>
<tr>
<th></th>
<th>Real A</th>
<th>Real B</th>
</tr>
</thead>
<tbody><tr>
<td>Predict A</td>
<td>10(TP)</td>
<td>20(FP)</td>
</tr>
<tr>
<td>Predict B</td>
<td>30(FN)</td>
<td>5(TN)</td>
</tr>
</tbody></table>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">该类的预测总数：某一行的和</span><br><span class="line">该类的真实总数：某一列的和</span><br><span class="line">预测对的数量：主对角线上的值</span><br><span class="line"></span><br><span class="line">以A为例：</span><br><span class="line">    TP: 预测为A，实际也为A &#x3D; 10</span><br><span class="line">    FP: 预测为A，实际不是A &#x3D; 20</span><br><span class="line">    TN: 预测为B，实际也为B &#x3D; 5</span><br><span class="line">    FN: 预测为B，实际为A &#x3D; 30 </span><br><span class="line">    Accuracy(和某个类别无关)：预测正确的数量 &#x2F; 样本总数量 &#x3D; 10 + 5 &#x2F; 10 + 20 + 30 + 5</span><br><span class="line">    Precision(A)：正确预测为A的数量 &#x2F; 预测为A的数量（行） &#x3D; 10 &#x2F; 10 + 20</span><br><span class="line">    Recall(A): 正确预测为A的数量 &#x2F; 真实为A的数量（列） &#x3D; 10 &#x2F; 10 + 30</span><br></pre></td></tr></table></figure>

<h4 id="27-分类问题的模型评估指标"><a href="#27-分类问题的模型评估指标" class="headerlink" title="27.分类问题的模型评估指标"></a>27.分类问题的模型评估指标</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">注意：每个类别都有自己的查准率、召回率、f1得分</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">假设有100张图片，50张狗（正样本），50张猫（负样本），模型预测结果为60张狗（其中有40张是正确的，还有20张是猫）、40张猫（10张狗 + 30张猫）</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">1.准确率（Accuracy）</span><br><span class="line">  公式：(TP+TN) &#x2F; (TP+TN+FP+FN)</span><br><span class="line">  解释：预测正确的数量 &#x2F; 样本总数量 &#x3D; (40 + 30) &#x2F; 100 &#x3D; 0.7</span><br><span class="line">  特点：如果样本不平衡，则准确率就没有参考价值</span><br><span class="line"></span><br><span class="line">2.查准率（Precision）</span><br><span class="line">  公式：TP &#x2F; (TP+FP) &#x3D; 40 &#x2F; 60 &#x3D; 0.67</span><br><span class="line">  解释：正确预测为正样本的数量 &#x2F; 预测为正样本的数量。Precision越高，表示FP越小，即误报越少</span><br><span class="line">  </span><br><span class="line">3.召回率&#x2F;查全率（Recall）</span><br><span class="line">  公式：TP &#x2F; (TP+FN) &#x3D; 40 &#x2F; 50 &#x3D; 0.8</span><br><span class="line">  解释：正确预测为正样本的数量 &#x2F; 真实正样本的数量。Recall越高，表示FN越小，即漏报越少</span><br><span class="line">  </span><br><span class="line">4.F1分数（F1 Score）</span><br><span class="line">  公式：F1 &#x3D; 2 * Precision * Recall &#x2F; (Precision + Recall)</span><br><span class="line">  解释：F1分数是查准率和召回率的调和平均数，在两者之间取得平衡。适用于需要在查准率和召回率之间权衡的场景。</span><br><span class="line"> </span><br><span class="line">5.混淆矩阵（Confusion Matrix）</span><br><span class="line"> </span><br><span class="line">6.ROC和AUC</span><br><span class="line">  ROC 曲线以FPR为横坐标，以TPR为纵坐标，连接不同阈值下的点绘制而成。</span><br><span class="line">    真正率（TPR）&#x3D; 灵敏度 &#x3D; Recall &#x3D; TP&#x2F;(TP+FN)</span><br><span class="line">    假正率（FPR） &#x3D; 1- 特异度 &#x3D; FP&#x2F;(FP+TN)，代表有多少负样本被错误得预测成了正样本</span><br><span class="line">  ROC 曲线越靠近左上角，说明模型性能越好</span><br><span class="line">  </span><br><span class="line">  AUC是 ROC 曲线下的面积，其值介于 0 和 1 之间，值越大，说明模型性能越好</span><br><span class="line">  </span><br><span class="line">  特点：可以避免样本不平衡的问题，因为TPR只关注正样本，FPR只关注负样本</span><br></pre></td></tr></table></figure>

<h4 id="28-回归问题的损失函数，为何使用平方（MSE）而不是绝对值（MAE）？"><a href="#28-回归问题的损失函数，为何使用平方（MSE）而不是绝对值（MAE）？" class="headerlink" title="28.回归问题的损失函数，为何使用平方（MSE）而不是绝对值（MAE）？"></a>28.回归问题的损失函数，为何使用平方（MSE）而不是绝对值（MAE）？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">因为MSE函数是可微的，而MAE函数不可微，具体的：</span><br><span class="line">1.曲线最低点可导</span><br><span class="line">2.越接近最低点，曲线的坡度逐渐放缓，有助于通过当前的梯度来判断接近最低点的程度</span><br></pre></td></tr></table></figure>
<p><img src="../images/ml_review_mse_loss.png" alt="mse_loss"></p>
<h4 id="29-损失函数和评估函数（指标）的区别？"><a href="#29-损失函数和评估函数（指标）的区别？" class="headerlink" title="29.损失函数和评估函数（指标）的区别？"></a>29.损失函数和评估函数（指标）的区别？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">损失函数作用于训练集，为梯度下降提供方向，用来训练模型参数</span><br><span class="line">评估函数（指标）作用于验证集和测试集，用于评估模型</span><br></pre></td></tr></table></figure>

<h4 id="30-什么是超参数？有哪些常用的超参数调优手段？"><a href="#30-什么是超参数？有哪些常用的超参数调优手段？" class="headerlink" title="30.什么是超参数？有哪些常用的超参数调优手段？"></a>30.什么是超参数？有哪些常用的超参数调优手段？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">超参数是在训练之前，人为预习设定的参数，而不是在训练中获得的参数</span><br><span class="line"></span><br><span class="line">调优手段：</span><br><span class="line">1.网格搜索（Grid Search）</span><br><span class="line">  定义：在预定义的超参数空间内进行穷举搜索，尝试所有可能的组合</span><br><span class="line">  特点：实现简单，但时间长，计算开销大</span><br><span class="line">  </span><br><span class="line">2.随机搜索（Random Search）</span><br><span class="line">  定义：从预定义的超参数空间内随机选择超参数组合进行评估</span><br><span class="line">  特点：相较网格搜索更高效，但可能无法找到最优组合</span><br><span class="line">  </span><br><span class="line">3.贝叶斯优化（Bayesian Optimization）</span><br><span class="line">  定义：一种基于贝叶斯定理的调优方法。它会根据已评估的超参数组合来预测新的超参数组合。</span><br><span class="line">  特点：比网格搜索和随机搜索更有效，但需要更多的计算时间 </span><br><span class="line">  </span><br><span class="line">4.遗传算法（Genetic Algorithms）</span><br><span class="line">  定义：使用自然选择等生物进化的思想来优化超参数</span><br></pre></td></tr></table></figure>

<h4 id="31-有哪些常见的超参数？各自对模型有怎样的影响？"><a href="#31-有哪些常见的超参数？各自对模型有怎样的影响？" class="headerlink" title="31.有哪些常见的超参数？各自对模型有怎样的影响？"></a>31.有哪些常见的超参数？各自对模型有怎样的影响？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">1.学习率（Learning Rate）</span><br><span class="line">  定义：学习率控制着模型参数更新的步长</span><br><span class="line">  影响：学习率太大导致无法收敛，太小导致收敛缓慢</span><br><span class="line">  </span><br><span class="line">2.批量大小（Batch Size）</span><br><span class="line">  定义：在一次迭代中使用的训练样本数量</span><br><span class="line">  影响：较大的batch size可以提高训练速度，但会占用更大的内存和计算资源，较小的则可能导致收敛不稳定</span><br><span class="line">  </span><br><span class="line">3.正则化参数（Regularization Parameter）</span><br><span class="line">  定义：用于控制模型的复杂度，以避免过拟合</span><br><span class="line">  影响：太大可能欠拟合，太小可能过拟合</span><br><span class="line">  </span><br><span class="line">4.隐藏层数量和神经元数量（Number of Hidden Layers and Neurons）</span><br><span class="line">  定义：决定了神经网络的容量和表达能力</span><br><span class="line">  影响：更多的隐藏层和神经元可以捕捉更复杂的模式，但也增加了模型复杂度和过拟合的风险</span><br><span class="line">  </span><br><span class="line">5.迭代次数（Epochs）</span><br><span class="line">  定义：模型训练的轮数</span><br><span class="line">  影响：过少可能导致模型没有学习到数据的特征，过多可能导致过拟合</span><br><span class="line">  </span><br><span class="line">6.优化器（Optimizer）</span><br><span class="line">  定义：优化器决定了模型的权重更新策略</span><br><span class="line">  影响：不同优化器有不同的收敛速度和稳定性，对模型的最终性能影响显著</span><br><span class="line">  </span><br><span class="line">7.权重初始化（Weight Initialization）</span><br><span class="line">  定义：权重初始化影响模型的初始状态，从而影响训练收敛速度和结果</span><br><span class="line">  影响：常见的方法有随机初始化、Xavier初始化和He初始化等。选择合适的初始化方法可以加快收敛速度，避免梯度消失或梯度爆炸</span><br></pre></td></tr></table></figure>

<h4 id="32-什么是置信概率？"><a href="#32-什么是置信概率？" class="headerlink" title="32. 什么是置信概率？"></a>32. 什么是置信概率？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">置信概率指模型对某个预测结果的确信程度，通常，分类模型（如逻辑回归、神经网络、随机森林等）在进行预测时，不仅给出一个类别标签，还会输出每个类别的置信概率。</span><br><span class="line">值越大说明越确定</span><br></pre></td></tr></table></figure>

<h4 id="33-什么是交叉验证？它有哪些常见类型？"><a href="#33-什么是交叉验证？它有哪些常见类型？" class="headerlink" title="33.什么是交叉验证？它有哪些常见类型？"></a>33.什么是交叉验证？它有哪些常见类型？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">交叉验证（Cross-Validation）是一种模型性能评估技术，在样本数量较少的情况下，它将数据集分成多份，每份轮流作为测试集，剩下部分作为训练集，通过这种方式，可以多次评估模型，每次的评估结果综合起来给出模型的总体性能</span><br><span class="line"></span><br><span class="line">常见类型：</span><br><span class="line">1.K折交叉验证（K-Fold CV）</span><br><span class="line">  定义: 将数据集随机分成k个子集（或称为“折”），然后进行k次训练和测试。每次用k-1个子集进行训练，用剩下的一个子集进行测试</span><br><span class="line">  </span><br><span class="line">2.留一法交叉验证（Leave-One-Out Cross-Validation, LOOCV）</span><br><span class="line">  定义: 留一法交叉验证是一种特殊的 K 折交叉验证，其中 k 等于样本数。在留一法交叉验证中，每次只使用一个样本作为测试集，其余样本作为训练集</span><br><span class="line">  特点：最大限度利用数据，但计算量大</span><br></pre></td></tr></table></figure>

<h4 id="34-对于类别不均衡问题，有哪些处理方法？"><a href="#34-对于类别不均衡问题，有哪些处理方法？" class="headerlink" title="34.对于类别不均衡问题，有哪些处理方法？"></a>34.对于类别不均衡问题，有哪些处理方法？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">数据层面：</span><br><span class="line">1.过采样（Oversampling）：增加少数类样本的数量</span><br><span class="line">2.欠采样（Undersampling）：减少多数类样本的数量</span><br><span class="line">3.合成数据（Synthetic Data Generation）：为少数类样本合成新的数据</span><br><span class="line"></span><br><span class="line">算法层面：</span><br><span class="line">1.调整类权重（Class Weight Adjustment）：给少数类样本赋予更高的权重，以增加其对损失函数的影响</span><br><span class="line">2.集成方法（Ensemble Methods）：使用集成学习方法，如Bagging、Boosting来提高模型对少数类的识别能力</span><br><span class="line">3.调整评估指标：使用适合不平衡数据集的评估指标，如PR曲线、F1、ROC、AUC</span><br></pre></td></tr></table></figure>

<h4 id="35-神经网络权重初始值如何设置？"><a href="#35-神经网络权重初始值如何设置？" class="headerlink" title="35.神经网络权重初始值如何设置？"></a>35.神经网络权重初始值如何设置？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1.不能使用零初始化</span><br><span class="line">2.随机初始化：对于小型网络，可以使用高斯分布或均匀分布来初始化权重，这有助于打破网络的对称性</span><br><span class="line">3.Xavier初始化：又称为Glorot初始化，该方法将权重的方差初始化为1&#x2F;输入特征数，当激活函数为sigmoid&#x2F;tanh时，适合用这个</span><br><span class="line">4.He初始化：又称为Delving初始化，该方法将权重的方差初始化为2&#x2F;输入特征数，当激活函数为RELU时，适合用这个</span><br></pre></td></tr></table></figure>

<h4 id="36-什么是线性回归？线性回归的特点是什么？"><a href="#36-什么是线性回归？线性回归的特点是什么？" class="headerlink" title="36.什么是线性回归？线性回归的特点是什么？"></a>36.什么是线性回归？线性回归的特点是什么？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">定义：用于分析两个或多个变量之间的关系的机器学习方法。它通过拟合一条直线来表示自变量和因变量之间的线性关系</span><br><span class="line">公式：y &#x3D; w0 + w1 * x1 + w2 * x2 + ... + wn * xn + b</span><br><span class="line">特点：容易计算和实现、难以很好地表达非线性的数据</span><br></pre></td></tr></table></figure>

<h4 id="37-什么是多项式回归？多项式回归的特点是什么？"><a href="#37-什么是多项式回归？多项式回归的特点是什么？" class="headerlink" title="37.什么是多项式回归？多项式回归的特点是什么？"></a>37.什么是多项式回归？多项式回归的特点是什么？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">定义：多项式回归是一种扩展的线性回归模型，用于处理自变量与因变量之间的非线性关系。虽然模型仍然是线性模型（因为参数是线性的），但它引入了自变量的多项式来捕捉更复杂的模式</span><br><span class="line">公式：y &#x3D; w0 + w1 * x + w2 * x^2 + ... + wn * x^n + b</span><br><span class="line">特点：可以拟合非线性关系</span><br></pre></td></tr></table></figure>

<h4 id="38-什么是决策树？工作原理是什么？特点？"><a href="#38-什么是决策树？工作原理是什么？特点？" class="headerlink" title="38.什么是决策树？工作原理是什么？特点？"></a>38.什么是决策树？工作原理是什么？特点？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">定义：通过树形结构来表示决策过程，每个节点表示一个属性，每个分支表示一个属性值，叶节点表示一个类或决策结果</span><br><span class="line"></span><br><span class="line">工作原理：</span><br><span class="line">1.选择特征：从数据集中选择一个特征进行分割。常用的标准包括信息增益、信息增益率、基尼指数等。</span><br><span class="line">2.分割数据：根据选择的特征，将数据分割成不同的子集。</span><br><span class="line">3.递归构建：对每个子集重复上述过程，直到满足停止条件，如达到最大深度或子集中没有足够的数据点。</span><br><span class="line">4.形成叶节点：当达到停止条件时，将当前节点设为叶节点，并分配一个预测值。</span><br><span class="line"></span><br><span class="line">特点：</span><br><span class="line">易于理解和解释、不需要预处理数据、可用于分类和回归任务</span><br><span class="line">受数据影响大（高方差），容易出现过拟合（特征多，参数复杂，缺乏正则化手段）</span><br></pre></td></tr></table></figure>

<h4 id="39-有哪些常见的决策树算法？"><a href="#39-有哪些常见的决策树算法？" class="headerlink" title="39.有哪些常见的决策树算法？"></a>39.有哪些常见的决策树算法？</h4><p><img src="../images/ml_review_decision_tree.png" alt="decision_tree"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ID3：基于信息增益来选择分裂属性（每步选择信息增益最大的属性作为分裂节点，树可能是多叉的）。</span><br><span class="line">C4.5：基于信息增益率来选择分裂属性（每步选择信息增益率最大的属性作为分裂节点，树可能是多叉的）。</span><br><span class="line">CART(Classification And Regression Tree)：基于基尼系数&#x2F;均方差来构建决策树（每步要求基尼系数最小，树是二叉的</span><br></pre></td></tr></table></figure>

<h4 id="40-CART-在分类问题和回归问题中的异同"><a href="#40-CART-在分类问题和回归问题中的异同" class="headerlink" title="40.CART 在分类问题和回归问题中的异同"></a>40.CART 在分类问题和回归问题中的异同</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">相同：</span><br><span class="line">• 在分类问题和回归问题中，CART 都是一棵二叉树，除叶子节点外的所有节点都有且仅有两个子节点；</span><br><span class="line">• 所有落在同一片叶子中的输入都有同样的输出。</span><br><span class="line"></span><br><span class="line">不同：</span><br><span class="line">• 在分类问题中，CART 使用基尼指数（Gini index）作为选择特征（feature）和划分（split）的依据；在回归问题中，CART 使用 mse（mean square error）或者 mae（mean absolute error）作为选择 feature 和 split 的 criteria。</span><br><span class="line">• 在分类问题中，CART 的每一片叶子都代表的是一个 class；在回归问题中，CART 的每一片叶子表示的是一个预测值，取值是连续的。</span><br></pre></td></tr></table></figure>

<h4 id="41-什么是集成学习？"><a href="#41-什么是集成学习？" class="headerlink" title="41.什么是集成学习？"></a>41.什么是集成学习？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">集成学习就是组合多个弱监督模型，以期望得到一个更好的强监督模型。集成学习潜在的思想是即便某一个弱分类器得到了错误的预测，其他分类器也可以将错误纠正回来</span><br><span class="line">类型：Bagging、Boosting、Stacking</span><br><span class="line">关键点：</span><br><span class="line">  1.弱学习器不能太“弱”，需要有一定的准确性</span><br><span class="line">  2.弱学习器之间要具有“多样性”，即弱学习器之间存在差异性</span><br></pre></td></tr></table></figure>

<h4 id="42-Bagging与Boosting的原理是什么？二者有何区别？"><a href="#42-Bagging与Boosting的原理是什么？二者有何区别？" class="headerlink" title="42.Bagging与Boosting的原理是什么？二者有何区别？"></a>42.Bagging与Boosting的原理是什么？二者有何区别？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Bagging: 利用bootstrap方法从整体数据集中采取有放回抽样得到N个数据集，在每个数据集上学习出一个模型，最后的预测结果利用N个模型的输出得到；具体地：分类问题采用N个模型预测投票的方式，回归问题采用N个模型预测平均的方式。通过随机抽取数据的方式减少了可能的数据干扰，因此Bagging模型具有低方差</span><br><span class="line">Boosting: 思路是逐步优化模型，持续地通过新模型来优化同一个基模型，从而不断减小模型的预测误差（偏差）</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">区别：</span><br><span class="line">1.Bagging中每个训练集互不相关，也就是每个基分类器互不相关，而Boosting中训练集要在上一轮的结果上进行调整，也使得其不能并行计算；</span><br><span class="line">2.Bagging中预测函数是均匀平等的，但在Boosting中预测函数是加权的</span><br><span class="line">3.从偏差-方差分解角度看，Bagging主要关注降低方差，而Boosting主要关注降低偏差</span><br></pre></td></tr></table></figure>

<h4 id="43-机器学习模型的偏差和方差是什么？"><a href="#43-机器学习模型的偏差和方差是什么？" class="headerlink" title="43.机器学习模型的偏差和方差是什么？"></a>43.机器学习模型的偏差和方差是什么？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">偏差：评判的是机器学习模型的准确度，偏差越小，模型越准确。它度量了算法的预测与真实结果的离散程度，刻画了学习算法本身的拟合能力。也就是每次打靶都比较准，比较靠近靶心。</span><br><span class="line">方差：评判的是机器学习模型的稳定性(或称精度)，方差越小，模型越稳定。它度量了训练集变动所导致的学习性能变化，刻画了数据扰动所造成的影响。也就是每次打靶，不管打得准不准，击中点都比较集中</span><br></pre></td></tr></table></figure>

<h4 id="44-什么是基于决策树的集合算法-集成学习-？"><a href="#44-什么是基于决策树的集合算法-集成学习-？" class="headerlink" title="44.什么是基于决策树的集合算法(集成学习)？"></a>44.什么是基于决策树的集合算法(集成学习)？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">基于决策树构建的Bagging或Boosting类模型。</span><br></pre></td></tr></table></figure>

<h4 id="45-简要介绍AdaBoost、GBDT、XGBoost"><a href="#45-简要介绍AdaBoost、GBDT、XGBoost" class="headerlink" title="45.简要介绍AdaBoost、GBDT、XGBoost"></a>45.简要介绍AdaBoost、GBDT、XGBoost</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">AdaBoost: 它通过对多个弱分类器进行加权组合来提高最终分类器的性能，具体的：在每一轮迭代中，样本的权重会根据前一轮的分类结果进行调整。分类错误的样本权重会增加，分类正确的样本权重会减小，以使后续的分类器更关注前一轮分类错误的样本</span><br><span class="line">GBDT: Gradient Boosting Decision Tree, 它通过构造一系列的决策树，每一棵树都会针对前一棵树的残差进行优化，以此来减少整个模型的误差。</span><br><span class="line">XGBoost: 是GBDT的改进版本，使用了二阶导数来优化决策树的划分点、添加正则化项、支持并行计算，来提升模型的性能</span><br></pre></td></tr></table></figure>

<h4 id="46-什么是随机森林？"><a href="#46-什么是随机森林？" class="headerlink" title="46.什么是随机森林？"></a>46.什么是随机森林？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RF是一种集成学习算法，在以决策树作为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中加入了随机属性的选择,由此，随机森林的基学习器的“多样性”不仅来自样本的扰动，还来自属性的扰动，使得最终集成的泛化能力进一步增强</span><br></pre></td></tr></table></figure>

<h4 id="47-什么是逻辑回归？它是如何实现二分类的？"><a href="#47-什么是逻辑回归？它是如何实现二分类的？" class="headerlink" title="47.什么是逻辑回归？它是如何实现二分类的？"></a>47.什么是逻辑回归？它是如何实现二分类的？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">逻辑回归是一种广义的线性回归，其原理是利用线性模型根据输入计算输出，然后在逻辑函数（sigmoid）和阈值作用下，将连续值转换为两个离散值（0或1），从而实现二分类</span><br></pre></td></tr></table></figure>

<h4 id="48-什么是逻辑函数（sigmoid）？它有什么特点？"><a href="#48-什么是逻辑函数（sigmoid）？它有什么特点？" class="headerlink" title="48.什么是逻辑函数（sigmoid）？它有什么特点？"></a>48.什么是逻辑函数（sigmoid）？它有什么特点？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">sigmoid函数能将(−∞,+∞)的值映射到(0,1)之间，通过选取合适的阈值转换为两个</span><br><span class="line">离散值，从而实现二分类</span><br><span class="line">公式：f(x) &#x3D; 1 &#x2F; (1 + exp(-x))</span><br><span class="line">导数：f&#39;(x) &#x3D; f(x) * (1 - f(x))</span><br><span class="line"></span><br><span class="line">代码实现：</span><br><span class="line">def sigmoid(x):</span><br><span class="line">    return 1 &#x2F; (1 + np.exp(-x))</span><br><span class="line">    </span><br><span class="line">特点：</span><br><span class="line">1.函数可微</span><br><span class="line">2.输出范围为(0,1)，可以解释为概率，为分类做准备</span><br><span class="line">3.单调递增：输入值越大，输出值越接近1；输入值越小，输出值越接近0</span><br></pre></td></tr></table></figure>

<h4 id="49-什么是信息熵？"><a href="#49-什么是信息熵？" class="headerlink" title="49.什么是信息熵？"></a>49.什么是信息熵？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">信息熵（information entropy）是度量样本集合纯度的常用指标，该值越大，表示该集合纯度越低（或越混乱），该值越小，表示该集合纯度越高（或越有序）</span><br></pre></td></tr></table></figure>

<h4 id="50-什么是交叉熵损失函数？"><a href="#50-什么是交叉熵损失函数？" class="headerlink" title="50.什么是交叉熵损失函数？"></a>50.什么是交叉熵损失函数？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">交叉熵（Cross Entropy Loss）是一种在机器学习和深度学习中广泛使用的损失函数，主要用来衡量真实概率与预测概率之间的差异</span><br><span class="line">二分类公式：−(ylog(p)+(1−y)log(1−p))</span><br><span class="line">多分类公式：- ∑(y(i) * log(p(i)))</span><br><span class="line">公式解释：y为真实标签（0或1），p为模型输出的预测值，i为第i个类别</span><br><span class="line"></span><br><span class="line">代码实现：</span><br><span class="line">def cross_entropy_error(p, y):</span><br><span class="line">    delta &#x3D; 1e-7 # 防止当出现np.log(0)时，np.log(0)会变为负无限大</span><br><span class="line">    return -np.sum(y * np.log(p + delta))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 神经网络版</span><br><span class="line">def cross_entropy_error(y, t):</span><br><span class="line">    if y.ndim &#x3D;&#x3D; 1:</span><br><span class="line">        t &#x3D; t.reshape(1, t.size)</span><br><span class="line">        y &#x3D; y.reshape(1, y.size)</span><br><span class="line"></span><br><span class="line">    # 在监督标签为one-hot-vector的情况下，转换为正确解标签的索引</span><br><span class="line">    if t.size &#x3D;&#x3D; y.size:</span><br><span class="line">        t &#x3D; t.argmax(axis&#x3D;1)</span><br><span class="line"></span><br><span class="line">    batch_size &#x3D; y.shape[0]</span><br><span class="line"></span><br><span class="line">    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) &#x2F; batch_size</span><br></pre></td></tr></table></figure>

<h4 id="51-什么是朴素贝叶斯分类？特点是什么？何时使用？"><a href="#51-什么是朴素贝叶斯分类？特点是什么？何时使用？" class="headerlink" title="51.什么是朴素贝叶斯分类？特点是什么？何时使用？"></a>51.什么是朴素贝叶斯分类？特点是什么？何时使用？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">朴素贝叶斯分类（Naive Bayes Classifier）是一种基于贝叶斯定理的概率分类方法。它假设特征之间是相互独立的，这一假设称为“朴素”假设。</span><br><span class="line"></span><br><span class="line">原理：</span><br><span class="line">1.对于给定的待分类样本，计算它属于每个类的后验概率。</span><br><span class="line">2.选择具有最大后验概率的类作为该样本的预测类别</span><br><span class="line"></span><br><span class="line">特点：</span><br><span class="line">1.简单易实现</span><br><span class="line">2.计算效率高</span><br><span class="line">3.独立性假设不现实：特征之间往往存在相关性，这一假设在许多情况下并不成立</span><br><span class="line"></span><br><span class="line">何时使用：</span><br><span class="line">根据先验概率计算后验概率的情况，且样本特征之间独立性较强</span><br></pre></td></tr></table></figure>

<h4 id="52-常见的朴素贝叶斯分类器有哪些？"><a href="#52-常见的朴素贝叶斯分类器有哪些？" class="headerlink" title="52.常见的朴素贝叶斯分类器有哪些？"></a>52.常见的朴素贝叶斯分类器有哪些？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">高斯朴素贝叶斯（Gaussian Naive Bayes）：假设特征服从高斯分布，常用于连续数据</span><br><span class="line">多项式朴素贝叶斯（Multinomial Naive Bayes）：适用于离散数据，常用于文本分类</span><br><span class="line">伯努利朴素贝叶斯（Bernoulli Naive Bayes）：适用于二元离散值或是稀疏的多元离散值</span><br></pre></td></tr></table></figure>

<h4 id="53-什么是支持向量机？"><a href="#53-什么是支持向量机？" class="headerlink" title="53.什么是支持向量机？"></a>53.什么是支持向量机？</h4><p><img src="../images/ml_review_svm.png" alt="svm"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">支持向量机（Support Vector Machine, SVM）是一种用于分类和回归的监督学习模型。它的基本思想是找到一个能够最大程度分离不同类别数据的超平面</span><br><span class="line"></span><br><span class="line">超平面（Hyperplane）：在n维空间中的一个具有n-1维的几何结构</span><br><span class="line">间隔（Margin）：是指超平面与离它最近的训练样本之间的距离。SVM通过最大化这个间隔来实现最优分类</span><br><span class="line">支持向量（Support Vectors）：是指位于边界上或边界附近的训练样本点。这些点对定义超平面的位置起关键作用</span><br></pre></td></tr></table></figure>

<h4 id="54-SVM寻找最优边界要求有哪些？"><a href="#54-SVM寻找最优边界要求有哪些？" class="headerlink" title="54.SVM寻找最优边界要求有哪些？"></a>54.SVM寻找最优边界要求有哪些？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">（1）正确性：对大部分样本都可以正确划分类别；</span><br><span class="line">（2）安全性：离支持向量的距离最远；</span><br><span class="line">（3）公平性：支持向量与分类边界的距离相等；</span><br><span class="line">（4）简单性：采用线性方程表示分类边界。如果在原始维度中无法做线性划分，那么就通过升维变换，在更高维度空间寻求线性分割超平面</span><br></pre></td></tr></table></figure>

<h4 id="55-SVM的工作原理？"><a href="#55-SVM的工作原理？" class="headerlink" title="55.SVM的工作原理？"></a>55.SVM的工作原理？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">线性可分情况：如果数据可以用一个直线（或超平面）完全分开，SVM会找到那个使得两个类别之间的间隔最大的超平面</span><br><span class="line">线性不可分情况：SVM通过引入软间隔（Soft Margin）和核函数（Kernel Function）来处理。软间隔允许一些数据点位于错误的一侧，但通过引入惩罚项来最小化错误分类的影响。核函数则通过将数据映射到高维空间，使得在高维空间中数据变得线性可分</span><br></pre></td></tr></table></figure>

<h4 id="56-SVM中的核函数是什么？常用的核函数有哪些？"><a href="#56-SVM中的核函数是什么？常用的核函数有哪些？" class="headerlink" title="56.SVM中的核函数是什么？常用的核函数有哪些？"></a>56.SVM中的核函数是什么？常用的核函数有哪些？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">主要作用是将原始特征空间中的数据映射到一个更高维的空间，使得在这个高维空间中，数据线性可分</span><br><span class="line"></span><br><span class="line">常用：</span><br><span class="line">1.线性核函数（Linear Kernel）：实际上没有进行升维，直接在原始空间中进行计算，适用于线性可分的数据</span><br><span class="line">2.多项式核函数（Polynomial Kernel）：用增加高次项特征的方法做升维变换，当多项式阶数高时复杂度会很高</span><br><span class="line">3.径向基核函数（Radial Basis Function, RBF）：又称高斯核函数，通过计算样本之间的欧几里得距离来实现映射，灵活性好，比多项式核函数参数少</span><br></pre></td></tr></table></figure>

<h4 id="57-SVM的特点？"><a href="#57-SVM的特点？" class="headerlink" title="57.SVM的特点？"></a>57.SVM的特点？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">优点：</span><br><span class="line">有严格的数学理论支持，可解释性强</span><br><span class="line">采用核函数之后，可以处理非线性分类&#x2F;回归任务</span><br><span class="line"></span><br><span class="line">缺点：</span><br><span class="line">训练时间长</span><br><span class="line">样本较多时，效率不高，故只适合小批量样本的任务</span><br></pre></td></tr></table></figure>

<h4 id="58-什么是聚类？"><a href="#58-什么是聚类？" class="headerlink" title="58.什么是聚类？"></a>58.什么是聚类？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">聚类是一种无监督学习方法，根据数据集中样本相似性，将它们分到不同的簇中，同一个簇中的样本之间相似度较高，不同簇之间的样本相似度较低</span><br><span class="line"></span><br><span class="line">主要方法：</span><br><span class="line">原型聚类：K-means</span><br><span class="line">密度聚类：DBSCAN</span><br><span class="line">层次聚类：凝聚层次</span><br></pre></td></tr></table></figure>

<h4 id="59-有哪些常用的相似度度量方式？"><a href="#59-有哪些常用的相似度度量方式？" class="headerlink" title="59.有哪些常用的相似度度量方式？"></a>59.有哪些常用的相似度度量方式？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">1.欧氏距离（Euclidean Distance）：L2距离</span><br><span class="line">公式：d(a, b) &#x3D; √(Σ(xi - xj)²)</span><br><span class="line"></span><br><span class="line">2.曼哈顿距离（Manhattan Distance）：L1距离</span><br><span class="line">公式：d(a, b) &#x3D; Σ|xi - xj|</span><br><span class="line"></span><br><span class="line">3.切比雪夫距离（Chebyshev Distance）： L∞ 距离</span><br><span class="line">公式：d(a, b) &#x3D; max(|xi - xj|)</span><br><span class="line"></span><br><span class="line">4.余弦相似度（Cosine Similarity）：表示两个向量的夹角余弦值</span><br><span class="line">公式：cos(θ) &#x3D; a·b &#x2F; ||a|| ||b||</span><br><span class="line">解释：a·b是点积（dot product），||a|| 和 ||b|| 分别为 a 和 b 的欧氏模长，||a|| &#x3D; √(a1^2 + a2^2 + ... + an^2)</span><br><span class="line"></span><br><span class="line">5.杰卡德相似系数（Jaccard Similarity Coefficient）：表示两个集合的交集与并集之比（与目标检测中的IOU概念相同）</span><br><span class="line">公式：J(A, B) &#x3D; |A ∩ B| &#x2F; |A ∪ B|</span><br></pre></td></tr></table></figure>

<h4 id="60-聚类问题的评价指标是什么？"><a href="#60-聚类问题的评价指标是什么？" class="headerlink" title="60.聚类问题的评价指标是什么？"></a>60.聚类问题的评价指标是什么？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">轮廓系数（Silhouette Coefficient）：综合考虑簇内紧密程度和簇间分离程度来衡量聚类效果，取值[-1,1]，越接近1越好</span><br><span class="line">公式：s(i) &#x3D; (b(i) - a(i)) &#x2F; max(b(i),a(i))</span><br><span class="line">解释：</span><br><span class="line">  a(i) &#x3D; average(i向量到所有它属于的簇中其它点的距离)</span><br><span class="line">  b(i) &#x3D; min (i向量到各个非本身所在簇的所有点的平均距离)</span><br></pre></td></tr></table></figure>

<h4 id="61-什么是K-Means聚类？"><a href="#61-什么是K-Means聚类？" class="headerlink" title="61.什么是K-Means聚类？"></a>61.什么是K-Means聚类？</h4><p><img src="../images/ml_review_kmeans.png" alt="kmeans"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">K-Means聚类是一种基于原型的聚类算法，通过迭代的方式，将每个数据点分配到K个预定义的簇中，目标是最小化每个簇内点与簇中心的距离之和</span><br><span class="line"></span><br><span class="line">步骤：</span><br><span class="line">1.确定簇的数量K：首先决定要分成的簇的数量K。</span><br><span class="line">2.初始化质心：随机选择K个点作为初始质心。</span><br><span class="line">3.分配数据点：将每个数据点分配到最近的质心，形成K个簇。</span><br><span class="line">4.更新质心：计算每个簇的质心，即簇内所有数据点的平均值。</span><br><span class="line">5.重复步骤3和4：不断重新分配数据点并更新质心，直到质心不再变化或达到预定的迭代次数。</span><br><span class="line"></span><br><span class="line">优点：</span><br><span class="line">简单易理解</span><br><span class="line">计算效率高</span><br><span class="line"></span><br><span class="line">缺点：</span><br><span class="line">需要预先设定K值</span><br><span class="line">对初始质心敏感</span><br><span class="line"></span><br><span class="line">何时使用：</span><br><span class="line">已知K值、数据分布有明显的中心</span><br></pre></td></tr></table></figure>

<h4 id="62-什么是DBSCAN（噪声密度）？"><a href="#62-什么是DBSCAN（噪声密度）？" class="headerlink" title="62.什么是DBSCAN（噪声密度）？"></a>62.什么是DBSCAN（噪声密度）？</h4><p><img src="../images/ml_review_dbscan.png" alt="DBSCAN"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">DBSCAN是一种基于密度的聚类算法，用于发现数据中的簇和噪声。DBSCAN与K-Means不同，不需要预先指定簇的数量，并且能够有效处理噪声点</span><br><span class="line"></span><br><span class="line">主要概念：</span><br><span class="line">• 核心点（Core point）：一个点的邻域内包含的点数大于等于MinPts个点，该点被称为核心点；</span><br><span class="line">• 边界点（Border point）：一个点的邻域内包含的点数少于MinPts，但该点位于某个核心点的邻域内；</span><br><span class="line">• 噪声点（Noise）：既不是核心点，也不是边界点的点；</span><br><span class="line"></span><br><span class="line">步骤：</span><br><span class="line">1.初始化：选择一个未访问的样本点p。</span><br><span class="line">2.检查核心点：如果p的邻域内的样本点数目大于等于MinPts，则p是一个核心点，创建一个新的簇C，并将p及其邻域内的所有点加入簇C。</span><br><span class="line">3.扩展簇：对于簇C中的每个点q，如果q也是核心点，则将q的邻域内的所有点也加入簇C。重复此过程，直到簇C不再扩展。</span><br><span class="line">4.处理剩余点：选择下一个未访问的点，重复步骤2和3，直到所有点都被访问过</span><br><span class="line"></span><br><span class="line">优点：</span><br><span class="line">可自动确定簇的数量</span><br><span class="line">可以发现形状不规则的聚类</span><br><span class="line">能够有效处理噪声点</span><br><span class="line"></span><br><span class="line">缺点：</span><br><span class="line">参数敏感：邻域半径和最少样本数量两个参数对聚类结果影响较大</span><br><span class="line"></span><br><span class="line">何时使用：</span><br><span class="line">（1）数据没有明显中心</span><br><span class="line">（2）噪声数据较多</span><br><span class="line">（3）未知聚簇的数量</span><br></pre></td></tr></table></figure>

<h4 id="63-什么是凝聚层次算法？"><a href="#63-什么是凝聚层次算法？" class="headerlink" title="63.什么是凝聚层次算法？"></a>63.什么是凝聚层次算法？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">凝聚层次算法（Agglomerative Clustering）是一种基于层次的聚类方法，主要用于数据点或簇的逐步合并，以形成层次结构。其核心思想是从每个数据点作为一个独立的簇开始，然后逐步合并最相似的簇，直到满足某种终止条件，如达到预设的簇数量或所有数据点都在同一个簇中</span><br><span class="line"></span><br><span class="line">步骤：</span><br><span class="line">1.初始化：将每个数据点视为一个单独的簇。</span><br><span class="line">2.计算相似度：计算所有簇之间的相似度，通常基于距离度量。</span><br><span class="line">3.合并簇：选择最相似的两个簇进行合并，形成一个新的更大的簇。</span><br><span class="line">4.重复过程：重复上述步骤，直到满足终止条件</span><br><span class="line"></span><br><span class="line">优点：</span><br><span class="line">没有聚类中心，不依赖中心的选择</span><br><span class="line">不需要事先指定簇的数量K</span><br><span class="line"></span><br><span class="line">缺点：</span><br><span class="line">计算复杂度高</span><br><span class="line">对噪声敏感</span><br></pre></td></tr></table></figure>

<h4 id="64-什么是神经网络？它有哪些常见类型？"><a href="#64-什么是神经网络？它有哪些常见类型？" class="headerlink" title="64.什么是神经网络？它有哪些常见类型？"></a>64.什么是神经网络？它有哪些常见类型？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">神经网络是一种模拟生物大脑结构和功能的机器学习技术，它由多个人工神经元组成，可以学习复杂的模式并作出预测</span><br><span class="line"></span><br><span class="line">常见类型：</span><br><span class="line">前馈神经网络（Feedforward Neural Networks, FNN）： 信息单向流动的网络，没有循环或反馈，最基础的神经网络类型</span><br><span class="line">卷积神经网络（Convolutional Neural Networks, CNN）： 常用于图像和视频处理，具有卷积层和池化层，擅长提取局部特征</span><br><span class="line">循环神经网络（Recurrent Neural Networks, RNN）： 具有循环连接，能够处理序列数据，如时间序列和自然语言处理。LSTM和GRU是RNN的改进版本</span><br></pre></td></tr></table></figure>

<h4 id="65-神经网络中的权重和偏置是什么？"><a href="#65-神经网络中的权重和偏置是什么？" class="headerlink" title="65.神经网络中的权重和偏置是什么？"></a>65.神经网络中的权重和偏置是什么？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">权重：控制输入信号重要性</span><br><span class="line">偏置：控制神经元被激活的难易程度</span><br></pre></td></tr></table></figure>

<h4 id="66-深度学习的优缺点是什么？"><a href="#66-深度学习的优缺点是什么？" class="headerlink" title="66.深度学习的优缺点是什么？"></a>66.深度学习的优缺点是什么？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">优点：</span><br><span class="line">1.擅长处理复杂数据</span><br><span class="line">2.自动特征提取</span><br><span class="line">3.高精度高性能</span><br><span class="line"></span><br><span class="line">缺点：</span><br><span class="line">1.对数据质量要求高</span><br><span class="line">2.模型复杂</span><br><span class="line">3.难以解释</span><br></pre></td></tr></table></figure>

<h4 id="67-什么是激活函数？为什么要使用激活函数？"><a href="#67-什么是激活函数？为什么要使用激活函数？" class="headerlink" title="67.什么是激活函数？为什么要使用激活函数？"></a>67.什么是激活函数？为什么要使用激活函数？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">在神经网络中，将输入信号的总和转换为输出信号的函数被称为激活函数（activation function）</span><br><span class="line">激活函数可以引入非线性因素，增强模型的表达能力，选择合适的激活函数还可以避免梯度消失或梯度爆炸问题</span><br></pre></td></tr></table></figure>

<h4 id="68-神经网络中常用的激活函数有哪些，各自有什么特点？"><a href="#68-神经网络中常用的激活函数有哪些，各自有什么特点？" class="headerlink" title="68.神经网络中常用的激活函数有哪些，各自有什么特点？"></a>68.神经网络中常用的激活函数有哪些，各自有什么特点？</h4><p><img src="../images/ml_review_sigmoid.png" alt="sigmoid"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1.sigmoid: 又叫逻辑(Logistic)函数，能将(-∞, +∞)的数值映射到(0, 1)的区间，可以用来做二分类</span><br><span class="line">输出范围：(0, 1)</span><br><span class="line">公式：f(x) &#x3D; 1 &#x2F; (1 + exp(-x))</span><br><span class="line">导数：f&#39;(x) &#x3D; f(x) * (1 - f(x))</span><br><span class="line">优点：平滑、易于求导</span><br><span class="line">缺点：容易出现梯度消失</span><br></pre></td></tr></table></figure>
<p><img src="../images/ml_review_tanh.png" alt="tanh"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">2.tanh: 双曲正切函数</span><br><span class="line">输出范围：(-1, 1)</span><br><span class="line">公式：f(x) &#x3D; (1 - exp(-2x)) &#x2F; (1 + exp(-2x))</span><br><span class="line">导数：f&#39;(x) &#x3D; 1 - f(x)^2</span><br><span class="line">优点：比sigmoid收敛快</span><br><span class="line">缺点：仍有轻微梯度消失</span><br></pre></td></tr></table></figure>
<p><img src="../images/ml_review_relu_variant.png" alt="leaky relu variant"><br><img src="../images/ml_review_relu.png" alt="relu"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">3.relu: Rectified Linear Units，修正线性单元，</span><br><span class="line">输出范围：[0,+∞)</span><br><span class="line">公式：relu(x) &#x3D; max(0, x)</span><br><span class="line">导数：f&#39;(x) &#x3D; 1 if x &gt; 0 else 0</span><br><span class="line">优点：计算速度快，避免了梯度消失问题（它的梯度在 x&gt;0 时始终为1）</span><br><span class="line">缺点：Dead ReLU问题：当输入为负时，梯度为0，导致神经元不再更新</span><br><span class="line"></span><br><span class="line">代码实现：</span><br><span class="line">def relu(x):</span><br><span class="line">    return np.maximum(0, x)</span><br></pre></td></tr></table></figure>
<p><img src="../images/ml_review_leaky_relu.png" alt="leaky_relu"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">4.Leaky ReLU</span><br><span class="line">输出范围：(-∞, +∞)</span><br><span class="line">公式：f(x) &#x3D; max(α * x, x)，α通常是一个小于1的常数（如0.01）</span><br><span class="line">导数：f&#39;(x) &#x3D; 1 if x &gt; 0 else α</span><br><span class="line">优点：解决了Dead ReLU问题</span><br><span class="line">缺点：需要额外参数α</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">5.ELU</span><br><span class="line">公式：x if x &gt; 0 else a(exp(x) - 1)</span><br><span class="line">优点：ReLU的梯度消失问题，同时还具有平滑性</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">6.softmax</span><br><span class="line">输出范围：(0,1)，总和为1</span><br><span class="line">公式：f(x) &#x3D; exp(x) &#x2F; sum(exp(x))；分子是输入信号的指数函数，分母是所有输入信号的指数函数之和</span><br><span class="line">特点：可以将多分类的输出数值转化为相对概率，而这些值的累和为1</span><br><span class="line"></span><br><span class="line">代码实现：</span><br><span class="line">def softmax(x):</span><br><span class="line">    if x.ndim &#x3D;&#x3D; 2:</span><br><span class="line">        x &#x3D; x - x.max(axis&#x3D;1, keepdims&#x3D;True)  # 防止溢出</span><br><span class="line">        x &#x3D; np.exp(x)</span><br><span class="line">        x &#x2F;&#x3D; x.sum(axis&#x3D;1, keepdims&#x3D;True)</span><br><span class="line">    elif x.ndim &#x3D;&#x3D; 1:</span><br><span class="line">        x &#x3D; x - np.max(x)  # 防止溢出</span><br><span class="line">        x &#x3D; np.exp(x) &#x2F; np.sum(np.exp(x))</span><br><span class="line">    return x</span><br></pre></td></tr></table></figure>
<p><img src="../images/ml_review_mish.png" alt="Mish"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">7.Mish</span><br><span class="line">公式：Mish(x)&#x3D;x⋅tanh(ln(1+exp(x)))</span><br></pre></td></tr></table></figure>
<p><img src="../images/ml_review_swish.png" alt="Swish"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">8.Swish</span><br><span class="line">公式：f(x) &#x3D; x * sigmoid(x)</span><br></pre></td></tr></table></figure>

<h4 id="69-激活函数的比较"><a href="#69-激活函数的比较" class="headerlink" title="69.激活函数的比较"></a>69.激活函数的比较</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">1)Sigmoid和RELU: Sigmoid容易出现梯度消失， RELU修正了梯度消失的问题</span><br><span class="line"></span><br><span class="line">2)Sigmoid和tanh</span><br><span class="line">  - Sigmoid范围0~1，均值点为0.5; tanh范围-1~1，均值点为0</span><br><span class="line">  - tanh收敛速度比sigmoid快</span><br><span class="line"></span><br><span class="line">3)Sigmoid和softmax: Sigmoid用于二分类，softmax用于多分类</span><br><span class="line"></span><br><span class="line">4)softmax和Relu: softmax主要用于输出层，Relu主要用于隐藏层</span><br></pre></td></tr></table></figure>

<h4 id="70-什么是损失函数？损失函数的作用是什么？"><a href="#70-什么是损失函数？损失函数的作用是什么？" class="headerlink" title="70.什么是损失函数？损失函数的作用是什么？"></a>70.什么是损失函数？损失函数的作用是什么？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">损失函数（Loss Function），又称代价函数（Cost Function），用来度量预测值和实际值之间的差异，在模型训练中，为梯度下降更新参数指明方向</span><br></pre></td></tr></table></figure>

<h4 id="71-什么是梯度？什么是梯度下降？"><a href="#71-什么是梯度？什么是梯度下降？" class="headerlink" title="71.什么是梯度？什么是梯度下降？"></a>71.什么是梯度？什么是梯度下降？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">梯度是一个向量，它表示一个函数在某一点的方向导数。简单来说，梯度告诉我们函数在某一点的增长最快的方向</span><br><span class="line">梯度下降(Gradient Descent)是一种优化算法，用于寻找损失函数的极小值。其基本思想是，从一个初始点开始，沿着梯度的负方向逐步移动，从而使函数值逐步减少，直到达到某个极小值</span><br><span class="line"></span><br><span class="line">步骤：</span><br><span class="line">1）求损失函数值</span><br><span class="line">2）是否达到停止条件？如果不是，计算损失函数的梯度</span><br><span class="line">3）按梯度的反方向走一小步（调整权重，Wi+1&#x3D;Wi−η∇f(Wi)）</span><br><span class="line">4）重复2～3步</span><br></pre></td></tr></table></figure>

<h4 id="72-什么是梯度消失？如何解决梯度消失问题？"><a href="#72-什么是梯度消失？如何解决梯度消失问题？" class="headerlink" title="72.什么是梯度消失？如何解决梯度消失问题？"></a>72.什么是梯度消失？如何解决梯度消失问题？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">梯度消失问题指的是在反向传播过程中，梯度在层与层之间逐渐变小，最终导致靠近输入层的隐层的权重更新非常缓慢甚至几乎不更新，使得模型难以学习到数据特征</span><br><span class="line"></span><br><span class="line">解决方法：</span><br><span class="line">1.选择合适的激活函数，如ReLU、Leaky ReLU等</span><br><span class="line">2.选择合适的权重初始化方式</span><br><span class="line">3.使用残差结构</span><br><span class="line">4.使用批量归一化（Batch Normalization）</span><br></pre></td></tr></table></figure>

<h4 id="73-什么是梯度爆炸？如何解决梯度爆炸问题？"><a href="#73-什么是梯度爆炸？如何解决梯度爆炸问题？" class="headerlink" title="73.什么是梯度爆炸？如何解决梯度爆炸问题？"></a>73.什么是梯度爆炸？如何解决梯度爆炸问题？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">梯度爆炸是指在神经网络训练过程中，梯度在反向传播的过程中不断累积，导致梯度值变得非常大，从而使权重更新幅度过大，导致网络训练不稳定，甚至无法收敛</span><br><span class="line"></span><br><span class="line">解决方法：</span><br><span class="line">1.动态调整学习率</span><br><span class="line">2.选择合适的激活函数</span><br><span class="line">3.梯度剪裁（Gradient Clipping）：在每次参数更新前对梯度进行剪裁，将梯度值控制在一定范围内，从而防止梯度过大</span><br></pre></td></tr></table></figure>

<h4 id="74-什么是反向传播算法？为何要使用？"><a href="#74-什么是反向传播算法？为何要使用？" class="headerlink" title="74.什么是反向传播算法？为何要使用？"></a>74.什么是反向传播算法？为何要使用？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">反向传播（Backpropagation algorithm）全称“误差反向传播”，是在深度神经网络中，根据输出层输出值，来反向调整隐层权重的一种方法</span><br><span class="line">为了对隐层的参数使用梯度下降，需要先将误差反向传播至隐层，然后才能应用</span><br></pre></td></tr></table></figure>

<h4 id="75-深度学习中，常见的优化器（梯度下降算法）有哪些？各有什么特点？"><a href="#75-深度学习中，常见的优化器（梯度下降算法）有哪些？各有什么特点？" class="headerlink" title="75.深度学习中，常见的优化器（梯度下降算法）有哪些？各有什么特点？"></a>75.深度学习中，常见的优化器（梯度下降算法）有哪些？各有什么特点？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1.随机梯度下降（SGD）：每次随机使用一个样本来计算梯度和更新权重。优点是计算快，缺点是收敛速度慢</span><br><span class="line">2.小批量梯度下降（Mini-batch Gradient Descent，MBGD）：结合了BGD和SGD的优点，使用一个小批量样本来计算梯度。平衡了计算效率和收敛速度</span><br><span class="line">3.动量法（Momentum）：在SGD的基础上引入了一个动量项α（通常取0.9），会考虑之前参数更新的方向和速度，解决了SGD的收敛速度慢和陷入局部最低点这两个问题</span><br><span class="line">4.Nesterov加速（NAG）：通过在梯度方向上进行预先的“看向”操作来加速收敛</span><br><span class="line">5.自适应梯度（Adagrad）：根据每个参数的历史梯度平方和来调整学习率</span><br><span class="line">6.RMSprop：采用滑动窗口加权平均值计算二阶动量，解决了Adagrad中学习率有时持续下降的问题</span><br><span class="line">7.Adam（Adaptive Moment Estimation）：结合了Momentum和RMSprop的优点，适应性强，收敛速度快，适合大多数深度学习任务</span><br><span class="line">8.AdamW：是Adam的改进版本，加入了权重衰减（Weight Decay）来正则化模型，防止过拟合。被证明在很多任务中比Adam效果更好</span><br></pre></td></tr></table></figure>

<h4 id="76-CNN中的feature-map、padding、stride分别是什么？"><a href="#76-CNN中的feature-map、padding、stride分别是什么？" class="headerlink" title="76.CNN中的feature map、padding、stride分别是什么？"></a>76.CNN中的feature map、padding、stride分别是什么？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">feature map：特征图，即经过卷积操作后输出的图像数据</span><br><span class="line">padding：填充，用来控制输出结果的大小。keras中，设置为valid时，表示不填充；设置为same时，表示输出与输入尺寸相同</span><br><span class="line">stride：步长，可用来控制输出结果的大小</span><br></pre></td></tr></table></figure>

<h4 id="77-卷积运算输出矩阵大小的计算公式？"><a href="#77-卷积运算输出矩阵大小的计算公式？" class="headerlink" title="77.卷积运算输出矩阵大小的计算公式？"></a>77.卷积运算输出矩阵大小的计算公式？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">OH &#x3D; (H + 2P - FH) &#x2F; S + 1</span><br><span class="line">OW &#x3D; (W + 2P - FW) &#x2F; S + 1</span><br><span class="line"></span><br><span class="line">解释:</span><br><span class="line">H&#x2F;W：输入图像的高宽</span><br><span class="line">P: padding</span><br><span class="line">FH&#x2F;FW: 卷积核高宽</span><br><span class="line">S: stride</span><br></pre></td></tr></table></figure>

<h4 id="78-CNN网络中的卷积层、激活层、池化层各有什么作用？"><a href="#78-CNN网络中的卷积层、激活层、池化层各有什么作用？" class="headerlink" title="78.CNN网络中的卷积层、激活层、池化层各有什么作用？"></a>78.CNN网络中的卷积层、激活层、池化层各有什么作用？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">卷积层（Convolutional Layer）：降维和提取特征</span><br><span class="line">激活层（Activation Layer）：引入非线性因素，增加模型表达能力</span><br><span class="line">池化层（Pooling Layer）：降采样，减少计算量，防止过拟合</span><br></pre></td></tr></table></figure>

<h4 id="79-什么是最大池化、平均池化？"><a href="#79-什么是最大池化、平均池化？" class="headerlink" title="79.什么是最大池化、平均池化？"></a>79.什么是最大池化、平均池化？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">最大池化（Max Pooling）：选取图像区域的最大值作为池化后的值，有助于提取关键信息</span><br><span class="line">平均池化（Average Pooling）：计算图像区域的平均值作为池化后的值，平滑了特征，有助于去除噪声</span><br></pre></td></tr></table></figure>

<h4 id="80-池化层有什么特点？"><a href="#80-池化层有什么特点？" class="headerlink" title="80.池化层有什么特点？"></a>80.池化层有什么特点？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1）没有要学习的参数</span><br><span class="line">2）通道数不发生变化</span><br><span class="line">3）对微小的变化具有鲁棒性</span><br></pre></td></tr></table></figure>

<h4 id="81-深度卷积网络中的降采样，有哪些方式？"><a href="#81-深度卷积网络中的降采样，有哪些方式？" class="headerlink" title="81.深度卷积网络中的降采样，有哪些方式？"></a>81.深度卷积网络中的降采样，有哪些方式？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1.stride大于1的pooling</span><br><span class="line">2.stride大于1的conv</span><br><span class="line">3.stride大于1的reorg，主要用于YOLO系列目标检测模型中</span><br></pre></td></tr></table></figure>

<h4 id="82-什么是dropout？为什么dropout能避免过拟合？"><a href="#82-什么是dropout？为什么dropout能避免过拟合？" class="headerlink" title="82.什么是dropout？为什么dropout能避免过拟合？"></a>82.什么是dropout？为什么dropout能避免过拟合？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">在神经网络训练过程中，根据设置的比例，随机忽略一部分神经元，可以有效防止过拟合，还能提升模型精度</span><br><span class="line"></span><br><span class="line">能避免过拟合的原因：</span><br><span class="line">1.降低神经元的相互依赖性</span><br><span class="line">2.降低模型复杂度</span><br><span class="line">3.多模型的集成学习，提升了泛化能力</span><br></pre></td></tr></table></figure>

<h4 id="83-什么是批量归一化（Batch-Normalization），其优点是什么？"><a href="#83-什么是批量归一化（Batch-Normalization），其优点是什么？" class="headerlink" title="83.什么是批量归一化（Batch Normalization），其优点是什么？"></a>83.什么是批量归一化（Batch Normalization），其优点是什么？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">以进行学习时的mini-batch为单位，按mini-batch进行归一化。具体而言，就是使mini-batch的数据分布的均值为0、标准差为1</span><br><span class="line"></span><br><span class="line">优点：</span><br><span class="line">1.加速收敛</span><br><span class="line">2.不那么依赖初始值</span><br><span class="line">3.防止过拟合</span><br></pre></td></tr></table></figure>

<h4 id="84-什么是分词？分词的作用是什么？"><a href="#84-什么是分词？分词的作用是什么？" class="headerlink" title="84.什么是分词？分词的作用是什么？"></a>84.什么是分词？分词的作用是什么？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">分词（Word Segmentation）指的是将一段连续的文本按照一定的标准切分成一个一个的词汇。</span><br><span class="line">作用：</span><br><span class="line">1.语义理解</span><br><span class="line">2.信息检索</span><br></pre></td></tr></table></figure>

<h4 id="85-中文分词有哪些方法？"><a href="#85-中文分词有哪些方法？" class="headerlink" title="85.中文分词有哪些方法？"></a>85.中文分词有哪些方法？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">一、基于规则的方法</span><br><span class="line">1.正向最大匹配法（MM，Maximum Matching）</span><br><span class="line">2.逆向最大匹配法（RMM，Reverse Maximum Matching）</span><br><span class="line">3.双向最大匹配法（BiMM，Bidirectional Maximum Matching）</span><br><span class="line"></span><br><span class="line">二、基于统计的方法</span><br><span class="line">1.N-gram模型：计算相邻n个字同时出现的概率，根据概率大小判断词语边界</span><br><span class="line">2.隐马尔可夫模型（HMM）：将词语序列视为一个马尔可夫链，根据模型参数判断词语边界</span><br></pre></td></tr></table></figure>

<h4 id="86-什么是词性标记？"><a href="#86-什么是词性标记？" class="headerlink" title="86.什么是词性标记？"></a>86.什么是词性标记？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">词性标记（Part-Of-Speech tagging, POS tagging）是将单词分配到各自对应词性的任务。</span><br><span class="line">它可以是名词、动词、形容词、副词、介词等</span><br></pre></td></tr></table></figure>

<h4 id="87-什么是词干提取？"><a href="#87-什么是词干提取？" class="headerlink" title="87.什么是词干提取？"></a>87.什么是词干提取？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">词干提取（stemming）是抽取词的词干或词根形式，</span><br><span class="line">样本中单词的单复数或时态对于语义分析并无太大影响，所以需要对单词进行</span><br><span class="line">词干提取，比如：cats -&gt; cat , catching -&gt; catch</span><br></pre></td></tr></table></figure>

<h4 id="88-什么是词袋模型？它的缺点是什么？"><a href="#88-什么是词袋模型？它的缺点是什么？" class="headerlink" title="88.什么是词袋模型？它的缺点是什么？"></a>88.什么是词袋模型？它的缺点是什么？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">词袋模型（Bag-of-words model，BoW）是一种文本表示方法，它将文本转换为词的集合（袋子），忽略词的顺序和语法关系，只关心词的出现频率</span><br><span class="line"></span><br><span class="line">缺点：</span><br><span class="line">1.忽略了词语顺序和上下文</span><br><span class="line">2.高维稀疏矩阵</span><br></pre></td></tr></table></figure>

<h4 id="89-什么是TF-IDF？"><a href="#89-什么是TF-IDF？" class="headerlink" title="89.什么是TF-IDF？"></a>89.什么是TF-IDF？</h4><p><img src="../images/ml_review_tfidf.png" alt="TF-IDF"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">TF-IDF（Term Frequency-Inverse Document Frequency，词频-逆文档频率）是一种文本分析和信息检索技术，常用于评估一个文档集中一个词对某份文档的重要程度，核心思想是，如果一个词在一篇文档中频繁出现，而在其他文档中很少出现，那么这个词对这篇文档具有较高的重要性</span><br><span class="line">由两部分组成：</span><br><span class="line">词频（Term Frequency，TF）&#x3D; 词语i在文档j中出现的次数 &#x2F; 文档j的总词数</span><br><span class="line">逆文档频率（Inverse Document Frequency，IDF）&#x3D; log(语料库的文档总数 &#x2F; (包含词语i的文档数 + 1))</span><br><span class="line">解释：由公式可知，词频越大，语义贡献度越大；出现的文档数越多（说明该词越通用），逆文档频率越接近0，语义贡献度越低</span><br></pre></td></tr></table></figure>

<h4 id="90-常用的文本表示方法有哪些？各自特点是什么？"><a href="#90-常用的文本表示方法有哪些？各自特点是什么？" class="headerlink" title="90.常用的文本表示方法有哪些？各自特点是什么？"></a>90.常用的文本表示方法有哪些？各自特点是什么？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">一、离散表示方法</span><br><span class="line">1.独热编码（One-Hot）</span><br><span class="line">定义：将每个词表示为一个向量，向量长度等于词典大小，当前词的位置为1，其余位置为0</span><br><span class="line">缺点：忽略了上下文关系、高维稀疏矩阵</span><br><span class="line"></span><br><span class="line">2.词袋模型（Bag of Words，BOW）</span><br><span class="line">定义：将文本转换为词的集合（袋子），通过统计词频来表示文本</span><br><span class="line">缺点：忽略了上下文关系、高维稀疏矩阵</span><br><span class="line"></span><br><span class="line">3.TF-IDF（词频-逆文档频率）</span><br><span class="line">定义：基于词袋模型，加入了逆文档频率来衡量词的重要性</span><br><span class="line">缺点：忽略了上下文关系、高维稀疏矩阵</span><br><span class="line"></span><br><span class="line">二、分布式表示方法</span><br><span class="line">1.N-gram</span><br><span class="line">定义：将连续出现的n个词语作为一个词组，并将其表示为向量</span><br><span class="line"></span><br><span class="line">2.共现矩阵</span><br><span class="line">定义：以词语周边的共现词的次数做为当前词语的向量</span><br><span class="line">缺点：高维稀疏矩阵</span><br><span class="line"></span><br><span class="line">3.词嵌入（Word Embeddings）</span><br><span class="line">定义：词嵌入模型，如Word2vec、GloVe和FastText，将词语映射到一个低维的连续向量空间中，能够捕捉到词语之间的语义和上下文关系</span><br><span class="line">缺点：计算资源要求高</span><br></pre></td></tr></table></figure>

<h4 id="91-什么是语料库？它的特点、作用是什么？"><a href="#91-什么是语料库？它的特点、作用是什么？" class="headerlink" title="91.什么是语料库？它的特点、作用是什么？"></a>91.什么是语料库？它的特点、作用是什么？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">语料库（corpus）是指存放语言材料的仓库。现代的语料库是指存放在计算机里的原始语料文本、或经过加工后带有语言学信息标注的语料文本</span><br><span class="line"></span><br><span class="line">特点：</span><br><span class="line">• 语料库中存放的是实际中真实出现过的语言材料</span><br><span class="line">• 语料库是以计算机为载体承载语言知识的基础资源</span><br><span class="line">• 真实语料需要经过分析、处理和加工，才能成为有用的资源</span><br><span class="line"></span><br><span class="line">作用：</span><br><span class="line">• 支持语言学研究和教学</span><br><span class="line">• 支持NLP系统的开发</span><br></pre></td></tr></table></figure>

<h4 id="92-什么是Word2vec？代表模型有哪些？"><a href="#92-什么是Word2vec？代表模型有哪些？" class="headerlink" title="92.什么是Word2vec？代表模型有哪些？"></a>92.什么是Word2vec？代表模型有哪些？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Word2vec是一种词嵌入技术，通过将单词转换为向量形式，使得词与词之间可以定量地度量它们之间的关系</span><br><span class="line"></span><br><span class="line">代表模型：</span><br><span class="line">CBOW（连续词袋模型）：从上下文单词预测目标词</span><br><span class="line">Skip-gram（跳字模型）：通过目标词来预测上下文单词</span><br></pre></td></tr></table></figure>

<h4 id="93-Word2vec中的负采样是什么？"><a href="#93-Word2vec中的负采样是什么？" class="headerlink" title="93.Word2vec中的负采样是什么？"></a>93.Word2vec中的负采样是什么？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">负采样（Negative Sampling）是Word2vec中的一种优化技术，用于加速模型的训练过程。其主要目标是简化Softmax函数的计算，将多分类转变为二分类，从而提升训练效率</span><br></pre></td></tr></table></figure>

<h4 id="94-常用的色彩空间有哪些？"><a href="#94-常用的色彩空间有哪些？" class="headerlink" title="94.常用的色彩空间有哪些？"></a>94.常用的色彩空间有哪些？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1.RGB</span><br><span class="line">2.HSV: 色相(hue)、饱和度(saturation)、亮度(value)</span><br><span class="line">3.YUV: 亮度Y、色差信号UV</span><br></pre></td></tr></table></figure>

<h4 id="95-什么是图像灰度化处理？具体步骤是怎样的？"><a href="#95-什么是图像灰度化处理？具体步骤是怎样的？" class="headerlink" title="95.什么是图像灰度化处理？具体步骤是怎样的？"></a>95.什么是图像灰度化处理？具体步骤是怎样的？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">图像灰度化处理是将彩色图像转换为灰度图像的过程。彩色图像通常使用RGB模型，每个像素由这三种颜色的不同强度组合而成。而灰度图像中，每个像素只包含一个表示亮度的值(0 ~ 255）</span><br><span class="line"></span><br><span class="line">步骤：</span><br><span class="line">1.提取RGB值：从彩色图像中提取每个像素的红、绿、蓝三个通道的值</span><br><span class="line">2.计算灰度值：根据某种方法计算每个像素的灰度值。常用的方法有以下几种：</span><br><span class="line">    平均法：灰度值 &#x3D; (R + G + B) &#x2F; 3</span><br><span class="line">    加权平均法：灰度值 &#x3D; 0.3 * R + 0.59 * G + 0.11 * B。这种方法更符合人眼对颜色敏感度的实际情况，人眼对绿色更敏感，因此绿通道的权重更大。</span><br><span class="line">    最大值法：灰度值 &#x3D; max(R, G, B)</span><br><span class="line">    最小值法：灰度值 &#x3D; min(R, G, B)</span><br><span class="line">3.生成灰度图像：将计算得到的灰度值赋给图像中的每个像素，从而得到灰度图像</span><br><span class="line"></span><br><span class="line">cv2代码实现：</span><br><span class="line"># 将BGR格式转为灰度图像</span><br><span class="line">img_gray &#x3D; cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)</span><br></pre></td></tr></table></figure>

<h4 id="96-什么是图像二值化-反二值化处理，有什么优点？"><a href="#96-什么是图像二值化-反二值化处理，有什么优点？" class="headerlink" title="96.什么是图像二值化/反二值化处理，有什么优点？"></a>96.什么是图像二值化/反二值化处理，有什么优点？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">图像二值化处理是将灰度图像或彩色图像转换为只有黑白两种颜色（即二值）的图像的过程</span><br><span class="line">通过设定一个阈值：</span><br><span class="line">二值化：像素值大于阈值的，设为255（白），小于阈值的，设为0（黑）</span><br><span class="line">反二值化：像素值大于阈值的，设为0（黑），小于阈值的，设为255（白）</span><br><span class="line"></span><br><span class="line">优点：简化数据，突出特征，便于后续处理</span><br><span class="line"></span><br><span class="line">cv2代码实现：</span><br><span class="line"></span><br><span class="line">img &#x3D; cv2.imread(&quot;..&#x2F;dl_data&#x2F;lena.jpg&quot;)</span><br><span class="line">img_gray &#x3D; cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)</span><br><span class="line">cv2.imshow(&quot;img_gray&quot;, img_gray)</span><br><span class="line"></span><br><span class="line"># 二值化</span><br><span class="line">t, binary &#x3D; cv2.threshold(img_gray, 100, 255, cv2.THRESH_BINARY)  # 100为阈值，大于为255，小于为0</span><br><span class="line">cv2.imshow(&quot;binary&quot;, binary)</span><br><span class="line"></span><br><span class="line"># 反二值化</span><br><span class="line">t2, binary_inv &#x3D; cv2.threshold(img_gray, 100, 255, cv2.THRESH_BINARY_INV)  # 100为阈值，大于为0，小于为255</span><br><span class="line">cv2.imshow(&quot;binary_inv&quot;, binary_inv)</span><br></pre></td></tr></table></figure>

<h4 id="97-什么是直方图均衡化？"><a href="#97-什么是直方图均衡化？" class="headerlink" title="97.什么是直方图均衡化？"></a>97.什么是直方图均衡化？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">直方图均衡化是一种图像处理技术，用于改善图像的对比度。通过调整图像像素的灰度值，使得输出图像的灰度直方图尽可能均匀分布，从而增强图像的细节和对比度</span><br><span class="line"></span><br><span class="line">cv2代码实现：</span><br><span class="line">img &#x3D; cv2.imread(&#39;..&#x2F;dl_data&#x2F;sunrise.jpg&#39;, 0)  # 0表示灰度图像</span><br><span class="line">cv2.imshow(&#39;img&#39;, img)</span><br><span class="line"></span><br><span class="line"># 直方图均衡化</span><br><span class="line">img_eq &#x3D; cv2.equalizeHist(img)</span><br><span class="line">cv2.imshow(&#39;img_eq&#39;, img_eq)</span><br></pre></td></tr></table></figure>

<h4 id="98-图像加法运算有什么应用？"><a href="#98-图像加法运算有什么应用？" class="headerlink" title="98.图像加法运算有什么应用？"></a>98.图像加法运算有什么应用？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">图像合成：例如，图像水印可以通过加法叠加到原始图像上</span><br><span class="line">图像去噪：通过对同一场景的多幅图像进行加法运算，可以得到平均值图像，从而减少噪声</span><br><span class="line"></span><br><span class="line">cv2代码实现：</span><br><span class="line"># 图像直接相加</span><br><span class="line">add_img &#x3D; cv.add(lena, lily)  # 图像直接相加，会导致图像过亮、过白</span><br><span class="line">cv.imshow(&quot;add_img&quot;, add_img)</span><br><span class="line"></span><br><span class="line"># 图像权重相加</span><br><span class="line">add_weighted_img &#x3D; cv.addWeighted(lena, 0.8, lily, 0.2, 0)  # 亮度调节量为50</span><br><span class="line">cv.imshow(&quot;add_weighted_img&quot;, add_weighted_img)</span><br></pre></td></tr></table></figure>

<h4 id="99-图像减法运算有什么应用？"><a href="#99-图像减法运算有什么应用？" class="headerlink" title="99.图像减法运算有什么应用？"></a>99.图像减法运算有什么应用？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">运动检测：通过对连续帧进行减法运算，可以检测出前后帧之间的差异，从而识别出运动物体</span><br><span class="line">背景去除：在固定背景的场景中，可以通过减去背景图像来提取前景物体</span><br><span class="line"></span><br><span class="line">cv2代码实现：</span><br><span class="line"># 相减</span><br><span class="line">subtract_img &#x3D; cv.subtract(src1, src2)</span><br></pre></td></tr></table></figure>

<h4 id="100-图像放大时，可以采用哪些插值法？"><a href="#100-图像放大时，可以采用哪些插值法？" class="headerlink" title="100.图像放大时，可以采用哪些插值法？"></a>100.图像放大时，可以采用哪些插值法？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">1.最近邻插值（Nearest Neighbor Interpolation）</span><br><span class="line">原理：选择最近的一个像素的值作为插值点的值</span><br><span class="line">优点：计算简单，速度快。</span><br><span class="line">缺点：放大后的图像会有明显的锯齿和块状效果，不平滑</span><br><span class="line"></span><br><span class="line">2.双线性插值（Bilinear Interpolation）</span><br><span class="line">原理：根据插值点周围的四个像素值，进行线性插值计算，得到插值点的值。</span><br><span class="line">优点：计算复杂度适中，插值结果较平滑。</span><br><span class="line">缺点：图像细节和边缘会有一定程度的模糊。</span><br><span class="line"></span><br><span class="line">3.双三次插值（Bicubic Interpolation）</span><br><span class="line">原理：利用插值点周围的16个像素（4x4邻域），进行三次多项式插值计算。</span><br><span class="line">优点：能较好地保留图像细节和边缘信息，插值结果平滑。</span><br><span class="line">缺点：计算复杂度较高，速度比双线性插值慢。</span><br><span class="line"></span><br><span class="line">cv2代码实现：</span><br><span class="line"># 放大</span><br><span class="line">dst_size2 &#x3D; (int(w * 2), int(h * 2))</span><br><span class="line">big_nearest &#x3D; cv.resize(img, dst_size2, interpolation&#x3D;cv.INTER_NEAREST)  # 最近邻插值</span><br><span class="line">big_linear &#x3D; cv.resize(img, dst_size2, interpolation&#x3D;cv.INTER_LINEAR)  # 双线性插值</span><br></pre></td></tr></table></figure>

<h4 id="101-对图像进行模糊平滑处理，可以使用哪些方式？"><a href="#101-对图像进行模糊平滑处理，可以使用哪些方式？" class="headerlink" title="101.对图像进行模糊平滑处理，可以使用哪些方式？"></a>101.对图像进行模糊平滑处理，可以使用哪些方式？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">1.均值模糊 ：通过用邻域内所有像素的平均值替换中心像素来进行平滑</span><br><span class="line">2.高斯模糊 ：会对邻域像素赋予不同的权重，距离中心像素越近的像素权重越大</span><br><span class="line">3.中值模糊 ：使用邻域内像素的中值来替换中心像素，特别适用于去除椒盐噪声</span><br><span class="line"></span><br><span class="line">cv2代码实现：</span><br><span class="line"># 均值模糊</span><br><span class="line">img_mean_blur &#x3D; cv.blur(img, (5, 5))</span><br><span class="line">cv.imshow(&quot;img_mean_blur&quot;, img_mean_blur)</span><br><span class="line"></span><br><span class="line"># 高斯模糊</span><br><span class="line"># 第三个参数为高斯核在X方向的标准差</span><br><span class="line">img_gaussian_blur &#x3D; cv.GaussianBlur(img, (5, 5), 3)</span><br><span class="line">cv.imshow(&quot;img_gaussian_blur&quot;, img_gaussian_blur)</span><br><span class="line"></span><br><span class="line"># 中值模糊</span><br><span class="line">img_median_blur &#x3D; cv.medianBlur(img, 5)</span><br><span class="line">cv.imshow(&quot;img_median_blur&quot;, img_median_blur)</span><br></pre></td></tr></table></figure>

<h4 id="102-对图像进行边沿检测，有哪些常用算子？"><a href="#102-对图像进行边沿检测，有哪些常用算子？" class="headerlink" title="102.对图像进行边沿检测，有哪些常用算子？"></a>102.对图像进行边沿检测，有哪些常用算子？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">1.Sobel算子：基于一阶导数，简单高效</span><br><span class="line"># cv.CV_64F: 输出图像深度，本来应该设置为-1，但如果设成-1，可能会发生计算错误，所以通常先设置为精度更高的CV_64F</span><br><span class="line"># dx, dy: x和y方向的导数</span><br><span class="line">img_sobel &#x3D; cv.Sobel(img, cv.CV_64F, 1, 1, ksize&#x3D;5)</span><br><span class="line">cv.imshow(&quot;img_sobel&quot;, img_sobel)</span><br><span class="line"></span><br><span class="line">2.laplacian算子：基于二阶导数，无方向性、噪声敏感</span><br><span class="line">img_laplacian &#x3D; cv.Laplacian(img, cv.CV_64F)</span><br><span class="line">cv.imshow(&quot;img_laplacian&quot;, img_laplacian)</span><br><span class="line"></span><br><span class="line">3.Canny算子：综合了多种边缘检测技术的改进型算子，抗噪声、效果好</span><br><span class="line">img_canny &#x3D; cv.Canny(img, 50, 150)  # 50和150是阈值</span><br><span class="line">cv.imshow(&quot;img_canny&quot;, img_canny)</span><br></pre></td></tr></table></figure>

<h4 id="103-图像的腐蚀有哪些实际的应用场景？"><a href="#103-图像的腐蚀有哪些实际的应用场景？" class="headerlink" title="103.图像的腐蚀有哪些实际的应用场景？"></a>103.图像的腐蚀有哪些实际的应用场景？</h4><p><img src="../images/ml_review_erode.png" alt="腐蚀"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">定义：沿着边界向内收缩</span><br><span class="line">应用：去除噪声、分离连在一起的物体、去掉小的凸起或毛刺</span><br><span class="line"></span><br><span class="line">cv2代码实现：</span><br><span class="line"># 腐蚀</span><br><span class="line">kernel &#x3D; np.ones((3, 3), np.uint8)</span><br><span class="line">erosion &#x3D; cv.erode(img, kernel, iterations&#x3D;3)</span><br></pre></td></tr></table></figure>

<h4 id="104-图像的膨胀有哪些实际的应用场景？"><a href="#104-图像的膨胀有哪些实际的应用场景？" class="headerlink" title="104.图像的膨胀有哪些实际的应用场景？"></a>104.图像的膨胀有哪些实际的应用场景？</h4><p><img src="../images/ml_review_dilate.png" alt="膨胀"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">定义：向外扩充</span><br><span class="line">应用：连接断开的物体、填补孔洞</span><br><span class="line"></span><br><span class="line">cv2代码实现：</span><br><span class="line"># 膨胀</span><br><span class="line">kernel &#x3D; np.ones((3, 3), np.uint8)</span><br><span class="line">dilate &#x3D; cv.dilate(img, kernel, iterations&#x3D;3)</span><br></pre></td></tr></table></figure>

<h4 id="105-图像的开运算有哪些实际的应用场景？"><a href="#105-图像的开运算有哪些实际的应用场景？" class="headerlink" title="105.图像的开运算有哪些实际的应用场景？"></a>105.图像的开运算有哪些实际的应用场景？</h4><p><img src="../images/ml_review_open.png" alt="开运算"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">定义：先腐蚀，后膨胀</span><br><span class="line">应用：去除噪声、分离物体</span><br><span class="line"></span><br><span class="line">cv2代码实现：</span><br><span class="line"># 开运算</span><br><span class="line">kernel &#x3D; np.ones((3, 3), np.uint8)</span><br><span class="line">res &#x3D; cv.morphologyEx(img, cv.MORPH_OPEN, kernel, iterations&#x3D;3)</span><br></pre></td></tr></table></figure>

<h4 id="106-图像的闭运算有哪些实际的应用场景？"><a href="#106-图像的闭运算有哪些实际的应用场景？" class="headerlink" title="106.图像的闭运算有哪些实际的应用场景？"></a>106.图像的闭运算有哪些实际的应用场景？</h4><p><img src="../images/ml_review_close.png" alt="闭运算"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">定义：先膨胀，后腐蚀</span><br><span class="line">应用：填补孔洞、连接物体</span><br><span class="line"></span><br><span class="line">cv2代码实现：</span><br><span class="line"># 闭运算</span><br><span class="line">kernel &#x3D; np.ones((1, 4), np.uint8)</span><br><span class="line">res &#x3D; cv.morphologyEx(img, cv.MORPH_CLOSE, kernel, iterations&#x3D;10)</span><br></pre></td></tr></table></figure>

<h4 id="107-图像的形态学梯度运算是什么？"><a href="#107-图像的形态学梯度运算是什么？" class="headerlink" title="107.图像的形态学梯度运算是什么？"></a>107.图像的形态学梯度运算是什么？</h4><p><img src="../images/ml_review_morph_gradient.png" alt="形态学梯度运算"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">定义：膨胀图像减去腐蚀图像</span><br><span class="line">应用：获取边缘信息</span><br><span class="line"></span><br><span class="line">cv2代码实现：</span><br><span class="line"># 形态学梯度</span><br><span class="line">kernel &#x3D; np.ones((2, 2), np.uint8)</span><br><span class="line">res &#x3D; cv.morphologyEx(img, cv.MORPH_GRADIENT, kernel)</span><br></pre></td></tr></table></figure>

<h4 id="108-什么是图像的礼帽运算？"><a href="#108-什么是图像的礼帽运算？" class="headerlink" title="108.什么是图像的礼帽运算？"></a>108.什么是图像的礼帽运算？</h4><p><img src="../images/ml_review_hat.png" alt="礼帽运算"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">主要用于突出图像中的亮细节或暗细节</span><br><span class="line">白礼帽运算 &#x3D; 原始图像 - 开运算图像</span><br><span class="line">黑礼帽运算 &#x3D; 闭运算图像 - 原始图像</span><br></pre></td></tr></table></figure>

<h4 id="109-在深度学习模型训练中，收敛速度和训练速度分别是什么？"><a href="#109-在深度学习模型训练中，收敛速度和训练速度分别是什么？" class="headerlink" title="109.在深度学习模型训练中，收敛速度和训练速度分别是什么？"></a>109.在深度学习模型训练中，收敛速度和训练速度分别是什么？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">收敛速度：指模型训练达到收敛状态所需的时间。收敛状态是指模型的损失函数不再发生显著变化，或者达到预先设定的终止条件</span><br><span class="line">训练速度：指模型训练过程中完成一次迭代所需的时间</span><br></pre></td></tr></table></figure>

<h4 id="110-在模型训练过程中，为何增加batch-size可能会导致模型的收敛速度变快？"><a href="#110-在模型训练过程中，为何增加batch-size可能会导致模型的收敛速度变快？" class="headerlink" title="110.在模型训练过程中，为何增加batch size可能会导致模型的收敛速度变快？"></a>110.在模型训练过程中，为何增加batch size可能会导致模型的收敛速度变快？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">batch size 越大，梯度噪声越小，梯度下降的方向更加准确，从而加快收敛速度</span><br></pre></td></tr></table></figure>

<h4 id="111-在SVM、逻辑回归等二分类模型中，在模型训练完成后，如果调高分类阈值，模型的precision、recall、auc值会如何变化？"><a href="#111-在SVM、逻辑回归等二分类模型中，在模型训练完成后，如果调高分类阈值，模型的precision、recall、auc值会如何变化？" class="headerlink" title="111.在SVM、逻辑回归等二分类模型中，在模型训练完成后，如果调高分类阈值，模型的precision、recall、auc值会如何变化？"></a>111.在SVM、逻辑回归等二分类模型中，在模型训练完成后，如果调高分类阈值，模型的precision、recall、auc值会如何变化？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">precision: 提高</span><br><span class="line">原因：precision &#x3D; TP &#x2F; (TP+FP)，当调高阈值时，模型会更倾向于将样本预测为负类，TP和FP都会减少，而FP（错误预测为正类）会减少的更大</span><br><span class="line"></span><br><span class="line">recall: 降低</span><br><span class="line">原因：recall &#x3D; TP &#x2F; (TP+FN)，当调高阈值时，模型会更倾向于将样本预测为负类，即FN会增加，TP减少</span><br><span class="line"></span><br><span class="line">auc: 不变</span><br><span class="line">原因：AUC是ROC曲线下面积，反映了模型在所有可能的分类阈值下的整体性能。本质上是不依赖于单一阈值的，而是反映了模型在不同阈值下的表现。因此，调高分类阈值不会改变AUC值。</span><br></pre></td></tr></table></figure>

<h4 id="112-LSTM对比原始RNN的最大改进是什么？bi-LSTM对比LSTM最大的改进是什么？"><a href="#112-LSTM对比原始RNN的最大改进是什么？bi-LSTM对比LSTM最大的改进是什么？" class="headerlink" title="112.LSTM对比原始RNN的最大改进是什么？bi-LSTM对比LSTM最大的改进是什么？"></a>112.LSTM对比原始RNN的最大改进是什么？bi-LSTM对比LSTM最大的改进是什么？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">LSTM解决了梯度消失，能处理更长的序列</span><br><span class="line">bi-LSTM能够提取反向序列特征</span><br></pre></td></tr></table></figure>

<h4 id="113-经过下列卷积操作后，3x3-conv-gt-3x3-conv-gt-2x2-max-pool-gt-3x3conv，卷积步长为1，没有填充，输出神经元感受野是多大？"><a href="#113-经过下列卷积操作后，3x3-conv-gt-3x3-conv-gt-2x2-max-pool-gt-3x3conv，卷积步长为1，没有填充，输出神经元感受野是多大？" class="headerlink" title="113.经过下列卷积操作后，3x3 conv -&gt; 3x3 conv -&gt; 2x2 max pool -&gt; 3x3conv，卷积步长为1，没有填充，输出神经元感受野是多大？"></a>113.经过下列卷积操作后，3x3 conv -&gt; 3x3 conv -&gt; 2x2 max pool -&gt; 3x3conv，卷积步长为1，没有填充，输出神经元感受野是多大？</h4><p><img src="../images/ml_review_rf.png" alt="感受野计算"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">公式解释：RFn表示第n层感受野大小，fn表示第n层滤波器大小，si表示到n-1层的步长的连乘</span><br><span class="line"></span><br><span class="line">con1:  fn &#x3D; 3, s &#x3D; 1</span><br><span class="line">con2:  fn &#x3D; 3, s &#x3D; 1</span><br><span class="line">pool3: fn &#x3D; 2, s &#x3D; 2</span><br><span class="line">con3:  fn &#x3D; 3, s &#x3D; 1</span><br><span class="line"></span><br><span class="line">RF1 &#x3D; 3</span><br><span class="line">RF2 &#x3D; 3 + (3 - 1) * 1 &#x3D; 5</span><br><span class="line">RF3 &#x3D; 5 + (2 - 1) * 1 * 1 &#x3D; 6</span><br><span class="line">RF4 &#x3D; 6 + (3 - 1) * 1 * 1 * 2 &#x3D; 10</span><br></pre></td></tr></table></figure>

<h4 id="114-简述离散化的好处，并写出离散化的常用方法有哪些？"><a href="#114-简述离散化的好处，并写出离散化的常用方法有哪些？" class="headerlink" title="114.简述离散化的好处，并写出离散化的常用方法有哪些？"></a>114.简述离散化的好处，并写出离散化的常用方法有哪些？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">好处：降维、降低噪声的影响</span><br><span class="line"></span><br><span class="line">方法：</span><br><span class="line">1.等宽区间离散化：将连续型数据划分成等宽的区间，每个区间对应一个离散值。例如，可以将用户年龄划分为“0-10岁”、“10-20岁”、“20-30岁”等区间</span><br><span class="line">2.等频离散化：将数据按照等频的原则进行划分。每个区间包含相同数量的数据点</span><br><span class="line">3.基于聚类的离散化：使用聚类算法（如K-means）将数据分成若干簇，每个簇作为一个离散区间</span><br></pre></td></tr></table></figure>

<h4 id="115-常见函数的求导公式"><a href="#115-常见函数的求导公式" class="headerlink" title="115.常见函数的求导公式"></a>115.常见函数的求导公式</h4><p><img src="../images/ml_review_dot_func.png" alt="求导公式"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y &#x3D; n^x，当x&lt;0时，y&#39; &#x3D; -n^xln(n)</span><br></pre></td></tr></table></figure>

<h4 id="116-利用梯度下降法优化目标函数ln-wx-，给定第一轮参数w-2-x-10，请写出第三轮优化后的参数值（学习率-0-1）"><a href="#116-利用梯度下降法优化目标函数ln-wx-，给定第一轮参数w-2-x-10，请写出第三轮优化后的参数值（学习率-0-1）" class="headerlink" title="116.利用梯度下降法优化目标函数ln(wx)，给定第一轮参数w=2, x=10，请写出第三轮优化后的参数值（学习率 = 0.1）"></a>116.利用梯度下降法优化目标函数ln(wx)，给定第一轮参数w=2, x=10，请写出第三轮优化后的参数值（学习率 = 0.1）</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">梯度下降参数更新公式：w &#x3D; w - η*(▲fnloss&#x2F;w)</span><br><span class="line">公式解释：η：学习率；▲fnloss&#x2F;w：损失函数关于参数w的导数</span><br><span class="line"></span><br><span class="line">对ln(wx)求导 &#x3D; ln(w) &#x3D; 1 &#x2F; w</span><br><span class="line">第一轮：w &#x3D; 2 - 0.1 * (1 &#x2F; 2) &#x3D; 1.95</span><br><span class="line">第二轮：w &#x3D; 1.95 - 0.1 * (1 &#x2F; 1.95) ≈ 1.8987</span><br><span class="line">第三轮：w &#x3D; 1.8987 - 0.1 * (1 &#x2F; 1.8987) ≈ 1.846</span><br></pre></td></tr></table></figure>

<h4 id="117-已知卷积层中输入尺寸32x32x3，有10个大小为5x5的卷积核，stride-1，pad-2，输出特征图大小为多少？总的特征数量为多少？"><a href="#117-已知卷积层中输入尺寸32x32x3，有10个大小为5x5的卷积核，stride-1，pad-2，输出特征图大小为多少？总的特征数量为多少？" class="headerlink" title="117.已知卷积层中输入尺寸32x32x3，有10个大小为5x5的卷积核，stride=1，pad=2，输出特征图大小为多少？总的特征数量为多少？"></a>117.已知卷积层中输入尺寸32x32x3，有10个大小为5x5的卷积核，stride=1，pad=2，输出特征图大小为多少？总的特征数量为多少？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">特征图尺寸计算公式：OH &#x3D; (H + 2P - FH) &#x2F; S + 1</span><br><span class="line"></span><br><span class="line">OH &#x3D; OW &#x3D; (32 + 2X2 - 5) &#x2F; 1 + 1 &#x3D; 32</span><br><span class="line">总特征数量 &#x3D; 32 x 32 x 10 &#x3D; 10240</span><br></pre></td></tr></table></figure>

<h4 id="118-Faster-RCNN中，ROI-pooling具体如何工作（怎么把不同大小的框，pooling到同样大小）？"><a href="#118-Faster-RCNN中，ROI-pooling具体如何工作（怎么把不同大小的框，pooling到同样大小）？" class="headerlink" title="118.Faster RCNN中，ROI pooling具体如何工作（怎么把不同大小的框，pooling到同样大小）？"></a>118.Faster RCNN中，ROI pooling具体如何工作（怎么把不同大小的框，pooling到同样大小）？</h4><p><img src="../images/fasterrcnn_roi_pooling.png" alt="RoI Pooling"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">这里有3个参数：pooled_w、pooled_h和spatial_scale</span><br><span class="line">- 首先使用spatial_scale参数(16)将每个RoI映射回(M&#x2F;16)x(N&#x2F;16)大小的feature map尺度；</span><br><span class="line">- 再将每个RoI对应的feature map区域水平分为 pool_w*pool_h 的网格；</span><br><span class="line">- 对网格的每一份都进行max pooling处理</span><br></pre></td></tr></table></figure>

<h4 id="119-简述SSD和YOLO的区别？"><a href="#119-简述SSD和YOLO的区别？" class="headerlink" title="119.简述SSD和YOLO的区别？"></a>119.简述SSD和YOLO的区别？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">YOLO通常比SSD速度更快，但准确性略低；SSD通常比YOLO准确性更高，但速度略慢</span><br></pre></td></tr></table></figure>

<h4 id="120-YOLOv2与YOLOv3的区别"><a href="#120-YOLOv2与YOLOv3的区别" class="headerlink" title="120.YOLOv2与YOLOv3的区别"></a>120.YOLOv2与YOLOv3的区别</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">骨干网：YOLOv3采用Darknet53，YOLOv2采用Darknet19</span><br><span class="line">分类器：YOLOv3采用sigmoid，能进行多标签分类，YOLOv2采用softmax，只能进行单标签分类</span><br><span class="line">多尺度融合：YOLOv3采用融合FPN（特征金字塔网络），实现多尺度检测</span><br></pre></td></tr></table></figure>

<h4 id="121-测试集中有1000个样本，600个是A类，400个B类，模型预测结果700个判断为A类，其中正确有500个，300个判断为B类，其中正确有200个。请计算B类的查准率（Precision）和召回率（Recall）"><a href="#121-测试集中有1000个样本，600个是A类，400个B类，模型预测结果700个判断为A类，其中正确有500个，300个判断为B类，其中正确有200个。请计算B类的查准率（Precision）和召回率（Recall）" class="headerlink" title="121.测试集中有1000个样本，600个是A类，400个B类，模型预测结果700个判断为A类，其中正确有500个，300个判断为B类，其中正确有200个。请计算B类的查准率（Precision）和召回率（Recall）"></a>121.测试集中有1000个样本，600个是A类，400个B类，模型预测结果700个判断为A类，其中正确有500个，300个判断为B类，其中正确有200个。请计算B类的查准率（Precision）和召回率（Recall）</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">B为正样本，混淆矩阵如下：</span><br><span class="line"></span><br><span class="line">|           | Real A  | Real B  |</span><br><span class="line">|-----------|---------|---------|</span><br><span class="line">| Predict A | 500(TN) | 200(FN) |</span><br><span class="line">| Predict B | 100(FP) | 200(TP) |</span><br><span class="line"></span><br><span class="line">precision &#x3D; TP &#x2F; (TP + FP) &#x3D; 200 &#x2F; (200 + 100) &#x3D; 2&#x2F;3</span><br><span class="line">recall &#x3D; TP &#x2F; (TP + FN) &#x3D; 200 &#x2F; (200 + 200) &#x3D; 1&#x2F;2 &#x3D; 0.5</span><br></pre></td></tr></table></figure>

<h4 id="122-Sigmoid函数的导数推导过程"><a href="#122-Sigmoid函数的导数推导过程" class="headerlink" title="122.Sigmoid函数的导数推导过程"></a>122.Sigmoid函数的导数推导过程</h4><p><img src="../images/ml_review_sigmoid_dot_process.png" alt="sigmoid推导"></p>
<h4 id="123-深度学习中，实现上采样有哪些方法？"><a href="#123-深度学习中，实现上采样有哪些方法？" class="headerlink" title="123.深度学习中，实现上采样有哪些方法？"></a>123.深度学习中，实现上采样有哪些方法？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1.插值法：最近邻、双线性</span><br><span class="line">2.反卷积</span><br><span class="line">3.反池化</span><br><span class="line">4.转置卷积</span><br></pre></td></tr></table></figure>

<h4 id="124-深度可分离卷积是什么？"><a href="#124-深度可分离卷积是什么？" class="headerlink" title="124.深度可分离卷积是什么？"></a>124.深度可分离卷积是什么？</h4><p><img src="../images/ml_review_original_conv.png" alt="普通卷积"><br><img src="../images/ml_review_depthwise_conv.png" alt="深度卷积"><br><img src="../images/ml_review_pointwise_conv.png" alt="逐点卷积"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">深度可分离卷积（Depthwise Separable Convolution）是一种卷积操作，它将标准卷积分解为两个独立的操作：深度卷积（Depthwise Convolution）和逐点卷积（Pointwise Convolution）。这种分解能够显著减少计算量和参数量，从而提高卷积神经网络的效率，特别适合移动端设备</span><br><span class="line">第一步：使用深度卷积，对每个输入通道分别进行卷积操作，比如对于一张RGB的3通道图像，会使用3个卷积核分别对每个通道进行卷积，得到3个feature map</span><br><span class="line">第二步：逐点卷积，使用n个 1×1x3 卷积核在所有深度卷积生成的特征图上进行卷积操作，得到n个特征图，以实现通道间的信息融合</span><br></pre></td></tr></table></figure>

<h4 id="125-LSTM跟GRU的区别"><a href="#125-LSTM跟GRU的区别" class="headerlink" title="125.LSTM跟GRU的区别"></a>125.LSTM跟GRU的区别</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.结构：LSTM有三个门（forget，input，output），GRU只有两个门（update和reset）</span><br><span class="line">2.计算效率：LSTM参数更多，推理更慢，GRU参数少，推理更快</span><br></pre></td></tr></table></figure>

<h4 id="126-什么是一维傅里叶变换？它在图像分析中有哪些作用？"><a href="#126-什么是一维傅里叶变换？它在图像分析中有哪些作用？" class="headerlink" title="126.什么是一维傅里叶变换？它在图像分析中有哪些作用？"></a>126.什么是一维傅里叶变换？它在图像分析中有哪些作用？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">一维傅里叶变换（1D Fourier Transform）是一种将一维信号从时域转换到频域的数学方法。它可以将一个信号分解为不同频率的正弦波和余弦波的叠加。</span><br><span class="line">在图像处理中，它能分离出图像高频部分和低频部分，从而实现边缘检测、图像去噪</span><br></pre></td></tr></table></figure>

<h4 id="127-常用图像分割算法有哪些？各有什么优缺点？"><a href="#127-常用图像分割算法有哪些？各有什么优缺点？" class="headerlink" title="127.常用图像分割算法有哪些？各有什么优缺点？"></a>127.常用图像分割算法有哪些？各有什么优缺点？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1.FCN：又称全卷积网络，去掉了CNN中的全连接层。对输入图像进行层层卷积、上采样，预测出每个像素所属的类别</span><br><span class="line">2.U-Net：模型简单、容易理解，针对样本较少的情况也能有较好分割效果</span><br><span class="line">3.Mask RCNN：在Faster RCNN 上增加了FCN来产生定位和分割信息</span><br><span class="line">4.DeepLab系列：引入了空洞卷积，条件随机场等策略，使得分割效果更好</span><br></pre></td></tr></table></figure>

<h4 id="128-简单介绍几种常用的人脸检测算法"><a href="#128-简单介绍几种常用的人脸检测算法" class="headerlink" title="128.简单介绍几种常用的人脸检测算法"></a>128.简单介绍几种常用的人脸检测算法</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">基于传统人脸检测算法：Haar级联人脸检测算法</span><br><span class="line">深度学习人脸检测算法：MTCNN、DeepFace、RetinaFace</span><br></pre></td></tr></table></figure>

<h4 id="129-你知道哪些OCR中的文字识别模型和文字检测模型？"><a href="#129-你知道哪些OCR中的文字识别模型和文字检测模型？" class="headerlink" title="129.你知道哪些OCR中的文字识别模型和文字检测模型？"></a>129.你知道哪些OCR中的文字识别模型和文字检测模型？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">文字识别模型: CRNN+CTC</span><br><span class="line">文字检测模型: CTPN、SegLink、DB</span><br></pre></td></tr></table></figure>

<h4 id="130-在图像处理中，滤波、模糊、去噪，这三者是什么关系？"><a href="#130-在图像处理中，滤波、模糊、去噪，这三者是什么关系？" class="headerlink" title="130.在图像处理中，滤波、模糊、去噪，这三者是什么关系？"></a>130.在图像处理中，滤波、模糊、去噪，这三者是什么关系？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">滤波(Filtering)：是一个广泛的概念，指通过某种方式修改图像的像素值以达到特定目的，比如低通滤波可以平滑模糊图像，高通滤波可以锐化增强图像</span><br><span class="line">模糊 (Blurring)：一种滤波的具体应用，使用了低通滤波操作，可以平滑图像，也可用于去噪。常见的模糊方法有均值模糊、中值模糊、高斯模糊</span><br><span class="line">去噪 (Denoising)：指从图像中去除噪声。去噪的方法有很多，滤波是其中一种，基于滤波的常见的去噪方法有均值滤波、中值滤波、高斯滤波</span><br></pre></td></tr></table></figure>

<h4 id="131-什么是HOUGH（霍夫）变换？"><a href="#131-什么是HOUGH（霍夫）变换？" class="headerlink" title="131.什么是HOUGH（霍夫）变换？"></a>131.什么是HOUGH（霍夫）变换？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HOUGH（霍夫）变换是一种图像处理技术，用于检测图像中的几何形状（如直线、圆和椭圆等）。它的主要思想是通过将图像空间（通常是二维空间）中的点转换到参数空间（如极坐标空间），然后在参数空间中寻找累加器的局部最大值，这些最大值对应于图像空间中的几何形状</span><br></pre></td></tr></table></figure>

<h4 id="132-列出机器学习中常见的分类算法，并比较各自的特点"><a href="#132-列出机器学习中常见的分类算法，并比较各自的特点" class="headerlink" title="132.列出机器学习中常见的分类算法，并比较各自的特点"></a>132.列出机器学习中常见的分类算法，并比较各自的特点</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">1.逻辑回归（Logistic Regression）：通过Sigmoid函数将线性回归的结果映射到0和1之间</span><br><span class="line">    优点：模型简单，易于理解和实现</span><br><span class="line">    缺点：只能处理二分类问题</span><br><span class="line">2.支持向量机（SVM）：基本思想是找到一个能够最大程度分离不同类别数据的超平面</span><br><span class="line">    优点：有严格的数学理论支持，可解释性强；采用核函数之后，可以处理非线性问题</span><br><span class="line">    缺点：计算复杂度高，适合中小数据集</span><br><span class="line">3.决策树（Decision Tree）：通过树形结构来表示决策过程，每个节点表示一个属性，每个分支表示一个属性值，叶节点表示一个类或决策结果</span><br><span class="line">    优点：易于理解和解释</span><br><span class="line">    缺点：容易过拟合</span><br><span class="line">4.朴素贝叶斯（Naive Bayes）：一种基于贝叶斯定理的概率分类方法。它假设特征之间是相互独立的，这一假设称为“朴素”假设</span><br><span class="line">    优点：容易实现，计算效率高</span><br><span class="line">    缺点：独立性假设在现实中往往不成立</span><br><span class="line">5.K近邻（K-Nearest Neighbors, KNN）：通过计算样本与训练集中最近的K个邻居的距离进行分类</span><br><span class="line">    优点：容易实现，易于理解</span><br><span class="line">    缺点：计算复杂度高</span><br><span class="line">6.随机森林（Random Forest）：基于决策树的Bagging类集成学习方法</span><br><span class="line">    优点：泛化能力强</span><br><span class="line">    缺点：计算复杂</span><br></pre></td></tr></table></figure>

<h4 id="133-使用opencv将一个图像旋转180°"><a href="#133-使用opencv将一个图像旋转180°" class="headerlink" title="133.使用opencv将一个图像旋转180°"></a>133.使用opencv将一个图像旋转180°</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rotate</span><span class="params">(img, angle, center=None, scale=<span class="number">1.0</span>)</span>:</span></span><br><span class="line">    h, w = img.shape[:<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 旋转中心默认为图像中心</span></span><br><span class="line">    <span class="keyword">if</span> center <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        center = (w / <span class="number">2</span>, h / <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    M = cv.getRotationMatrix2D(center, angle, scale)</span><br><span class="line">    <span class="keyword">return</span> cv.warpAffine(img, M, (w, h))</span><br></pre></td></tr></table></figure>

<h4 id="134-机器学习中的降维有哪些常用方法？"><a href="#134-机器学习中的降维有哪些常用方法？" class="headerlink" title="134.机器学习中的降维有哪些常用方法？"></a>134.机器学习中的降维有哪些常用方法？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.主成分分析（PCA）：通过找到数据中最大方差的方向（主成分），将数据投影到这些方向上，从而实现降维</span><br><span class="line">2.线性判别分析（LDA) ：主要用于分类问题。与PCA类似，但LDA不仅考虑数据的方差，还考虑类间差异，目的是最大化类间差异和最小化类内差异。</span><br></pre></td></tr></table></figure>

<h4 id="135-vgg16的网络结构？"><a href="#135-vgg16的网络结构？" class="headerlink" title="135.vgg16的网络结构？"></a>135.vgg16的网络结构？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">网络结构：16层，包含13个卷积层（卷积+最大池化）和3个全连接层，输出1000个分类</span><br><span class="line">1.卷积层：卷积核统一为3x3，stride&#x3D;1， padding&#x3D;1</span><br><span class="line">2.池化层：统一为2x2，stride&#x3D;2</span><br><span class="line">3.全连接层：2个FC-4096，1个FC-1000</span><br><span class="line">4.使用ReLU激活函数</span><br></pre></td></tr></table></figure>

<h4 id="136-ResNet的网络结构？"><a href="#136-ResNet的网络结构？" class="headerlink" title="136.ResNet的网络结构？"></a>136.ResNet的网络结构？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ResNet旨在解决深层网络训练中的梯度消失问题。其核心思想是通过引入残差连接（shortcut connection）来简化网络的训练过程，使得可以训练更深的网络结构</span><br><span class="line">1.残差块(Residual Block)：通过shortcut connection将输入直接加到输出上，从而避免了梯度消失的问题</span><br><span class="line">2.全局平均池化层：以降低参数量并提取全局特征</span><br><span class="line">3.全连接层：搭配softmax给出1000个类别</span><br></pre></td></tr></table></figure>

<h4 id="137-什么是学习率的余弦退火？"><a href="#137-什么是学习率的余弦退火？" class="headerlink" title="137.什么是学习率的余弦退火？"></a>137.什么是学习率的余弦退火？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">定义：余弦退火（Cosine Annealing）是一种在深度学习训练过程中动态调整学习率的方法。其核心思想是通过余弦函数来降低学习率，从而帮助模型更好地收敛并避免过拟合</span><br><span class="line">公式：η_t &#x3D; η_min + (η_max - η_min) * 1&#x2F;2 * (1 + cos(t * π &#x2F; T))</span><br><span class="line">公式解释：η_t是当前学习率，η_min和η_max分别是最小和最大学习率，t是当前训练步数，T是总训练步数</span><br><span class="line"></span><br><span class="line">在训练初期会设置一个较高的学习率，随着训练的进行，学习率按照余弦函数的变化规律逐渐减小，直到达到一个预设的最小值。这种方法可以有效地平衡训练过程中的速度和稳定性，使得模型在接近全局最优解时能够更加精细地调整参数</span><br><span class="line">实际中会结合热重启(Warm Restart)，Warm Restart是指在训练过程中定期重启学习率，并在每次重启时增加学习率，以模拟不同的训练阶段。这种机制可以帮助模型跳出局部最小值，找到更优的解</span><br></pre></td></tr></table></figure>

<h4 id="138-简述yolov8网络结构"><a href="#138-简述yolov8网络结构" class="headerlink" title="138.简述yolov8网络结构"></a>138.简述yolov8网络结构</h4><h4 id="139-用python实现iou计算"><a href="#139-用python实现iou计算" class="headerlink" title="139.用python实现iou计算"></a>139.用python实现iou计算</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">intersection</span><span class="params">(box1,box2)</span>:</span></span><br><span class="line">    box1_x1,box1_y1,box1_x2,box1_y2 = box1[:<span class="number">4</span>]</span><br><span class="line">    box2_x1,box2_y1,box2_x2,box2_y2 = box2[:<span class="number">4</span>]</span><br><span class="line">    x1 = max(box1_x1,box2_x1)</span><br><span class="line">    y1 = max(box1_y1,box2_y1)</span><br><span class="line">    x2 = min(box1_x2,box2_x2)</span><br><span class="line">    y2 = min(box1_y2,box2_y2)</span><br><span class="line">    <span class="keyword">return</span> (x2-x1)*(y2-y1)</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">union</span><span class="params">(box1,box2)</span>:</span></span><br><span class="line">    box1_x1,box1_y1,box1_x2,box1_y2 = box1[:<span class="number">4</span>]</span><br><span class="line">    box2_x1,box2_y1,box2_x2,box2_y2 = box2[:<span class="number">4</span>]</span><br><span class="line">    box1_area = (box1_x2-box1_x1)*(box1_y2-box1_y1)</span><br><span class="line">    box2_area = (box2_x2-box2_x1)*(box2_y2-box2_y1)</span><br><span class="line">    <span class="keyword">return</span> box1_area + box2_area - intersection(box1,box2)</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">iou</span><span class="params">(box1,box2)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> intersection(box1,box2)/union(box1,box2)</span><br></pre></td></tr></table></figure>

<h4 id="140-什么是NMS？并用python实现"><a href="#140-什么是NMS？并用python实现" class="headerlink" title="140.什么是NMS？并用python实现"></a>140.什么是NMS？并用python实现</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">预测结果中，可能多个预测结果间存在重叠部分，需要保留交并比（IoU）最大的、去掉非最大的预测结果，这就是非极大值抑制（Non-Maximum Suppression，简写作NMS）</span><br><span class="line"></span><br><span class="line">NMS的算法步骤如下：</span><br><span class="line"></span><br><span class="line">1.将所有框放入队列中</span><br><span class="line">2.先找到置信度最高的框（假设为A）</span><br><span class="line">3.将A放入结果数组中</span><br><span class="line">4.依次计算其他框与A的IoU值</span><br><span class="line">5.如果某个框（假设为B）的IoU大于给定阈值（比如0.7），则认为B和A框定的是同一个物体，删除B</span><br><span class="line">6.循环上述步骤，直到队列中没有框了</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># NMS</span></span><br><span class="line">boxes.sort(key=<span class="keyword">lambda</span> x: x[<span class="number">5</span>], reverse=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">result = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> len(boxes)&gt;<span class="number">0</span>:</span><br><span class="line">    result.append(boxes[<span class="number">0</span>])</span><br><span class="line">    boxes = [box <span class="keyword">for</span> box <span class="keyword">in</span> boxes <span class="keyword">if</span> iou(box,boxes[<span class="number">0</span>])&lt;<span class="number">0.7</span>]</span><br></pre></td></tr></table></figure>

<h4 id="141-mAP值是如何计算的？"><a href="#141-mAP值是如何计算的？" class="headerlink" title="141.mAP值是如何计算的？"></a>141.mAP值是如何计算的？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mAP（mean Average Precision）指的是平均精度均值，是用来评估目标检测算法性能的指标。它的计算方法如下：</span><br><span class="line">1.对于每个类别，将检测结果按照置信度从大到小排序。</span><br><span class="line">2.依次计算每个预测框的查准率和召回率，并绘制出查准率-召回率曲线。</span><br><span class="line">3.计算该类别下面积最大的查准率-召回率曲线下的面积作为该类别的AP（Average</span><br><span class="line">Precision）。</span><br><span class="line">4.对所有类别的AP求平均得到mAP。</span><br></pre></td></tr></table></figure>

<h4 id="142-使用pytorch训练模型的完整流程"><a href="#142-使用pytorch训练模型的完整流程" class="headerlink" title="142.使用pytorch训练模型的完整流程"></a>142.使用pytorch训练模型的完整流程</h4><p>1.create dataset</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KeypointsDataset</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, img_dir, data_file)</span>:</span></span><br><span class="line">        self.img_dir = img_dir</span><br><span class="line">        <span class="keyword">with</span> open(data_file, <span class="string">"r"</span>) <span class="keyword">as</span> f:</span><br><span class="line">            self.data = json.load(f)</span><br><span class="line">        </span><br><span class="line">        self.transforms = transforms.Compose([</span><br><span class="line">            transforms.ToPILImage(),</span><br><span class="line">            transforms.Resize((<span class="number">224</span>, <span class="number">224</span>)),</span><br><span class="line">            transforms.ToTensor(),</span><br><span class="line">            transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">        ])</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.data)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, idx)</span>:</span></span><br><span class="line">        item = self.data[idx]</span><br><span class="line">        img = cv2.imread(<span class="string">f"<span class="subst">&#123;self.img_dir&#125;</span>/<span class="subst">&#123;item[<span class="string">'id'</span>]&#125;</span>.png"</span>)</span><br><span class="line">        h,w = img.shape[:<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)</span><br><span class="line">        img = self.transforms(img)</span><br><span class="line">        kps = np.array(item[<span class="string">'kps'</span>]).flatten()</span><br><span class="line">        kps = kps.astype(np.float32)</span><br><span class="line"></span><br><span class="line">        kps[::<span class="number">2</span>] *= <span class="number">224.0</span> / w <span class="comment"># Adjust x coordinates</span></span><br><span class="line">        kps[<span class="number">1</span>::<span class="number">2</span>] *= <span class="number">224.0</span> / h <span class="comment"># Adjust y coordinates</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> img, kps</span><br><span class="line">        </span><br><span class="line">train_dataset = KeypointsDataset(<span class="string">"data/images"</span>,<span class="string">"data/data_train.json"</span>)</span><br><span class="line">val_dataset = KeypointsDataset(<span class="string">"data/images"</span>,<span class="string">"data/data_val.json"</span>)</span><br><span class="line"></span><br><span class="line">train_loader = DataLoader(train_dataset, batch_size=<span class="number">8</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">val_loader = DataLoader(val_dataset, batch_size=<span class="number">8</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>2.create model</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = models.resnet50(pretrained=<span class="literal">True</span>)</span><br><span class="line">model.fc = torch.nn.Linear(model.fc.in_features, <span class="number">14</span>*<span class="number">2</span>) <span class="comment"># Replaces the last layer</span></span><br><span class="line">model = model.to(device)</span><br></pre></td></tr></table></figure>
<p>3.train model</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">criterion = torch.nn.MSELoss()</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">1e-4</span>)</span><br><span class="line"></span><br><span class="line">epochs=<span class="number">20</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">    <span class="keyword">for</span> i, (imgs,kps) <span class="keyword">in</span> enumerate(train_loader):</span><br><span class="line">        imgs = imgs.to(device)</span><br><span class="line">        kps = kps.to(device)</span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        outputs = model(imgs)</span><br><span class="line">        loss = criterion(outputs, kps)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">f"Epoch <span class="subst">&#123;epoch&#125;</span>, iter <span class="subst">&#123;i&#125;</span>, loss: <span class="subst">&#123;loss.item()&#125;</span>"</span>)</span><br></pre></td></tr></table></figure>
<p>4.validate model</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">model.eval()</span><br><span class="line"></span><br><span class="line">total_loss = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> i, (imgs,kps) <span class="keyword">in</span> enumerate(val_loader):</span><br><span class="line">        imgs = imgs.to(device)</span><br><span class="line">        kps = kps.to(device)</span><br><span class="line">        outputs = model(imgs)</span><br><span class="line">        </span><br><span class="line">        loss = criterion(outputs, kps)</span><br><span class="line">        total_loss += loss.item()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> i != <span class="number">0</span> <span class="keyword">and</span> i % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            average_loss = total_loss / i</span><br><span class="line">            print(<span class="string">f"val_iter <span class="subst">&#123;i&#125;</span>, val_avg_loss: <span class="subst">&#123;average_loss&#125;</span>"</span>)</span><br></pre></td></tr></table></figure>
<p>5.save model</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.save(model.state_dict(), <span class="string">"keypoints_model.pth"</span>)</span><br></pre></td></tr></table></figure>

<h4 id="143-什么是SIFT和HOG特征提取？"><a href="#143-什么是SIFT和HOG特征提取？" class="headerlink" title="143.什么是SIFT和HOG特征提取？"></a>143.什么是SIFT和HOG特征提取？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SIFT（尺度不变特征变换）：检测并描述图像中的局部特征点，具有尺度和旋转不变性。</span><br><span class="line">HOG（方向梯度直方图）：用于描述图像局部梯度方向分布，常用于行人检测。</span><br></pre></td></tr></table></figure>

    </div>

    
    
    

      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/ml/" rel="tag"><i class="fa fa-tag"></i> ml</a>
              <a href="/tags/dl/" rel="tag"><i class="fa fa-tag"></i> dl</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/python-review/" rel="prev" title="Python复习">
      <i class="fa fa-chevron-left"></i> Python复习
    </a></div>
      <div class="post-nav-item">
    <a href="/golden-quotes-of-maybe-you-should-talk-to-someone/" rel="next" title="《也许你该找个人聊聊》金句摘录">
      《也许你该找个人聊聊》金句摘录 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    
  <div class="comments">
    <div id="disqus_thread">
      <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
    </div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-数据从哪里来？如何构建数据集？"><span class="nav-text">1.数据从哪里来？如何构建数据集？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-数据量多大？"><span class="nav-text">2.数据量多大？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-数据量不够如何处理？"><span class="nav-text">3.数据量不够如何处理？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-采用的模型是什么？为什么使用YOLOv8？"><span class="nav-text">4.采用的模型是什么？为什么使用YOLOv8？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-什么情况下使用OpenCV，什么情况下使用深度学习？"><span class="nav-text">5.什么情况下使用OpenCV，什么情况下使用深度学习？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-准确率是多少？"><span class="nav-text">6.准确率是多少？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-写项目经验注意的问题"><span class="nav-text">7.写项目经验注意的问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-什么是有监督学习（Supervised-Learning）和无监督学习（Unsupervised-Learning）？请举例说明每种类型的应用场景"><span class="nav-text">8.什么是有监督学习（Supervised Learning）和无监督学习（Unsupervised Learning）？请举例说明每种类型的应用场景</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-贝叶斯公式及推导过程，有哪些应用场景？"><span class="nav-text">9.贝叶斯公式及推导过程，有哪些应用场景？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#10-什么是似然？"><span class="nav-text">10.什么是似然？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#11-什么是欠拟合、过拟合？如何避免过拟合？如何避免欠拟合？"><span class="nav-text">11.什么是欠拟合、过拟合？如何避免过拟合？如何避免欠拟合？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#12-神经网络加速训练方法有哪些？"><span class="nav-text">12.神经网络加速训练方法有哪些？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#13-目标检测常用算法有哪些，简述对算法的理解"><span class="nav-text">13.目标检测常用算法有哪些，简述对算法的理解</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#14-什么是感受野？"><span class="nav-text">14.什么是感受野？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#15-什么是正则化？L1、L2、smooth-L1正则化的区别"><span class="nav-text">15.什么是正则化？L1、L2、smooth L1正则化的区别</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#16-Loss-Function、Cost-Function-和-Objective-Function-的区别"><span class="nav-text">16.Loss Function、Cost Function 和 Objective Function 的区别</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#17-什么是特征归一化？为什么要归一化？"><span class="nav-text">17.什么是特征归一化？为什么要归一化？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#18-归一化常用方法？"><span class="nav-text">18.归一化常用方法？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#19-归一化处理适用模型"><span class="nav-text">19.归一化处理适用模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#20-什么是标准化？常用方法？"><span class="nav-text">20.什么是标准化？常用方法？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#21-标准化和归一化的联系和区别"><span class="nav-text">21.标准化和归一化的联系和区别</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#22-均值、离差、离差方、方差、标准差之间的关系"><span class="nav-text">22.均值、离差、离差方、方差、标准差之间的关系</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#23-方差和标准差有什么区别？"><span class="nav-text">23.方差和标准差有什么区别？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#24-回归问题的模型评估指标"><span class="nav-text">24.回归问题的模型评估指标</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#25-分类问题中的TP、FP、TN、FN是什么"><span class="nav-text">25.分类问题中的TP、FP、TN、FN是什么</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#26-如何查看混淆矩阵"><span class="nav-text">26.如何查看混淆矩阵</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#27-分类问题的模型评估指标"><span class="nav-text">27.分类问题的模型评估指标</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#28-回归问题的损失函数，为何使用平方（MSE）而不是绝对值（MAE）？"><span class="nav-text">28.回归问题的损失函数，为何使用平方（MSE）而不是绝对值（MAE）？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#29-损失函数和评估函数（指标）的区别？"><span class="nav-text">29.损失函数和评估函数（指标）的区别？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#30-什么是超参数？有哪些常用的超参数调优手段？"><span class="nav-text">30.什么是超参数？有哪些常用的超参数调优手段？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#31-有哪些常见的超参数？各自对模型有怎样的影响？"><span class="nav-text">31.有哪些常见的超参数？各自对模型有怎样的影响？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#32-什么是置信概率？"><span class="nav-text">32. 什么是置信概率？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#33-什么是交叉验证？它有哪些常见类型？"><span class="nav-text">33.什么是交叉验证？它有哪些常见类型？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#34-对于类别不均衡问题，有哪些处理方法？"><span class="nav-text">34.对于类别不均衡问题，有哪些处理方法？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#35-神经网络权重初始值如何设置？"><span class="nav-text">35.神经网络权重初始值如何设置？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#36-什么是线性回归？线性回归的特点是什么？"><span class="nav-text">36.什么是线性回归？线性回归的特点是什么？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#37-什么是多项式回归？多项式回归的特点是什么？"><span class="nav-text">37.什么是多项式回归？多项式回归的特点是什么？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#38-什么是决策树？工作原理是什么？特点？"><span class="nav-text">38.什么是决策树？工作原理是什么？特点？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#39-有哪些常见的决策树算法？"><span class="nav-text">39.有哪些常见的决策树算法？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#40-CART-在分类问题和回归问题中的异同"><span class="nav-text">40.CART 在分类问题和回归问题中的异同</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#41-什么是集成学习？"><span class="nav-text">41.什么是集成学习？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#42-Bagging与Boosting的原理是什么？二者有何区别？"><span class="nav-text">42.Bagging与Boosting的原理是什么？二者有何区别？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#43-机器学习模型的偏差和方差是什么？"><span class="nav-text">43.机器学习模型的偏差和方差是什么？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#44-什么是基于决策树的集合算法-集成学习-？"><span class="nav-text">44.什么是基于决策树的集合算法(集成学习)？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#45-简要介绍AdaBoost、GBDT、XGBoost"><span class="nav-text">45.简要介绍AdaBoost、GBDT、XGBoost</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#46-什么是随机森林？"><span class="nav-text">46.什么是随机森林？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#47-什么是逻辑回归？它是如何实现二分类的？"><span class="nav-text">47.什么是逻辑回归？它是如何实现二分类的？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#48-什么是逻辑函数（sigmoid）？它有什么特点？"><span class="nav-text">48.什么是逻辑函数（sigmoid）？它有什么特点？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#49-什么是信息熵？"><span class="nav-text">49.什么是信息熵？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#50-什么是交叉熵损失函数？"><span class="nav-text">50.什么是交叉熵损失函数？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#51-什么是朴素贝叶斯分类？特点是什么？何时使用？"><span class="nav-text">51.什么是朴素贝叶斯分类？特点是什么？何时使用？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#52-常见的朴素贝叶斯分类器有哪些？"><span class="nav-text">52.常见的朴素贝叶斯分类器有哪些？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#53-什么是支持向量机？"><span class="nav-text">53.什么是支持向量机？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#54-SVM寻找最优边界要求有哪些？"><span class="nav-text">54.SVM寻找最优边界要求有哪些？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#55-SVM的工作原理？"><span class="nav-text">55.SVM的工作原理？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#56-SVM中的核函数是什么？常用的核函数有哪些？"><span class="nav-text">56.SVM中的核函数是什么？常用的核函数有哪些？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#57-SVM的特点？"><span class="nav-text">57.SVM的特点？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#58-什么是聚类？"><span class="nav-text">58.什么是聚类？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#59-有哪些常用的相似度度量方式？"><span class="nav-text">59.有哪些常用的相似度度量方式？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#60-聚类问题的评价指标是什么？"><span class="nav-text">60.聚类问题的评价指标是什么？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#61-什么是K-Means聚类？"><span class="nav-text">61.什么是K-Means聚类？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#62-什么是DBSCAN（噪声密度）？"><span class="nav-text">62.什么是DBSCAN（噪声密度）？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#63-什么是凝聚层次算法？"><span class="nav-text">63.什么是凝聚层次算法？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#64-什么是神经网络？它有哪些常见类型？"><span class="nav-text">64.什么是神经网络？它有哪些常见类型？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#65-神经网络中的权重和偏置是什么？"><span class="nav-text">65.神经网络中的权重和偏置是什么？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#66-深度学习的优缺点是什么？"><span class="nav-text">66.深度学习的优缺点是什么？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#67-什么是激活函数？为什么要使用激活函数？"><span class="nav-text">67.什么是激活函数？为什么要使用激活函数？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#68-神经网络中常用的激活函数有哪些，各自有什么特点？"><span class="nav-text">68.神经网络中常用的激活函数有哪些，各自有什么特点？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#69-激活函数的比较"><span class="nav-text">69.激活函数的比较</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#70-什么是损失函数？损失函数的作用是什么？"><span class="nav-text">70.什么是损失函数？损失函数的作用是什么？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#71-什么是梯度？什么是梯度下降？"><span class="nav-text">71.什么是梯度？什么是梯度下降？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#72-什么是梯度消失？如何解决梯度消失问题？"><span class="nav-text">72.什么是梯度消失？如何解决梯度消失问题？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#73-什么是梯度爆炸？如何解决梯度爆炸问题？"><span class="nav-text">73.什么是梯度爆炸？如何解决梯度爆炸问题？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#74-什么是反向传播算法？为何要使用？"><span class="nav-text">74.什么是反向传播算法？为何要使用？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#75-深度学习中，常见的优化器（梯度下降算法）有哪些？各有什么特点？"><span class="nav-text">75.深度学习中，常见的优化器（梯度下降算法）有哪些？各有什么特点？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#76-CNN中的feature-map、padding、stride分别是什么？"><span class="nav-text">76.CNN中的feature map、padding、stride分别是什么？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#77-卷积运算输出矩阵大小的计算公式？"><span class="nav-text">77.卷积运算输出矩阵大小的计算公式？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#78-CNN网络中的卷积层、激活层、池化层各有什么作用？"><span class="nav-text">78.CNN网络中的卷积层、激活层、池化层各有什么作用？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#79-什么是最大池化、平均池化？"><span class="nav-text">79.什么是最大池化、平均池化？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#80-池化层有什么特点？"><span class="nav-text">80.池化层有什么特点？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#81-深度卷积网络中的降采样，有哪些方式？"><span class="nav-text">81.深度卷积网络中的降采样，有哪些方式？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#82-什么是dropout？为什么dropout能避免过拟合？"><span class="nav-text">82.什么是dropout？为什么dropout能避免过拟合？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#83-什么是批量归一化（Batch-Normalization），其优点是什么？"><span class="nav-text">83.什么是批量归一化（Batch Normalization），其优点是什么？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#84-什么是分词？分词的作用是什么？"><span class="nav-text">84.什么是分词？分词的作用是什么？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#85-中文分词有哪些方法？"><span class="nav-text">85.中文分词有哪些方法？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#86-什么是词性标记？"><span class="nav-text">86.什么是词性标记？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#87-什么是词干提取？"><span class="nav-text">87.什么是词干提取？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#88-什么是词袋模型？它的缺点是什么？"><span class="nav-text">88.什么是词袋模型？它的缺点是什么？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#89-什么是TF-IDF？"><span class="nav-text">89.什么是TF-IDF？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#90-常用的文本表示方法有哪些？各自特点是什么？"><span class="nav-text">90.常用的文本表示方法有哪些？各自特点是什么？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#91-什么是语料库？它的特点、作用是什么？"><span class="nav-text">91.什么是语料库？它的特点、作用是什么？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#92-什么是Word2vec？代表模型有哪些？"><span class="nav-text">92.什么是Word2vec？代表模型有哪些？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#93-Word2vec中的负采样是什么？"><span class="nav-text">93.Word2vec中的负采样是什么？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#94-常用的色彩空间有哪些？"><span class="nav-text">94.常用的色彩空间有哪些？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#95-什么是图像灰度化处理？具体步骤是怎样的？"><span class="nav-text">95.什么是图像灰度化处理？具体步骤是怎样的？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#96-什么是图像二值化-反二值化处理，有什么优点？"><span class="nav-text">96.什么是图像二值化&#x2F;反二值化处理，有什么优点？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#97-什么是直方图均衡化？"><span class="nav-text">97.什么是直方图均衡化？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#98-图像加法运算有什么应用？"><span class="nav-text">98.图像加法运算有什么应用？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#99-图像减法运算有什么应用？"><span class="nav-text">99.图像减法运算有什么应用？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#100-图像放大时，可以采用哪些插值法？"><span class="nav-text">100.图像放大时，可以采用哪些插值法？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#101-对图像进行模糊平滑处理，可以使用哪些方式？"><span class="nav-text">101.对图像进行模糊平滑处理，可以使用哪些方式？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#102-对图像进行边沿检测，有哪些常用算子？"><span class="nav-text">102.对图像进行边沿检测，有哪些常用算子？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#103-图像的腐蚀有哪些实际的应用场景？"><span class="nav-text">103.图像的腐蚀有哪些实际的应用场景？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#104-图像的膨胀有哪些实际的应用场景？"><span class="nav-text">104.图像的膨胀有哪些实际的应用场景？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#105-图像的开运算有哪些实际的应用场景？"><span class="nav-text">105.图像的开运算有哪些实际的应用场景？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#106-图像的闭运算有哪些实际的应用场景？"><span class="nav-text">106.图像的闭运算有哪些实际的应用场景？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#107-图像的形态学梯度运算是什么？"><span class="nav-text">107.图像的形态学梯度运算是什么？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#108-什么是图像的礼帽运算？"><span class="nav-text">108.什么是图像的礼帽运算？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#109-在深度学习模型训练中，收敛速度和训练速度分别是什么？"><span class="nav-text">109.在深度学习模型训练中，收敛速度和训练速度分别是什么？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#110-在模型训练过程中，为何增加batch-size可能会导致模型的收敛速度变快？"><span class="nav-text">110.在模型训练过程中，为何增加batch size可能会导致模型的收敛速度变快？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#111-在SVM、逻辑回归等二分类模型中，在模型训练完成后，如果调高分类阈值，模型的precision、recall、auc值会如何变化？"><span class="nav-text">111.在SVM、逻辑回归等二分类模型中，在模型训练完成后，如果调高分类阈值，模型的precision、recall、auc值会如何变化？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#112-LSTM对比原始RNN的最大改进是什么？bi-LSTM对比LSTM最大的改进是什么？"><span class="nav-text">112.LSTM对比原始RNN的最大改进是什么？bi-LSTM对比LSTM最大的改进是什么？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#113-经过下列卷积操作后，3x3-conv-gt-3x3-conv-gt-2x2-max-pool-gt-3x3conv，卷积步长为1，没有填充，输出神经元感受野是多大？"><span class="nav-text">113.经过下列卷积操作后，3x3 conv -&gt; 3x3 conv -&gt; 2x2 max pool -&gt; 3x3conv，卷积步长为1，没有填充，输出神经元感受野是多大？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#114-简述离散化的好处，并写出离散化的常用方法有哪些？"><span class="nav-text">114.简述离散化的好处，并写出离散化的常用方法有哪些？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#115-常见函数的求导公式"><span class="nav-text">115.常见函数的求导公式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#116-利用梯度下降法优化目标函数ln-wx-，给定第一轮参数w-2-x-10，请写出第三轮优化后的参数值（学习率-0-1）"><span class="nav-text">116.利用梯度下降法优化目标函数ln(wx)，给定第一轮参数w&#x3D;2, x&#x3D;10，请写出第三轮优化后的参数值（学习率 &#x3D; 0.1）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#117-已知卷积层中输入尺寸32x32x3，有10个大小为5x5的卷积核，stride-1，pad-2，输出特征图大小为多少？总的特征数量为多少？"><span class="nav-text">117.已知卷积层中输入尺寸32x32x3，有10个大小为5x5的卷积核，stride&#x3D;1，pad&#x3D;2，输出特征图大小为多少？总的特征数量为多少？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#118-Faster-RCNN中，ROI-pooling具体如何工作（怎么把不同大小的框，pooling到同样大小）？"><span class="nav-text">118.Faster RCNN中，ROI pooling具体如何工作（怎么把不同大小的框，pooling到同样大小）？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#119-简述SSD和YOLO的区别？"><span class="nav-text">119.简述SSD和YOLO的区别？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#120-YOLOv2与YOLOv3的区别"><span class="nav-text">120.YOLOv2与YOLOv3的区别</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#121-测试集中有1000个样本，600个是A类，400个B类，模型预测结果700个判断为A类，其中正确有500个，300个判断为B类，其中正确有200个。请计算B类的查准率（Precision）和召回率（Recall）"><span class="nav-text">121.测试集中有1000个样本，600个是A类，400个B类，模型预测结果700个判断为A类，其中正确有500个，300个判断为B类，其中正确有200个。请计算B类的查准率（Precision）和召回率（Recall）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#122-Sigmoid函数的导数推导过程"><span class="nav-text">122.Sigmoid函数的导数推导过程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#123-深度学习中，实现上采样有哪些方法？"><span class="nav-text">123.深度学习中，实现上采样有哪些方法？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#124-深度可分离卷积是什么？"><span class="nav-text">124.深度可分离卷积是什么？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#125-LSTM跟GRU的区别"><span class="nav-text">125.LSTM跟GRU的区别</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#126-什么是一维傅里叶变换？它在图像分析中有哪些作用？"><span class="nav-text">126.什么是一维傅里叶变换？它在图像分析中有哪些作用？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#127-常用图像分割算法有哪些？各有什么优缺点？"><span class="nav-text">127.常用图像分割算法有哪些？各有什么优缺点？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#128-简单介绍几种常用的人脸检测算法"><span class="nav-text">128.简单介绍几种常用的人脸检测算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#129-你知道哪些OCR中的文字识别模型和文字检测模型？"><span class="nav-text">129.你知道哪些OCR中的文字识别模型和文字检测模型？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#130-在图像处理中，滤波、模糊、去噪，这三者是什么关系？"><span class="nav-text">130.在图像处理中，滤波、模糊、去噪，这三者是什么关系？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#131-什么是HOUGH（霍夫）变换？"><span class="nav-text">131.什么是HOUGH（霍夫）变换？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#132-列出机器学习中常见的分类算法，并比较各自的特点"><span class="nav-text">132.列出机器学习中常见的分类算法，并比较各自的特点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#133-使用opencv将一个图像旋转180°"><span class="nav-text">133.使用opencv将一个图像旋转180°</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#134-机器学习中的降维有哪些常用方法？"><span class="nav-text">134.机器学习中的降维有哪些常用方法？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#135-vgg16的网络结构？"><span class="nav-text">135.vgg16的网络结构？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#136-ResNet的网络结构？"><span class="nav-text">136.ResNet的网络结构？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#137-什么是学习率的余弦退火？"><span class="nav-text">137.什么是学习率的余弦退火？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#138-简述yolov8网络结构"><span class="nav-text">138.简述yolov8网络结构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#139-用python实现iou计算"><span class="nav-text">139.用python实现iou计算</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#140-什么是NMS？并用python实现"><span class="nav-text">140.什么是NMS？并用python实现</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#141-mAP值是如何计算的？"><span class="nav-text">141.mAP值是如何计算的？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#142-使用pytorch训练模型的完整流程"><span class="nav-text">142.使用pytorch训练模型的完整流程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#143-什么是SIFT和HOG特征提取？"><span class="nav-text">143.什么是SIFT和HOG特征提取？</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="roubin"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">roubin</p>
  <div class="site-description" itemprop="description">芝兰其室，金石其心<br/>仁义为友，道德为师</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">146</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">41</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">89</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/satorioh" title="GitHub → https://github.com/satorioh" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:wangbinxp@gmail.com" title="E-Mail → mailto:wangbinxp@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2015 – 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">roubin</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  
  <script>
    (function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();
  </script>




  
<script src="/js/local-search.js"></script>













  

  

<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://roubinme.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>
<script>
  var disqus_config = function() {
    this.page.url = "https://roubin.me/ml-review/";
    this.page.identifier = "ml-review/";
    this.page.title = "ML/DL复习";
    };
  NexT.utils.loadComments(document.querySelector('#disqus_thread'), () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://roubinme.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });
</script>

</body>
</html>
